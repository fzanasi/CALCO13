
%\documentclass[10pt]{llncs}

\documentclass[3p]{elsarticle}

\usepackage{url}
\usepackage{vaucanson-g}
\sloppy

%\usepackage{times} %changes font to times, which saves lots of space!
\usepackage{longtable}
\usepackage{wrapfig}

\usepackage[small]{caption}
\setlength{\captionmargin}{12pt}

\usepackage{color}

\usepackage{graphicx,epsfig,color}

\usepackage{xypic}
\input xy
\xyoption{all}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amssymb}
%
% --- Modifications of mathcode (cf. TeX book p. 344) ---
%
\mathcode`:="003A  % : ordinary symbol
\mathcode`;="003B  % ; ordinary symbol
\mathcode`?="003F  % ? ordinary symbol
\mathcode`|="026A  % | ordinary symbol
\mathcode`<="4268  % < abbreviates \langle
\mathcode`>="5269  % > abbreviates \rangle

\mathchardef\ls="213C    % less symbol (< used as \langle)
\mathchardef\gr="213E    % greater symbol (> used as \rangle)
\mathchardef\uparrow="0222  % adaptation mathmode of uparrow
\mathchardef\downarrow="0223  % adaptation mathmode of downarrow


\newcommand{\lbb}{\mathopen{[\![}}
\newcommand{\rbb}{\mathclose{]\!]}}
\newcommand{\bb}[1]{\lbb #1 \rbb}

\def\pow#1{{\mathcal P_\omega}#1}
\newcommand\pto\rightharpoonup
\newcommand\E\varepsilon
\newcommand{\pp}[1]{\|#1\|}

\def\mean#1{[\![ \, #1 \, ]\!]}
\def\expr#1{<\!< \, #1 \, >\!>}
\def\ceil#1{\lceil\, #1 \,\rceil}

\def\prob#1#2{#2 \cdot #1}

\newcommand{\cbox}[1]{\vspace{0.2cm}\noindent
  \fbox{\parbox{.97\textwidth}{#1}}\vspace{0.2cm}}

\usepackage{eucal}
%\usepackage[all]{xy}
%\DeclareMathSizes{10}{9}{3}{2}

\def\tr#1{\stackrel{#1}{\to}}          % Labeled Transitions
\newcommand{\X}{\mathcal{X}}           % constant stream X


\newcommand{\fG}{\mathcal{G}}    % Generic Functor G
\newcommand{\fW}{\mathcal{W}}    % Functor W
\newcommand{\fL}{\mathcal{L}}    % Functor L
\newcommand{\emp}{\epsilon}           % Empty Function
\newcommand{\der}{d}           % Derivative Function

%PRODUCTS
\newcommand{\setproduct}{\times} %Cartesian Product of sets and functions
\newcommand{\vectproduct}{\times} %(Bi)Product of Vector Spaces and linear maps
\newcommand{\matrixproduct}{\times} %Product of Matrices
\newcommand{\streamproduct}{\times} %Product of Streams
\newcommand{\beh}[3]{\left[\!\left[ #1 \right]\!\right]^{#2}_{#3}} % Final Morphism
\newcommand{\Beh}[3]{\Big[\!\!\Big[ #1 \Big]\!\!\Big ]^{#2}_{#3}} % Final Morphism
\newcommand{\image}{\mathrm{im}} % image
%%%% MICHELE'S MACRO
\newcommand{\comp}{\circ}               % composition of functions and linear maps
\newcommand{\dimn}{\mathrm{dim}}   % dimension
\newcommand{\transp}{{}^{\mathrm{t}}}  % transpose
\newcommand{\K}{\mathbb{K}}            % generic field K
\newcommand{\SR}{\mathbb{S}}            % generic field K
\newcommand{\dual}[1]{ {#1}^\star}     % dual space
\newcommand{\ddual}[1]{{#1}^{\star\star}}  % double dual space
\newcommand{\ann}{o}                    % annihilator
\newcommand{\kernel}{\mathrm{ker}} % kernel
\newcommand{\wa}{{\sc wa}}             % "wa"
\newcommand{\lwa}{{\sc lwa}}           % "lwa"
\newcommand{\Span}{\mathrm{span}}  % span
\newcommand{\R}{R}%{\mathcal{R}}    % relation
\newcommand{\mik}[1]{\marginpar{ \textbf{MiB:} {\footnotesize #1}}} % for margin notes
\newcommand{\mar}[1]{\marginpar{ \textbf{MaB:} {\footnotesize #1}}} % for margin notes


%UNUSED MACROS
\newcommand{\posreals}{{\mathbb{R}^{\geq 0}}}  % nonnegative reals
\newcommand{\reals}{{\mathbb{R}}}              % reals
\newcommand{\lpar}{\langle}
\newcommand{\rpar}{\rangle}
\newcommand{\RQ}{\reals^Q}             % free vector space on R generated by Q
\newcommand{\FVS}[1]{{#1}^Q}           % generic free vector space generated by Q
\newcommand{\fps}{{\sc fps}}                  % "fps"
\newcommand{\fpset}{\reals\langle\!\langle A\rangle\!\rangle}    %  formal power series
\newcommand{\defin}{\stackrel{\triangle}{=}} % for def.


\usepackage[thmmarks,hyperref]{ntheorem}
%
%
% FOR TCS
%
%
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{example}{Example}
\newdefinition{definition}{Definition}
\newproof{proof}{Proof}
\newtheorem{corollary}{Corollary}

%\newproof{pot}{Proof of Theorem \ref{thm2}}


%
%
% COMMENTATI PER TCS
%
%
%\theoremseparator{.}
%%\theoremheaderfont{\normalfont}
%\theorembodyfont{\normalfont}
%\theoremsymbol{\ensuremath{\clubsuit}}
%\renewtheorem{definition}{\textsc{Definition}}
%%
%\theoremsymbol{}
%\theoremheaderfont{\normalfont\bfseries}
%\theorembodyfont{\normalfont\itshape}
%\renewtheorem{lemma}[definition]{\textsc{Lemma}}
%%
%\theoremheaderfont{\normalfont\bfseries}
%\theorembodyfont{\normalfont\itshape}
%\renewtheorem{theorem}[definition]{\textsc{Theorem}}
%%
%\theoremheaderfont{\normalfont\bfseries}
%\theorembodyfont{\normalfont\itshape}
%\renewtheorem{proposition}[definition]{\textsc{Proposition}}
%%
%\theoremheaderfont{\normalfont\bfseries}
%\theorembodyfont{\normalfont\itshape}
%\renewtheorem{corollary}[definition]{\textsc{Corollary}}
%%
%\theoremheaderfont{\normalfont\bfseries}
%\theorembodyfont{\upshape}
%\theoremsymbol{\ensuremath{\spadesuit}}
%\renewtheorem{example}[definition]{\textsc{Example}}
%%
%\theorembodyfont{\upshape}
%\theoremheaderfont{\normalfont}
%\theoremstyle{nonumberplain}
%\theoremsymbol{\ensuremath{\Box}}
%\renewtheorem{proof}{\textsc{Proof}}



\title{A Coalgebraic Perspective on Linear Weighted Automata}%\tnoteref{t1,t2}}
%\tnotetext[t1]{This document is a collaborative effort.}
%\tnotetext[t2]{The second title footnote which is a longer
%longer than the first one and with an intention to fill
%in up more than one line while formatting.}
%\title{}

\author[ens]{Filippo Bonchi\corref{cor1}}%\fnref{fn1}}
\ead{filippo.bonchi@ens-lyon.fr}
\author[liacs,cwi]{Marcello Bonsangue}%\corref{cor1}\fnref{fn1}}
\ead{marcello@liacs.nl}
\author[dsif]{Michele Boreale}%\corref{cor1}\fnref{fn1}}
\ead{boreale@dsi.unifi.it}
\author[cwi,nej]{Jan Rutten}%\corref{cor1}\fnref{fn1}}
\ead{jan.rutten@cwi.nl}
\author[cwi]{Alexandra Silva}%\corref{cor1}\fnref{fn1}}
\ead{ams@cwi.nl}


\cortext[cor1]{Corresponding author}
%\cortext[cor2]{Principal corresponding author}
%\fntext[fn1]{This is the specimen author footnote.}

\address[ens]{ENS Lyon, Universit\'{e} de Lyon, LIP (UMR 5668 CNRS ENS Lyon UCBL INRIA), 46 All\'e d'Italie, 69364 Lyon, France}
\address[liacs]{Leiden Institute Advanced Computer Science, Niels Bohrweg 1, 2333 CA Leiden, The Netherlands}
\address[cwi]{Centrum Wiskunde \& Informatica, Science Park 123, 1098 XG Amsterdam, The Netherlands}
\address[dsif]{Dipartimento di Sistemi e Informatica, Universit\`{a} di Firenze, Viale Morgagni 65, I-50134 Firenze, Italy}
\address[nej]{Radboud Universiteit Nijmegen, Heyendaalseweg 135, 6525 AJ Nijmegen, The Netherlands}

%\author{Filippo Bonchi\inst{1}\and Marcello Bonsangue\inst{2,3}\and Michele Boreale\inst{4}\and \\
%Jan Rutten\inst{3,5} \and Alexandra Silva\inst{3}%\thanks{Partially supported by the
%Funda\c{c}\~ao para a Ci\^encia e a Tecnologia, Portugal, under grant
%number SFRH/BD/27482/2006.}
%}
%
%\institute{
%CNRS and LIP, ENS-lyon \and
%LIACS - Leiden University \and
%Centrum voor Wiskunde en Informatica (CWI) \and
%Dipartimento di Sistemi e Informatica - Universit\`a di Firenze \and
%Radboud Universiteit Nijmegen
%}

\newcommand\F{\mathcal{F}}
\newcommand\V{\mathcal{V}}

\begin{document}

\begin{abstract}
Weighted automata are a generalization of non-deterministic automata where each transition,
in addition to an input letter, has also a quantity expressing the weight (e.g. cost or probability)
of its execution. As for non-deterministic automata, their behaviours can be expressed in terms
of either (weighted) bisimilarity or (weighted) language equivalence.

Coalgebras provide a categorical framework for the uniform study of state-based systems and
their behaviours. In this work, we show that coalgebras can suitably model weighted automata in
two different ways: coalgebras on $Set$ (the category of sets and functions) characterize
weighted bisimilarity, while coalgebras on $Vect$ (the category of vector spaces and linear
maps) characterize weighted language equivalence.

Relying on the second characterization, we show three different procedures for computing
weighted language equivalence. The first one consists in a generalizion of the usual partition
refinement algorithm for ordinary automata. The second one is the backward version of the
first one. The third procedure relies on a syntactic representation of rational weighted
languages.

%No prior knowlegde of coalgebras or linear algebra is assumed.
\end{abstract}

\maketitle




\section{Introduction}

Weighted automata were introduced in Sch\"utzenberger's classical
paper~\cite{Schutzenberger61b}. They are of great importance in
computer science~\cite{wahandbook}, arising in different areas of application, such as
speech recognition~\cite{Moh97}, image compression~\cite{image-comp}, control
theory~\cite{isidori} and quantitative modelling~\cite{LarsenS91,qt-hb}.
%
% In most of the applications, it is crucial
%to
%calculate shortest distance or minimal cost paths~\cite{Moh09}, and
%also to model
%stochastic or quantitative aspects of a computation.
%
%
They can be seen as a generalization of non-deterministic
automata, where each transition has a weight associated to it.
This weight is an element of a semiring, representing, for example,
the cost or probability of taking the transition.

The behaviour of weighted automata is usually given in terms of
weighted languages (also called formal power series ~\cite{SS78,BR-series}),
that are functions assigning a weight to each finite string  $w \in A^*$ over
an input alphabet $A$. For computing the weight
given to a certain word, the semiring structure plays a key role:
 the multiplication of the semiring is used to accumulate
the  weight of a path by multiplying the weights of each transition
in the path, while the addition of the semiring computes the weight of
a string $w$ by summing up the weights of the paths labeled with
$w$~\cite{Kui97}.
%
Alternatively, the behaviour of weighted automata can be expressed in terms of
weighted bisimilarity~\cite{german}, that is, an extension of bisimilarity (for
non-deterministic automata) subsuming several kinds of quantitative equivalences
such as, for example, probabilistic bisimilarity \cite{DBLP:conf/concur/JouS90}.
As in the case of non-deterministic automata, (weighted) bisimilarity implies
strictly (weighted) language equivalence.

%Ordinary nondeterministic
%automata are a special case of weighted automata using the two elements Boolean
%semiring. For this case, classical (regular) languages can be
%viewed as power series using their associated characteristic function.

%Weighted automata still retain non-deterministic behavior, as two different
%transitions outgoing from the same state may be labelled by the same input
%action, possibly with different weights. Deterministic weighted automata are
%of interest because their construction is tightly connected with the existence of
%minimal automata recognizing the same weighted language.
%The classical powerset construction for obtaining a language-equivalent
%deterministic automaton from a non-deterministic one can be generalized to weighted
%automata, as long as the semiring respects certain restrictions~\cite{Moh09,KM05}.
%The states of the determinized automaton are finite ``subsets  of weighted states'' of
%the original non-deterministic automaton, or, more formally, functions
%from the set of states to the semiring that are almost everywhere zero.
%Differently from the classical case, though, the weighted automaton  obtained by the
%powerset construction might be infinite. Usually, one restricts the attention to semirings
%for which determinization is guaranteed to terminate and produce a finite result, such as
%locally finite and tropical semirings, and extensions thereof~\cite{Moh09,KM05}.
%%\mik{I have taken out the ref. [Mohri97], which seems to be subsumed by the previous ones.
%%Besides, the claim that [Mohri97] relies on partition-refinement seemed unjustified.}
%%An algorithm for computing a minimal deterministic weighted automaton is a generalization of the
%%partition-refinement algorithm for classical finite automata~\cite{Moh97}.

In this paper, we study {\em linear weighted automata}, which
are ``deterministic'' weighted automata where the set of states forms a vector space.
A linear weighted automaton can be viewed as the result of ``determinizing''  an
ordinary  weighted automaton with weights in a generic {\em field}, using some kind of
``weighted powerset construction''. As such, linear weighted automata are  typically
infinite-state. The key point is that the linear structure of the state-space allows
for finite representations of these automata and effective algorithms operating on them.

To be more specific, the goal of the present paper is to undertake a \emph{systematic study of
the behavioural equivalences and minimization algorithms for (linear) weighted automata}.
To achieve this goal, we will benefit from a coalgebraic perspective on linear weighted automata.
The theory of coalgebras offers a unifying mathematical framework for the study of
many different types of state-based systems and infinite data structures.
Given a functor $\fG\colon  C \to C$ on a category $C$, a $\fG$-coalgebra is a pair consisting of an
object $X$ in $C$ (representing the state space of the system) and a morphism $f:X \to \fG X$ (determining the dynamics
of the system). Under mild conditions, functors $\fG$ have a final coalgebra (unique up to isomorphism)
into which every $\fG$-coalgebra can be mapped via a unique so-called $\fG$-homomorphism.
The final coalgebra can be viewed as the universe of all possible $\fG$-behaviours: the
unique homomorphism into the final coalgebra maps every state of a coalgebra to a
canonical representative of its behaviour. This provides a general notion of behavioural
equivalence ($\approx_{\fG}$): two states are equivalent if and only if they are mapped to the same element
of the final coalgebra.

%Our first contribution in this paper is to recast both weighted bisimilarity and
%weighted language equivalence in the theory of coalgebras. We see an ordinary weighted
%automaton as a coalgebra of the functor $\fW = \F \times V_\F(-)^A$ on $Set$
%(the category of sets and functions), where $\V_\F(X)$ is the free vector
%space on a field $\F$ with $X$ as basis. Intuitively, a $\fW$ coalgebra associates
%a weight $o(x) \in \F$ to each state $x \in X$  and a weight $t(x)(a)(x')$ to each
%transition labelled by $a \in A$ from the state $x$ to the state $x'$.
%Note that for each state $x$ and input $a$ only finitely many transitions will
%have a non-null weight.
%
%Adapting the above reasoning, we model linear weighted automata  as coalgebras
%of the functor $\fL = \F \times (-)^A$ on $Vect$ (the category of vector spaces and
%linear maps). A linear weighted automaton $<o,t>:X \to \F \times X^A$ consists of
%two maps $o$ and $t$ preserving the linear structure of the set of states. If
%if $B$  is a (finite) base for $X$, then $o(b)$ represents the output weight of $b$,
%and $t(b)(a)$ is the vector representing the weights of the outgoing transitions from $b$
%labelled by $a$. The automata is deterministic, as for each label $a$ and state $b$, at
%most one element of $B$ will have a non-null weight.

Our first contribution in this paper is to recast both weighted bisimilarity and
weighted language equivalence in the theory of coalgebras. We see weighted
automata for a field $\K$ and alphabet $A$, as coalgebras of the functor $\fW = \K \times \K^{-^A}$ on $Set$
(the category of sets and functions). Concretely, a $\fW$-coalgebra consists of a set of states $X$
and a function $<o,t>\colon X \to \K \times \K^{X^A}$ where, for each state $x\in X$, $o\colon X \to \K$ assigns
an output weight in $\K$ and $t \colon X \to \K^{X^A}$ assigns a function in $\K^{X^A}$.
For each symbol $a\in A$ and state $x'\in X$, $t(x)(a)(x')$ is a weight $k\in \K$ representing the
weight of a transition from $x$ to $x'$ with label $a$, in symbols $x\tr{a,k}x'$.
If  $t(x)(a)(x')=0$, then there is no $a$-labeled transition from $x$ to $x'$.
Note that there could exist several weighted transitions with the same label outgoing from the same state:
$x \tr{a,k_1}x_1,\, x\tr{a,k_2}x_2,\, \dots ,\, x\tr{a,k_n}x_n$.


Adapting the above reasoning, we model linear weighted automata  as coalgebras
of the functor $\fL = \K \times (-)^A$ on $Vect$ (the category of vector spaces and
linear maps). A linear weighted automaton consists of a vector space $V$ and a linear map $<o,t>\colon V \to \K \times V^A$ where,
as before, $o\colon V \to \K$ defines the output and $t\colon V \to V^A$ the transition structure.
More precisely, for each vector $v\in V$ and $a\in A$, $t(v)(a)=v'$ means that there is a transition
from $v$ to $v'$ with label $a$, in symbols $v \tr{a}v'$.
Note that the transition structure is now ``deterministic'', since for each vector $v$ and input $a \in A$ there is only one vector $v'\in V$.
When $V=\K^X$, each vector $v \in V$ can be seen as a linear combination of states $x_1, \dots, x_n \in X$,
i.e., $v = k_1x_1 + \dots + k_nx_n$ for some $k_1,\dots, k_n \in \K$.
Therefore, the transitions $x \tr{a,k_1}x_1, \, \dots,\, x\tr{a,k_n}x_n$ of a weighted automaton
correspond to a single transition $x\tr{a}(k_1x_1 + \dots + k_nx_n)$ of a linear weighted automaton.

%\mar{New paragraph - please check}
We show that $\approx_{\fW}$ (i.e., the behavioural equivalence induced by $\fW$) coincides
with weighted bisimulation while $\approx_{\fL}$ coincides with weighted language equivalence.
Determinization is the construction for moving from ordinary weighted automata
and weighted bisimilarity to linear weighted automata and weighted
language equivalence. Similar to the powerset construction, determinization
combines all the states within one vector, but unlike the determinization of
a non-deterministic automaton, the the resulting state space will not be finite but
forming a vector space of finite dimension. On this respect, our determinization
differs from the construction described by Mohri~\cite{Moh97} for a subclass of
weighted automata with weights on a semiring (rather than a field), which
associates states of the determinized weighted automaton with a set of states
of the original weighted automaton.

Once we have fixed the mathematical framework, we investigate three different
types of  algorithms for computing $\approx_{\fL}$. These algorithms work under the
assumption that the underlying vector space has finite dimension. The first is a forward
algorithm that  generalizes the usual partition-refinement algorithm for ordinary automata:
one starts by decreeing as equivalent states with the same output values, then refines the obtained relation by separating
states for which outgoing transitions go to states that are not already
equivalent. Linearity of the automata plays a crucial role to ensure termination of the
algorithm. Indeed, the    equivalences computed at each iteration    can be represented
as \emph{finite}-dimensional sub-spaces in the given vector space.  The resulting
descending chain of sub-spaces must  therefore   converge in a finite number of steps,
despite the fact that the state-space itself is infinite. We also show that each
iteration of the algorithm coincides with the equivalence generated by each step of
the (standard) construction of the final coalgebra via the final sequence.
The minimal linear representations of weighted automata over a field
was first considered by Schutzenberger~\cite{Schutzenberger61b}. This algorithm was
reformulated in a more algebraic and somewhat simplified fashion in the book of Berstel and Reutenauer~\cite{BR-series}. Their algorithm is different from our method, as
it is related to the construction of a basis for a subgroup of a free group. Further, no
evident connections can be traced between their treatment  and the notions
of bisimulation and coalgebras.

The second algorithm proceeds in a similar way, but uses a backward procedure. It has been introduced
by the third author together with linear weighted automata~\cite{Bor09}.
In this case,  the algorithm starts from the \emph{complement} -- in a precise
geometrical sense -- of   the relation identifying vectors with
equal weights. Then  it  incrementally  computes the space of all
states that are \emph{backward} reachable from this relation. The largest
bisimulation is obtained by taking the complement  of this space.
The advantage of this algorithm over the previous one is that the size of the
intermediate relations is typically much smaller. The presentation of this algorithm
in~\cite{Bor09} is  somewhat more concrete, as there is no attempt at a
coalgebraic treatment and the underlying field is fixed to $\reals$ (for example,
this leads to using  orthogonal complements  rather than dual spaces and annihilators,
which we consider in Section~\ref{sec:linearpr}). No connection is made with rational series.

%Geometrically,
%$R_0$ is represented by $\kernel(\phi)^\bot$, while
%``going backward" means working with the transpose transition
%functions  $\transp   t_a$  rather than with $t_a$. Taking the
%complement of a relation  actually means taking its annihilator.
% This essentially leads one to work within $\dual V$ rather than $V$.
%  We use $U+W$ to denote $\Span(U\cup W)$.
%We show that taking the quotient of a linear weighted automaton by a
%linear bisimulation is equivalent to projecting the original
%state space of the automaton onto the orthogonal complement of the sub-space
%representing the bisimulation. The obtained minimal automaton is canonical,
%recognizes the same weighted language as the original one, and can be constructed
%in linear-time (essentially in the same way as by the algorithm proposed by
%Sch\"utzenberger~\cite{Schutzenberger61b}).

Finally, the third algorithm is new and uses the fact that equivalent states
are mapped by the unique homomorphism into the same element of the final
coalgebra.  We characterize the final morphism
in terms of so-called rational weighted languages
(which are also known as rational formal power series).
This characterization is useful for the computation
of the kernel of the final homomorphism, which
consists of weighted language equivalence.
Taking again advantage of the linear structure of our
automata, calculating the kernel of the above homomorphism will correspond
to solving a linear system of equations.

The results in this paper are  presented  for weighted automata with
weights taken from a field, as opposed to the more general and classical
definition where weights from a semiring are considered. This restriction
is convenient for presentation purposes and, as we will discuss in
Section \ref{sec:discussion}, many of the results (but not all) can be
extended to semirings.



%\mik{Related work to be completed.}
%\paragraph{Related work}
%
%Weighted automata were introduced in Sch\"utzenberger classical
%paper~\cite{Schutzenberger61b}. There,  a minimization algorithm was also presented.
%This algorithm was reformulated in a more algebraic and   somewhat simplified fashion in
%Berstel and Reutenauer book~\cite{BR-series}. Yet, no evident connections  can be traced
%between their treatment  and the notions of bisimulation and coalgebras. Linear weighted
%automata and bisimulation appear in the third author's conference paper~\cite{Bor09}.
%Some of the material there is expanded and revised in the present paper. The approach
%of~\cite{Bor09}   is somewhat more concrete, though, as there is   no attempt at a
%coalgebraic treatment and the underlying field is fixed to $\reals$ (for example,
%this leads to using  orthogonal complements  rather than dual spaces and annihilators,
%which we consider in Section~\ref{sec:linearpr}). No connection is made with rational series.


A coalgebraic perspective on weighted automata is by no means the only
approach to understand their structure and properties, as is already
clear from the various references to related work mentioned above
(more will follow in Section \ref{sec:discussion}).
We have found the application of coalgebra as a general framework
for the study of dynamical systems and infinite behaviour
in the present setting useful for a number of reasons,
which we shall briefly discuss next.

An important feature of the coalgebraic methodology is that
once a class of systems is identified as the class
of coalgebras of a certain type (formally, a functor), then
several things come for free, following from the
general theory of universal coalgebra \cite{Rutten00}:
(i) the semantics or behaviour
of each system is obtained by a unique homomorphism into
the final coalgebra; (ii) with each coalgebra type, a canonical
notion of behavioural equivalence (bisimulation) is associated;
(iii) the homomorphism into the final coalgebra identifies
all and only those states that are
equivalent; (iv) consequently, the image of the system under this final
coalgebra homomorphism is its minimization with respect the canonical
notion of behavioural equivalence.
Yet another advantage of the general perspective of coalgebra is that
it offers a framework in which it is possible to relate
different types of systems in a rigorous manner.

By identifying, in the present setting, the different types of weighted
automata (notably, classical branching weighted automata and linear weighted
automata) as different types of coalgebras, we obtain
immediately an appropriate notion of behavioural equivalence
for each of them.
As a consequence, we have been able to
put the different existing notions of equivalence
of weighted automata (weighted bisimilarity and weighted
language equivalence) into a coherent perspective.
Using their
coalgebraic characterisations, it was relatively straightforward
to give a precise description of the transformation
of (branching) weighted automata into linear weighted automata,
by means of a generalised version of the well-known powerset
construction.
Our coalgebraic characterisation
has furthermore led to a canonical description of the minimization
of linear weighted automata,
in Section~\ref{sec:rational}. The details of this construction
are very similar to the use of rational power series
and linear systems of equations \cite{BR-series}. What is
pleasant about the coalgebraic approach is that the present description
of the minimization of linear weighted automata is an instance
of the canonical and general insights from universal coalgebra,
mentioned above.

\paragraph{Structure of the paper}
%
In Section \ref{sec:coalg} we introduce weighted automata and coalgebras.
We also show that $\fW$-coalgebras characterize weighted automata
and weighted bisimilarity. In Section \ref{sec:fromlatolwa},
after recalling some preliminary notions of linear algebras, we show that
each weighted automaton can be seen as a linear weighted automaton, i.e., an
$\fL$-coalgebra. This change of perspective allows us to coalgebraically capture
weighted language equivalence. In Section \ref{sec:linearpr}, we show the forward
and the backward algorithm while, in Section \ref{sec:rational}, we first introduce a
syntactic characterization of rational weighted languages and then we shows how to
employ it in order to compute $\approx_{\fL}$. In Section \ref{sec:discussion}, after
summarizing the main results of the paper, we discuss how to extend them to the
case of automata with weights in a semiring.

Section \ref{sec:difference} and Section \ref{sec:finalsequence} show some interesting
minor results that could be safely skipped by the not interested reader. The presentation
is self-contained and does not require any prior knowledge on the theory of coalgebras.


\section{Weighted Automata as Coalgebras}\label{sec:coalg}
%
%\mik{Restructured a bit this section - check.}
We will first introduce the fundamental definitions and facts about weighted automata, weighted bisimilarity and their
characterization as coalgebras over $Set$, the category of sets and
functions. We will next introduce  weighted language equivalence over weighted automata. In the final subsection, we will discuss a further equivalence that naturally arises from the theory of coalgebras; this equivalence will   play no role in the rest of the paper, though.

\subsection{Fundamental definitions}
First we fix some notation. We will denote sets by capital letters
$X,Y,Z\dots $ and functions by lower case $f,g,h\dots$. Given a set
$X$, $id_X$ is the identity function and, given two functions
$f\colon  X\to Y$ and $g\colon  Y \to Z$, $g\comp f$ is their
composition. The product of two sets $X,Y$ is $X \setproduct Y$ with
the projection functions $\pi_1 \colon X\times Y \to X$ and $\pi_2 \colon X\times
Y \to Y$. The product of two functions $f_1\colon X_1 \to Y_1$ and
$f_2\colon X_2 \to Y_2$ is $f_1\setproduct f_2$ defined for all
$<x_1,x_2>\in X_1\setproduct X_2$ by $(f_1\setproduct
f_2)<x_1,x_2>=<f(x_1),f(x_2)>$. The disjoint union of $X, Y$ is
$X+Y$ with injections $\kappa_1\colon X \to X+Y$ and $\kappa_2\colon Y \to X+Y$.
The union of $f_1\colon X_1 \to Y_1$ and $f_2\colon X_2 \to Y_2$ is
$f_1+f_2$ defined for all $z\in X+Y$ by
$(f_1+f_2)(\kappa_i(z))=\kappa_i((f_i(z)))$ (for $i \in \{1,2\}$).
 The set of functions $\varphi\colon
Y \to X$ is denoted by $X^Y$. For $f\colon X_1 \to X_2$, the
function $f^Y\colon X_1^Y \to X_2^Y$ is defined for all $ \varphi
\in X_1^Y$ by $f^Y(\varphi)=\lambda y \in Y. f(\varphi (y))$. The
collection of finite subsets of $X$ is denoted by $\pow (X)$ and the
empty set by $\emptyset$. For a set of letters $A$, $A^*$ denotes the
set of all finite words over $A$; $\epsilon$ the empty word; and $w_1w_2$
the concatenation of words $w_1,w_2 \in A^*$.

We fix a field $\mathbb K$. We use $k_1,k_2, \dots$ to range over
elements of $\mathbb K$. The sum of $\mathbb K$ is denoted by $+$,
the product by $\cdot$, the additive identity by $0$ and the
multiplicative identity by $1$. The \emph{support} of a function
$\varphi$ from a set $X$ to a field $\mathbb K$ is the set $\{x\in X
\mid \varphi(x)\neq 0\}$.
%The formal definition of field, as well as
%those of other basic algebraic structures can be found in the
%Appendix.

\medskip

Weighted automata~\cite{Schutzenberger61b,wahandbook} are a
generalization of ordinary automata where transitions in addition to
an input letter have also a weight in a field $\K$ and each state is
not just accepting or rejecting but has an associated output weight
in $\K$.

Formally, a \emph{weighted automaton} (\textsc{wa}, for short) with
input alphabet $A$ is a triple $(X,<o,t>)$, where $X$ is a set of
states, $o\colon  X \to \K$ is an output function associating to
each state its output weight and $t\colon  X \to (\K^X)^A$ is the
transition relation that associates a weight to each transition. We
shall use the following notation: $x\tr{a,k}y$ means that
$t(x)(a)(y)=k$. Weight $0$ means no transition.

If the set of states is finite, a \textsc{wa} can be conveniently
represented in form of matrices. First of all, we have to fix an
ordering $(x_1, \dots , x_n)$ of the set of states $X$. Then the
transition relation $t$ can be represented by a family of matrices
$\{T_a\}_{a\in A}$ where each $T_a\in \K^{n \setproduct n}$ is an
$\K$-valued square matrix, with $T_a(i,j)$ specifying the value of
the $a$-transition from $x_j$ to $x_i$, i.e., $t(x_j)(a)(x_i)$. Note that we define the matrices $T_a$ to have the source state as column index and the target state as row index. The
output weight function $o$ can be represented as an $\K$-valued row
vector in $\K^{1 \setproduct n}$ that we will denote by the capital
letter $O$.


For a concrete example, let $\K=\mathbb{R}$ (the field of real
numbers) and $A=\{a,b\}$ and consider the weighted automata $(X,
<o_X,t_X>)$ and $(Y, <o_Y,t_Y>)$ in Fig. \ref{fig:cospan}. Their
representation as matrix is the following.
%
%\begin{tabular}{ccc}
%$M_a^1$ & $=$ &$\left(%
%\begin{array}{ccc}
%  0 & 0 & 0 \\
%  1 & 0 & 0 \\
%  -1 & 0 & 0 \\
%\end{array}\right)%
% \\$o^1$ &$=$ &$ \left(%
%\begin{array}{ccc}
%  0 & 1 & 1 \\
%\end{array}\right)$
%\end{tabular}

\begin{center}
$O_X= \left(%
\begin{array}{ccc}
  1 & 1 & 1 \\
\end{array}\right)$
$T_{X_a}=\left(%
\begin{array}{ccc}
  1 & 0 & 0 \\
  1 & 3 & 0 \\
  1 & 0 & 3 \\
\end{array}\right)$
$T_{X_b}=\left(%
\begin{array}{ccc}
  3 & 3 & 3 \\
  0 & 0 & 0 \\
  0 & 0 & 0 \\
\end{array}\right)$
\hspace{0.2cm}
$O_Y= \left(%
\begin{array}{c}
  1 \\
\end{array}\right)$
$T_{Y_a}=\left(%
\begin{array}{c}
  3
\end{array}\right)$
$T_{Y_b}=\left(%
\begin{array}{c}
  3
\end{array}\right)$
\end{center}
%
%
%
%
% OLD VERSION:WRONG
%
%Given a relation $R\subseteq X \setproduc Y$, the function $R_{X}\colon
%\mathcal{P}(X)\to \mathcal{P}(Y)$ is defined $\forall Q\subseteq X$
%as $R_{X}(Q)=\{y \in Y \text{ s.t. } \exists x\in Q \text{ and }
%xRy\}$. Analogously, for $R_{Y}\colon  \mathcal{P}(Y)\to
%\mathcal{P}(X)$. We will write $R_X(x)$ to denote $R_X(\{x\})$,
%e.g., for $R$ being the relation in Fig. \ref{fig:cospan},
%$R_X(x_2)=\{y_2,y_3\}$ and $R_Y(R_X(x_2))=\{x_2,x_3\}$.
%%
%%
%%
%%
%
%Given two weighted automata $(X,o_1,t_1)$ and $(Y,o_2,t_2)$, a
%relation $R\subseteq X \setproduct Y$ is a \emph{weighted bisimulation}
%if for all $(x,y)\in R$, it holds that:
%\begin{enumerate}
%\item $o_1(x)=o_2(y)$,
%\item $\forall a\in A$, $x'\in X$, $\sum_{x''\in
%R_Y(R_X(x'))}t_1(x)(a)(x'') = \sum_{y'' \in R_X(x')}t_2(y)(a)(y'')$,
%\item $\forall a\in A$, $y'\in Y$, $\sum_{y''\in
%R_X(R_Y(y'))}t_2(y)(a)(y'') = \sum_{x'' \in R_Y(y')}t_1(x)(a)(x'')$.
%\end{enumerate}
%\emph{Weighted Bisimilarity} (in symbols $\sim_w$) is defined as the
%largest weighted bisimulation.

\emph{Weighted bisimilarity} generalizes the abstract semantics of
several kind of probabilistic and stochastic systems. This has been
introduced by Buchholz in \cite{german} for weighted automata with a
finite state space. Here we extend that definition to (possibly
infinite-states) automata with \emph{finite branching}, i.e., those
$(X, <o,t>)$ such that for all $ x\in X, a\in A$, $t(x)(a)(x')\neq
0$ for finitely many $x'$. This will be needed in the sequel, when we model weighted automata coalgebraically, to ensure
that the final coalgebra exists (the final coalgebra can be thought of the universe of possible behaviours and will be used to provide semantics to each state of the automaton).

Hereafter we will always implicitly refer to weighted automata with
finite branching. Moreover, given an $x\in X$ and an equivalence
relation $R\subseteq X\setproduct X$ we will write $[x]_R$ to denote
the equivalence class of $x$ with respect to $R$.

\begin{definition}\label{def:bis}
Let $(X, <o,t>)$ be a weighted automaton. An equivalence relation
$R\subseteq X \setproduct X$ is a \emph{weighted bisimulation} if
for all $(x_1,x_2)\in R$, it holds that:
\begin{enumerate}
\item $o(x_1)=o(x_2)$,
\item $\forall a\in A$, $x'\in X$, $\sum_{x''
\in [x']_R}t(x_1)(a)(x'') = \sum_{x'' \in [x']_R}t(x_2)(a)(x'')$.
\end{enumerate}
\emph{Weighted bisimilarity} (in symbols $\sim_w$) is defined as the
largest weighted bisimulation.
\end{definition}

%The authors of \cite{german} first define a \emph{functional
%simulation} between $(Q_1,o_1,t_1)$ and $(Q_2,o_2,t_2)$ as a
%function $h\colon  Q_1 \to Q_2$ such as for all $q_1\in Q_1$\colon
%\begin{enumerate}
%\item $o_2(h(q_1))=o_1(q_1)$,
%\item $\forall a\in A$, $q_2\in Q_2$, $t_2(h(q_1),a,q_2)= \sum_{q'\in
%h^{-1}(q_2)}t_1(q_1,a,q')$.
%\end{enumerate}
%
%Then, given two weighted automata $(Q_1,o_1,t_1)$ and
%$(Q_2,o_2,t_2)$, $q_1\in Q_1$ is \emph{bisimilar} to $q_2\in Q_2$
%(in symbols $q_1 \sim_w q_2$) if there exists a weighted automata
%$(Q_3,o_3,t_3)$ and two functional simulation $h_1\colon  Q_1 \to Q_3$ and
%$h_2\colon  Q_2\to Q_3$ such that $h_1(q_1)=h_2(q_2)$.
%

For instance, the relation $R_h$ in Fig.\ref{fig:cospan} is a
weighted bisimulation.

%\begin{figure}
%\centering
%\epsfig{file=cospan, width=0.5\textwidth} \\
%\caption{From left to right, three weighted automata over
%$\mathbb{R}$\colon   $(S,o^1,t^1)$, $(Q,o',t')$ and $(T,o^2,t^2)$. The
%dashed arrows denotes the $W$-homomorphisms $h_1\colon  S \to Q$ and $h_2\colon  T
%\to Q$. These induce the relation $R=\{(s_1,t_1),\;
%(s_2,t_2),\;(s_2,t_3),\;(s_3,t_2),\;(s_3,t_3)\}$}\label{fig:cospan}
%\end{figure}

\begin{figure}[t]{%\small
\MediumPicture
%
%\hspace*{1.2cm}
\begin{center}
\VCDraw{ \HideGrid\HideFrame \ChgEdgeLabelScale{.8}
\begin{VCPicture}{(0,0)(10,3.2)}
% states
\State[x_1]{(0,1)}{A}
\State[x_2]{(4,2)}{B}\State[x_3]{(4,0)}{C}
%
\State[y_1]{(10,1)}{D}
%
% initial--final
\FinalR{s}{A}{1} \FinalR{e}{B}{1}\FinalL{e}{C}{1}  %\Final{C}
%
\FinalR{s}{D}{1}
% transitions
\VArcR{arcangle=-45,ncurv=1}{B}{A}{b,3}
\VArcL{arcangle=45,ncurv=1}{C}{A}{b,3}
%
%
%\EdgeR[.65]{B}{A}{b,3} \EdgeR[.65]{C}{A}{b,3}
%
\EdgeL[.5]{A}{B}{a,1} \EdgeR[.5]{A}{C}{a,1} \LoopW[0.5]{A}{a,1/b,3}
\LoopN[0.5]{B}{a,3} \LoopS[0.5]{C}{a,3}\LoopE[0.5]{D}{a,3/b,3}
%
%
\ChgEdgeLineStyle{dashed}
%
\Edge{A}{D} \VArcL{arcangle=-45,ncurv=1}{F}{D}{}
%
\VArcL{arcangle=30,ncurv=1}{B}{D}{}\VArcR{arcangle=-30,ncurv=1}{C}{D}{}
\end{VCPicture}
}
\end{center}
\vspace*{.4cm} \caption{The weighted automata $(X,<o_X,t_X>)$ and
$(Y,<o_Y,t_Y>)$ (from left to right). The dashed arrow denote the
$\fW$-homomorphism $h\colon  X \to Y$. This induces the equivalence
relation $R_h=X\setproduct X$ that equates all the states in
$X$.}\label{fig:cospan}}
\end{figure}

%\fi







% on the category $\mathbf{Set}$ of sets
%and functions.

%\cbox{FILIPPO: Here I introduce coalgebra for a generic category
%$\mathbf{C}$ and then I specialized to coalgebras on $\mathbf{Set}$
%and Polynomial functors. Maybe it is better to start directly by
%talking about coalgebras on $\mathbf{Set}$. In such a way, also the
%last paragraph, about bisimulation becomes more easy (bisimulations
%in other categories then in $\mathbf{Set}$ are slightly more
%complex)}

\bigskip

Now, we will show that weighted automata and weighted bisimilarity
can be suitably characterized through \emph{coalgebras}
\cite{Rutten00}.

We first recall some basic definitions about coalgebras. Given a
functor $\fG\colon  C \to C$ on a category $C$, a {\em
$\fG$-coalgebra} is an object $X$ in $C$ together with an arrow
$f\colon  X \to \fG X$. For many categories and functors, such a pair
$(X,f)$ represents a transition system, the \emph{type} of which is
determined by the functor $\fG$. Viceversa, many types of transition
systems (e.g., deterministic automata, labeled transition systems
and probabilistic transition systems) can be captured by a functor.

A {\em $\fG$-homomorphism\/} from a $\fG$-coalgebra $(X,f)$ to a
$\fG$-coalgebra $(Y,g)$ is an arrow $h\;\colon\; X \to Y$ preserving
the transition structure, {\em
i.e.}, such that %$g\comp h = G h \comp f$
the following diagram commutes.
%$\fG$-coalgebras and $\fG$-homomorphisms form the category $Coalg_\fG$.
\[
\xymatrix{X \ar[d]_{f}\ar[r]^{h}& Y\ar[d]^{g}\\
\fG X \ar[r]_{\fG h}& \fG Y}\ \ \ \ \ \ \ \ \
%\ \ g\comp h = \fG h \comp f
\]
%
A $\fG$-coalgebra $(\Omega,\omega)$ is said to be {\em final} if for
any $\fG$-coalgebra $(X,f)$ there exists a unique $\fG$-homomorphism
$\beh{-}{\fG}{X} \colon  X\to \Omega$. Final coalgebra can be viewed
as the universe of all possible $\fG$-\emph{behaviours}: the unique
homomorphism $\beh{-}{\fG}{X} \colon X\to \Omega$ maps every state
of a coalgebra $X$ to a canonical representative of its behaviour.
This provides a general notion of behavioural equivalence: two
states $x_1,x_2\in X$ are \emph{$\fG$-behaviourally equivalent}
($x_1 \approx_\fG x_2$) iff $\beh{x_1}{\fG}{X}=\beh{x_2}{\fG}{X}$
\footnote{Here we are implicitly assuming that $C$ is a
concrete category~\cite{ahs:abstract-concrete-cats}, i.e., there exists a faithful
functor $U\colon C \to Set$. By writing $x_1,x_2\in X$, we formally mean that $x_1,x_2\in UX$ and by $\beh{x_i}{\fG}{X}$, we mean $U(\beh{-}{\fG}{X})x_i$.}.

%\cbox{Describe the Final Coalgebra w.r.t. Minimization}

%DESCRITTA MEGLIO NELLA SEZIONE APPROPRIATA
%
%If $C$ has a \emph{final object} $1$ and the functor $\fG$ is
%\emph{accessible} \cite{}, then the final coalgebra is the limit of
%the \emph{terminal sequence}
%%
%$$1 \leftarrow \fG1 \leftarrow \fGG1 \leftarrow \dots$$
%%

The functors corresponding to many well known types of systems are
shown in \cite{Rutten00}. In this section we will show a functor
$\fW\colon  Set \to Set$ such that $\approx_\fW$ coincides with
weighted bisimilarity. In order to do that, we need to introduce the
\emph{field valuation functor}.

\begin{definition}[Field valuation
Functor]\label{def:monoidfunctor} Let $\mathbb K$ be a field. The
field valuation functor $\mathbb K^{-}_{\omega}\colon Set \to Set$
is defined as follows. For each set $X$, $\mathbb K^{X}_{\omega}$ is
the set of functions from $X$ to $\mathbb K$ with finite support.
For each function $h \colon X \to Y$, $\mathbb K^{h}_{\omega}\colon
\mathbb K^{X}_{\omega} \to \mathbb K^{Y}_{\omega}$ is the function
mapping each $\varphi \in \mathbb K^{X}_{\omega}$ into $\varphi^{h}
\in \mathbb K^{Y}_{\omega}$ defined, for all $y\in Y$, by
\begin{small}
$$\varphi^{h}(y)= \sum_{x' \in
h^{-1}(y)} \varphi(x') $$
\end{small}
\end{definition}
%
Note that the above definition employs only the additive monoid of
$\mathbb K$, i.e., the element $0$ and the $+$ operator. For this
reason, such functor is often defined in literature (e.g., in
~\cite{gumm}) for commutative monoids instead of fields.

We need two further ingredients. Given a set $B$, the functor
$B\setproduct -\colon Set \to Set$ maps every set $X$ into
$B\setproduct X$ and every function $f\colon  X\to Y$ into
$id_B\setproduct f\colon B\setproduct X\to B\setproduct Y$. Given a
finite set $A$, the functor $-^A\colon  Set \to Set$ maps $X$ into
$X^A$ and $f\colon  X\to Y$ into $f^A\colon  X^A \to Y^A$ (recall that $f^A$ is defined for all $\varphi\in X^A$ as $f^A(\varphi)=\lambda a \in A. f(\varphi (a))$).

Now, the functor corresponding to weighted automata with input
alphabet $A$ over the field $\K$ is $\fW=\K \setproduct
(\K_{\omega}^{-})^A\colon   Set \to Set$.
%
%
%that maps each set $Q$ into $\mathbb{S} \setproduct
%(\mathbb{S}_{\omega}^{Q})^A$ and each function $h\colonQ_1 \to Q_2$ into
%$(id_{\mathbb{S}}, (\mathbb{S}_{\omega}^{h})^A)\colon \mathbb{S} \setproduct
%(\mathbb{S}_{\omega}^{Q_1})^A \to \mathbb{S} \setproduct
%(\mathbb{S}_{\omega}^{Q_2})^A$.
%
%
%
%$=\mathbb{S} \setproduct (\mathbb{S}_{\omega}^{-})^A\colon $. For any set $Q$,
%$W(Q)$ is the cartesian product of $\mathbb{S}$ and
%$(\mathbb{S}_{\omega}^{Q})^A$.
Note that every function $f\colon  X\to \fW(X)$ consists of a pair
of functions $<o,t>$ with $o\colon  X\to \K$ and $t\colon  X \to
(\K_{\omega}^{X})^A$. Therefore any $\fW$-coalgebra $(X,f)$ is a
weighted automaton $(X,<o,t>)$ (and vice versa).
%. Conversely, only the
%weighted automata \emph{with finite branching} (i.e., those such
%that $\forall x\in X, a\in A$, $t(x)(a)(x')\neq 0$ for finitely many
%$x'$) can be represented as $W$-coalgebras, because all the
%functions in $\K_{\omega}^{X}$ must have finite support.
%\cbox{The restriction to finite branching is needed for having final
%coalgebra.}
%
\begin{proposition}[\cite{thesis}]
The functor $\fW$ has a final coalgebra.
\end{proposition}
\begin{proof}
By~\cite[Theorem 7.2]{GS01}, the fact that $\fW$ is bounded is enough to guarantee the existence of a final
coalgebra.
Intuitively, a functor $F$ is bounded by some cardinal nember $c$, if for all $F$-coalgebras $(X,f)$ and all states $x\in X$,
the set of states ``reachable'' from $x$ has cardinality smaller or equal than $c$. For instance the powerset functor
${\mathcal P}(-)$ is not bounded (and does not have final coalgebra~\cite{Rutten00}), while the \emph{finite}
powerset functor ${\mathcal P_\omega}(-)$ is bounded by $\omega$.
Also, the functor $\fW$ is bounded by $\omega$ because of the finite branching condition.
\end{proof}

In order to show that the equivalence induced by the final
$\fW$-coalgebra ($\approx_\fW$) coincides with weighted bisimilarity
($\sim_w$), it is instructive to spell out the notion of
$\fW$-homomorphism. A function $h\colon  X\to Y$ is a
$\fW$-homomorphism between weighted automata $(X,<o_X,t_X>)$ and
$(Y,<o_Y,t_Y>)$ if the following diagram commutes.
\[
\xymatrix@C=2cm{X \ar[d]_{<o_X,t_X>}\ar[r]^{h}& Y\ar[d]^{<o_Y,t_Y>}\\
\K \setproduct(\K_{\omega}^{X})^A
\ar[r]_{id\setproduct(\K_{\omega}^{h})^A}& \K
\setproduct(\K_{\omega}^{Y})^A}\ \ \ \ \ \ \ \ \
%\ \ g\comp h = G h \comp f
\]
This means that for all $ x \in X, y \in Y, a\in A$,
\begin{center}
$o_X(x)=o_Y(h(x))$ and $\sum_{x'\in
h^{-1}(y)}t_X(x)(a)(x')=t_Y(h(x))(a)(y)$. \end{center}
%

%
% BOX COMMENTATO
%
%
%\cbox{When $X$ and $Y$ are finite, we can fix an ordering
%$X=\{x_1,\dots,x_n\}$ and $Y=\{y_1,\dots,y_m\}$ and representing
%$h\colon  X\to Y$ as an $m\setproduct n$-matrix where $h(i,j)=1$ if
%$h(x_j)=y_i$ and $0$ otherwise. In this case, the commutativity of
%the diagram above can simply be riformulated by mean of matrix
%multiplication:
%$$o_1=o_2\setproduct h \qquad \forall a\in A, h\setproduct t^1_a=t^2_a\setproduct
%h$$}



%Consider a $W$-coalgebra $(X, <o_X,t_X>)$.
For every $\fW$-homomorphism $h\colon (X, <o_X,t_X>) \to (Y,
<o_Y,t_Y>)$, the equivalence relation $R_h=\{(x_1,x_2) \;|\;
h(x_1)=h(x_2)\}$ is a weighted bisimulation. Indeed, by the
properties of $\fW$-homomorphisms and by definition of $R_h$, for
all $(x_1,x_2) \in R_h$
\[o_X(x_1)=o_Y(h(x_1))=o_Y(h(x_2))=o_X(x_2)\] and for all $ a \in A$, for all $ y \in Y$
%
\[\sum_{x''\in
h^{-1}(y)}t_X(x_1)(a)(x'')=t_Y(h(x_1))(a)(y)=t_Y(h(x_2))(a)(y)=\sum_{x''\in
h^{-1}(y)}t_X(x_2)(a)(x'')\text{.} \] Trivially, the latter implies
that for all $ x' \in X$
\[\sum_{x''\in [x']_{R_h}}t_X(x_1)(a)(x'')=\sum_{x'' \in [x']_{R_h}}t_X(x_2)(a)(x'')\text{.} \]

For an example look at the function $h$ depicted by the dotted
arrows in Fig. \ref{fig:cospan}: $h$ is a $\fW$-homomorphism and
$R_h$ is a weighted bisimulation.

Conversely, every bisimulation $R$ on $(X, <o_X,t_X>)$ induces the
coalgebra $(X/R, <o_{X/R},t_{X/R}>)$ where $X/R$ is the set of all
equivalence classes of $X$ w.r.t. $R$ and $o_{X/R}\colon X/R \to \K$
and $t_{X/R}\colon X/R \to (\K^{X/R}_{\omega})^A$ are defined for
all $x_1,x_2\in X$, $a\in A$ by
\[o_{X/R}([x_1]_R)=
o_X(x_1) %(note that it is well defined, since $\forall x_2\in
%\{x_1\}_R$, $o_X(x_1)=o_X(x_2)$)
\;\;\;\;\;\;\;\; t_{X/R}([x_1]_R)(a)([x_2]_R)=\sum_{x'\in
[x_2]_R}t_X(x_1)(a)(x')\text{.}\]
%(note that is well defined since $x_2\in \{x_1\}_R$, $\sum_{y'\in
%\{y\}_R}t_X(x_1)(a)(y'))=\sum_{y'\in \{y\}_R}t_X(x_2)(a)(y')$.
Note that both $o_{X/R}$ and $t_{X/R}$ are well defined (i.e.,
independent from the choice of the representative) since $R$ is a
weighted bisimulation. Most importantly, the function
$\varepsilon_R\colon  X \to X/R$ mapping $x$ into $[x]_R$ is a
$\fW$-homomorphism.
\[
\xymatrix@C=2cm{X \ar[d]_{<o_X,t_X>} \ar@{.>}@(ur,ul)[rr]|{\beh{-}{\fW}{X}} \ar[r]^{\varepsilon_R}& X/R\ar[d]|{<o_{X/R},t_{X/R}>} \ar@{.>}[r]^{\beh{-}{\fW}{X/R}} & \Omega \ar[d]^{\omega}\\
\fW(X) \ar@{.>}@(d,d)[rr]|{\fW(\beh{-}{\fW}{X})}
\ar[r]_{\fW(\varepsilon_R)}& \fW(X/R)
\ar@{.>}[r]_{\fW(\beh{-}{\fW}{X/R})}& \fW(\Omega)}
%\ \ g\comp h = G h \comp f
\]



\begin{theorem}\label{theo:bis}
Let $(X,<o,t>)$ be a weighted automaton and let $x_1,x_2$ be two
states in $X$. Then, $x_1\sim_w x_2$ iff $x_1\approx_\fW x_2$, i.e.,
$\beh{x_1}{\fW}{X}= \beh{x_2}{\fW}{X}$.
\end{theorem}
\begin{proof}
The proof follows almost trivially from the above observations.

%Let $(X, <o_X,t_X>)$ be a $\fW$-coalgebra.
%
If $x_1\approx_\fW x_2$, i.e.,
$\beh{x_1}{\fW}{X}=\beh{x_2}{\fW}{X}$, then $(x_1,x_2)\in
R_{\beh{-}{\fW}{X}}$ and $R_{\beh{-}{\fW}{X}}$ is a weighted
bisimulation because $\beh{-}{\fW}{X}$ is a $\fW$-homomorphism. Thus
$x_1\sim_w x_2$

If $x_1 \sim_w x_2$, then there exists a weighted bisimulation $R$
such that $(x_1,x_2)\in R$. Let $(X/R, <o_{X/R},t_{X/R}>)$ and
$\varepsilon_R\colon  X \to X/R$ be the $\fW$-coalgebra and the
$\fW$-homomorphism described above. Since there exists a unique
$\fW$-homomorphism from $(X, <o_X,t_X>)$ to the final coalgebra,
then $\beh{-}{\fW}{X} =
 \beh{-}{\fW}{X/R} \comp \varepsilon_R$. Since
$\varepsilon_R(x_1)=\varepsilon_R(x_2)$, then
$\beh{x_1}{\fW}{X}=\beh{x_2}{\fW}{X}$, i.e., $x_1 \approx_\fW x_2$.
\end{proof}

\subsection{Weighted language equivalence}
The semantics of weighted automata can also be defined in terms of
\emph{weighted languages}. A weighted language over $A$ and $\K$ is
a function $\sigma\colon  A^* \to \K$ assigning to each word in
$A^*$ a weight in $\K$. For each \textsc{wa} $(X,<o,t>)$, the
function $l_X\colon X \to \K^{A^*}$ assigns to each state $x \in X$
its recognized weighted language. For all words $a_1\dots a_n \in
A^*$, it is defined by
$$l_X(x)(a_1\dots a_n)=\sum \{k_1 \cdot  \stackrel{}{\dots} \cdot\,
k_n \cdot k \mid x=x_1\tr{a_1,k_1}\dots \tr{a_n,k_n}x_n \text{ and
}o(x_n)=k\}\text{.}$$
%It is easy to see that the above definition generalizes the one of
%accepted language of non-deterministic automata. More generally, non
%deterministic automata can be though as weighed automata over the
%semiring $2$ as well as languages can be though as weighted
%languages over $2$.
We will often use the following characterization: for all $ w \in
A^{\star}$,
$$l_X(x)(w)= \left\{%
\begin{array}{ll}
    o(x), & \hbox{if $w=\epsilon$;} \\
    \sum_{x'\in X}(t(x)(a)(x')\cdot l_X(x')(w')), & \hbox{if $w=aw'$.} \\
\end{array}%
\right.$$

Two states $x_1, x_2\in X$ are said to be \emph{weighted language
equivalent} (denoted by $x_1\sim_l x_2$) if $l_X(x_1)=l_X(x_2)$. In
\cite{german}, it is shown that if two states are weighted bisimilar
then they are also weighted language equivalent. For completeness,
we recall here the proof.
\begin{figure}[t]{%\small
\MediumPicture
%
%\hspace*{1.2cm}
\begin{center}
\VCDraw{ \HideGrid\HideFrame \ChgEdgeLabelScale{.8}
\begin{VCPicture}{(0,0)(3.5,2)}
% states
\State[x_1]{(0,1)}{A} \State[x_2]{(1.75,2)}{B}
\State[x_3]{(3.5,2)}{C}\State[x_4]{(1.75,0)}{D}\State[x_5]{(3.5,0)}{E}
% initial--final
\FinalR{s}{A}{0} \FinalR{s}{B}{0}\FinalR{s}{C}{2}  %\Final{C}
\FinalR{s}{D}{0} \FinalR{s}{E}{0}

% transitions
\EdgeL[.65]{A}{B}{a,1} \EdgeL[.5]{B}{C}{a,1} \EdgeR[.65]{A}{D}{a,1}
\EdgeR[.5]{D}{E}{a,1}
%
\end{VCPicture}
\hspace*{1.5cm}
\begin{VCPicture}{(0,0)(3.5,2)}
% states
\State[y_1]{(0,1)}{A} \State[y_2]{(1.75,1)}{B}
\State[y_3]{(3.5,2)}{C}\State[y_5]{(3.5,0)}{E}
% initial--final
\FinalR{s}{A}{0} \FinalR{s}{B}{0}\FinalR{s}{C}{2}  %\Final{C}
 \FinalR{s}{E}{0}

% transitions
\EdgeL[.5]{A}{B}{a,1} \EdgeL[.65]{B}{C}{a,1} \EdgeR[.65]{B}{E}{a,1}
%
\end{VCPicture}
\hspace*{1.5cm}
\begin{VCPicture}{(0,0)(3.5,2)}
% states
\State[z_1]{(0,1)}{A} \State[z_2]{(1.75,1)}{B}
\State[z_3]{(3.5,1)}{C}
% initial--final
\FinalR{s}{A}{0} \FinalR{s}{B}{0}\FinalR{s}{C}{1}  %\Final{C}
% transitions
\EdgeL[.5]{A}{B}{a,1} \EdgeL[.5]{B}{C}{a,2}
%
\end{VCPicture}
\hspace*{1.5cm}
\begin{VCPicture}{(0,0)(3.5,2)}
% states
\State[u_1]{(0,1)}{A} \State[u_2]{(1.75,1)}{B}
\State[u_3]{(3.5,1)}{C}
% initial--final
\FinalR{s}{A}{0} \FinalR{s}{B}{0}\FinalR{s}{C}{2}  %\Final{C}
% transitions
\EdgeL[.5]{A}{B}{a,1} \EdgeL[.5]{B}{C}{a,1}
%
\end{VCPicture}
}
\end{center}
\vspace*{.2cm} \caption{The states $x_1,y_1,z_1$ and $u_1$ in the above automaton
recognize the language mapping $aa$ into $2$ and the other words
into $0$.
 Although they are all language equivalent, they are not bisimilar.}\label{fig:examplebislan}}
\end{figure}
%\fi




%
\begin{proposition}
$\sim_w \subseteq \sim_l$
\end{proposition}
\begin{proof}
We prove that if  $R$ is a weighted bisimulation, then for all
$(x_1,x_2)\in R$, $l_X(x_1)=l_X(x_2)$. We use induction on words
$w\in A^*$.

If $w=\epsilon$, then $l_X(x_1)(w)=o(x_1)$ and $l_X(x_2)(w)=o(x_2)$
and $o(x_1)=o(x_2)$ since $R$ is a weighted bisimulation.

If $w=aw'$, then $$l_X(x_1)(w) =\sum_{x'\in X}(t(x_1)(a)(x')\cdot
l_X(x')(w'))\text{.}$$ By induction hypothesis for all $ x''\in
[x']_R$, $l_X(x'')(w')=l_X(x')(w')$. Thus in the above summation we
can group all the states $x''\in [x']_R$ as follows.
%
$$l_X(x_1)(w) =\sum_{[x']_R\in X/R}\left( \left(\sum_{x''\in [x']_R}
t(x_1)(a)(x'') \right) \cdot l_X(x')(w')  \right)$$
%
Since $(x_1,x_2)\in R$ and $R$ is a weighted bisimulation, the above
summation is equivalent to%$\forall x'$, $\sum_{x''\in \{x'\}_R}
%t(x_1)(a)(x'')=\sum_{x''\in \{x'\}_R} t(x_2)(a)(x'')$.
$$\sum_{[x']_R\in X/R}\left( \left(\sum_{x''\in [x']_R}
t(x_2)(a)(x'') \right) \cdot l_X(x')(w')  \right)$$ that, by the previous arguments, is
equal to $l_X(x_2)(w)$.
\end{proof}

The inverse inclusion does not hold: the states $x_1,y_1,z_1$ and $u_1$ in
Fig.\ref{fig:examplebislan} are language equivalent but they are not
equivalent according to weighted bisimilarity.
%
%In the next section, we will provide a characterization of $\sim_l$
%by mean of coalgebras on $Vect$, the category of vector spaces and
%linear maps.

%\begin{figure}
%\centering
%\begin{tabular}{c|c|c|c}
%\epsfig{file=A, width=0.2\textwidth} & \epsfig{file=B,
%width=0.2\textwidth} & \epsfig{file=C, width=0.2\textwidth} &
%\epsfig{file=D, width=0.2\textwidth}\\
%(A) & (B) & (C) & (D)
%\end{tabular}
% \caption{The states $s_1$ and $t_1$ in (A),(B), (C) and (D) recognizes the language that maps $ab$ into $2$ and all the other words into $0$.
% Although they are language equivalent, they are not bisimilar.}\label{fig:examplebislan}
%\end{figure}
%
%\bigskip


%
%MOVED ABOVE
%
%The equivalences that we have introduced so far relates the states
%of a single automaton. They also induce notions of equivalence on
%  states of possibly distinct automata, this way: given $(X,<o_X,t_X>)$ and
%$(Y,<o_Y,t_Y>)$, the states $x\in X$ and $y\in Y$ are equivalent
%(according to a certain equivalence) if the they are equivalent in
%the automaton $(X+Y, <o_X+o_Y, t_X+t_Y>)$. In the case of $\sim_l$
%this amounts to say that $x\sim_l y$ iff $l_X(x)=l_Y(y)$ and, in the
%case of $\approx_{\fW}$, to say that $x\approx_{\fW}y$ iff
%$\beh{x}{\fW}{X}=\beh{y}{\fW}{Y}$.

\subsection{On the difference between $\fW$-bisimilarity and $\fW$-behavioural equivalence}\label{sec:difference}

\begin{figure}[t]{%\small
\MediumPicture
%
%\hspace*{1.2cm}
\begin{center}
\VCDraw{ \HideGrid\HideFrame \ChgEdgeLabelScale{.8}
\begin{VCPicture}{(0,0)(9,2)}
% states
\State[x_1]{(0,1)}{A}
\State[x_2]{(1.75,2)}{B}\State[x_3]{(1.75,0)}{C}
%
\State[z_1]{(4,1)}{D}\State[z_2]{(5,1)}{E}
%
\State[y_1]{(7.25,1)}{F}

% initial--final
\FinalR{s}{A}{0} \FinalR{s}{B}{1}\FinalR{s}{C}{1}  %\Final{C}
%
\FinalR{s}{D}{0} \FinalR{s}{E}{1}
%
\FinalR{s}{F}{0}

% transitions
\EdgeL[.65]{A}{B}{a,1} \EdgeR[.65]{A}{C}{a,-1}
%
%\EdgeL[.65]{F}{G}{a,2} \EdgeR[.65]{F}{H}{a,-2}
%
\ChgEdgeLineStyle{dashed}
%
\Edge{A}{D} \VArcL{arcangle=-45,ncurv=1}{F}{D}{}
%
\VArcL{arcangle=30,ncurv=1}{B}{E}{}\VArcR{arcangle=-30,ncurv=1}{C}{E}{}
%\VArcL{arcangle=-30,ncurv=1}{G}{E}{}
%\VArcL{arcangle=30,ncurv=1}{H}{E}{}
\end{VCPicture}
}
\end{center}
\vspace*{.2cm} \caption{From left to right, three weighted automata
over $\mathbb{R}$: $(X,<o_X,t_X>)$, $(Z,<o_Z,t_Z>)$ and
$(Y,<o_Y,t_Y>)$. The dashed arrows denotes the $\fW$-homomorphisms
$h_1\colon  X \to Z$ and $h_2\colon  Y \to Z$. The states $x_1$ and
$y_1$ are behaviourally equivalent, but they are not
$\fW$-bisimilar.}\label{fig:cexample}}
\end{figure}


We conclude this section with an example showing the difference
between $\fW$-behavioral equivalence (and hence weighted bisimulation)
and another canonical equivalence notion from the theory of coalgebra,
namely $\fW$-bisimulation. This result is not needed for understanding
the next sections, and  therefore this sub-section can be safely skipped.
%
%In \cite{gumm}, it shown that the monoidal evaluation functor
%preserves weak pullbacks only under certain conditions. Weighted
%automata are one of the few (at our knowledge, the only) interesting
%example where the endofunctor does not preserve weak pullback. In
%order to have a concrete feeling consider the morphism $h_1:X\to Z$
%in Fig. \ref{fig:cospan}. The state $x_1$ is itself a subcoalgebra
%(Def.xxx in \cite{Rutten00}), while the state $x_1$ is not (the
%smallest subcoalgebra containing $x_1$ also contains $x_2$ and
%$x_3$). This proves that $\fW$ does not preserves weak pullbacks.
%Indeed, Theorem xxx in \cite{Rutten00} guarantees that whenever the
%endofunctor $G$ preserves weak pullback, then for all
%$G$-homomorphisms $h\colonX\to Y$, the inverse image of a subcoalgebra of
%$Y$ is a subcoalgebra of $X$.

The theory of coalgebras provides an alternative definition of
equivalence, namely $\fG$-bisimilarity ($\simeq_\fG$), that
coincides with $\fG$-behavioural equivalence whenever the functor
$\fG$ preserves \emph{weak pullbacks} \cite{Rutten00}. In the case
of weighted automata, the functor $\fW$ does not preserve weak
pullbacks and $\simeq_\fW$ is strictly included into $\approx_\fW$.
%
Since weighted automata are one of the few interesting cases where
this phenomenon arises, we now show an example of two states that
are in $\approx_\fW$, but not in $\simeq_\fW$ (the paper~\cite{Gumm09}
was of great inspiration for the construction of this example).

%\footnote{In order to better explain the example, we consider the
%extended definition $\approx_\fW$ that relates the states of
%different automata as described above. This is equivalent to say
%that the states $x\in X$ and $y\in Y$ of the weighted automata
%$(X,<o_X,t_X>)$ and $(Y,<o_Y,t_Y>)$ are $\fW$-behavioural equivalent
%if and only if $\beh{x}{\fW}{X}=\beh{y}{\fW}{Y}$.}



%\cbox{FILIPPO: Jan was suggesting to insert this definition after
%Definition 1. However, in my opinion it is not a good idea, since
%the two definitions do not coincides. In my opinion, this last part
%of the section (that could also become a subsection) should be
%interesting mainly for those people quite knowledgeable in
%coalgebras.}
%
First, let us instantiate the general coalgebraic definition of
bisimulation and bisimilarity to the functor $\fW$. A
\emph{$\fW$-bisimulation} between two $\fW$-coalgebras
$(X,<o_X,t_X>)$ and $(Y,<o_Y, t_Y>)$ is a relation $R\subseteq
X\setproduct Y$ such that there exists $<o_R,t_R>\colon R\to \fW(R)$
making the following diagram commute. The largest $\fW$-bisimulation
is called $\fW$-bisimilarity ($\simeq_\fW$).

%
\[
\xymatrix@C=1.5cm{X \ar[d]|{<o_X,t_X>}& R \ar[l]_{\pi_1} \ar[d]|{<o_R,t_R>} \ar[r]^{\pi_2}& Y\ar[d]|{<o_Y,t_Y>}\\
\fW(X) & \fW(R) \ar[l]^{\fW(\pi_1)} \ar[r]_{\fW(\pi_2)}& \fW(Y)}\ \
\ \ \ \ \ \ \
%\ \ g\comp h = G h \comp f
\]
%
Note that the actual definition of $\approx_{\fW}$ relates the states
of a single automaton. We can extend it in order to relate
states of possibly distinct automata: given $(X,<o_X,t_X>)$ and
$(Y,<o_Y,t_Y>)$, the states $x\in X$ and $y\in Y$ are equivalent w.r.t. $\approx_{\fW}$ iff
$\beh{x}{\fW}{X}=\beh{y}{\fW}{Y}$.


Consider now the coalgebras in Fig.\ref{fig:cexample}: $x_1
\approx_\fW y_1$, but $x_1 \not \simeq_\fW y_1$. For the former, it
is enough to observe that the function $h_1$ and $h_2$ (represented
by the dashed arrows) are $\fW$-homomorphisms, and by uniqueness of
$\beh{-}{\fW}{}$:
$\beh{x_1}{\fW}{X}=\beh{h_1(x_1)}{\fW}{Z}=\beh{z_1}{\fW}{Z}=\beh{h_2(y_1)}{\fW}{Z}=\beh{y_1}{\fW}{Y}$.
For $x_1 \not \simeq_\fW y_1$, note that there exists no $R\subseteq
X \setproduct Y$ that is a $\fW$-bisimulation and such that
$(x_1,y_1)\in R$. Since $x_2$ and $x_3$ have different output values than
$y_1$, then neither $(x_2,y_1)$
nor $(x_3,y_1)$ can belong to a bisimulation. Thus the only
remaining relation on $X\times Y$ is the one equating just $x_1$ and
$y_1$, i.e., $R=\{(x_1,y_1)\}$. But this is not a $\fW$-bisimulation
since there exists no $<o_R,t_R>$ making the leftmost square of the
above diagram commute. In order to understand this fact, note that
$\pi_1^{-1}(x_2)=\emptyset$ and $\pi_1^{-1}(x_3)=\emptyset$. Thus
for all possible choices of $<o_R,t_R>$, the function
$\fW(\pi_1)\circ <o_R, t_R>$ maps $(x_1,y_1)$ into a pair
$<k,\varphi>$ where $\varphi(a)(x_2)=0$ and $\varphi(a)(x_3)=0$. On
the other side of the square, we have that $<o_X,t_X> \circ
\pi_1(x_1,y_1)= <o_X(x_1),t_X(x_1)>$ and $t_X (x_1)(a)(x_2)=1$ and
$t_X (x_1)(a)(x_3)=-1$.

%
%Suppose ad absurd that there exists a $<o_R, t_R>$ such that
%$W(\pi_1)\circ <o_R, t_R>=<o_X,t_X> \circ \pi_1$. Then
%$W(\pi_1)\circ <o_R, t_R>(x_1,y_1)=<o_X(x_1),t_X(x_1)>$ $<o_X,t_X>
%\circ \pi_1(x_1,y_1)= <o_X(x_1),t_X(x_1)>$






\section{Linear Weighted Automata as Linear Coalgebras}\label{sec:linearcoalg}

In this section, we will introduce linear weighted automata as
coalgebras for an endofunctor $\fL\colon Vect\to Vect$, where $Vect$
is the category of vector spaces and linear maps over a field $\K$.
The goal of this approach is to characterize weighted language
equivalence as the behavioural equivalence induced by the final
$\fL$-coalgebra.

\subsection{Preliminaries}\label{sec:preliminaries}
First we fix some notations and recall some basic facts on vector
spaces and linear maps. We use $v_1,v_2,\dots$ to range over vectors
and $V,W\dots $ to range over vector spaces on a field $\K$. Given a
vector space $V$, a vector $v\in V$ and a $k\in \K$, the scalar
product is denoted by $k \cdot v$ (or $k v$ for short).
%
The \emph{space spanned} by an $I$-indexed family of vectors
$B=\{v_i\}_{i\in I}$ is the space $\Span(B)$ of all $v$ such that
$$v=k_1v_{i_1} + k_2v_{i_2} + \dots + k_nv_{i_n}$$
where %all the $k_i$ are scalar (i.e., $k_i\in \K$)
%and
for all $j$, $v_{i_j}\in B$. In this case, we say that $v$ is a
\emph{linear combination} of the vectors in $B$. A set of vectors is
\emph{linearly independent} if none of its elements can be expressed
as the linear combination of the remaining ones. A \emph{basis} for
the space $V$ is a linearly independent set of vectors that spans
the whole $V$. All the basis of $V$ have the same cardinality which
is called the \emph{dimension} of $V$ (denoted by $dim(V)$). If
$(v_1, \dots v_n)$ is a basis for $V$, then each vector $v\in V$ is
equal to $k_1v_1 + \dots + k_nv_n$ for uniquely determined $k_1, \dots, k_n \in
\K$. For this reason, each vector $v$ can be represented as a
$n\setproduct 1$-column vector
$$v=\left(
\begin{array}{c}
k_1\\
\vspace{-0.3cm}. \\
\vspace{-0.3cm}
. \\
%\vspace{-0.3cm}
. \\
k_n
\end{array}
\right)$$
%
We use $f,g,\dots$ to range over linear maps. Identity and
composition of maps
are denoted as usual.
%
%
%Given finite dimensions vectors spaces $V$ and
%$W$, a linear maps $f\colon V\to W$ can be represented as a matrix. Let
%$(v_1, \dots v_n)$ and $(w_1\dots w_m)$ be basis for, respectively,
%$V$ and $W$, $f$ is the $m\setproduct n$-matrix having as $i$-column the
%the $m\time1$-vector $f(v_i)$ expressed in $(w_1\dots w_m)$.
%
%
%
If $B_V=(v_1, \dots v_n)$ and $B_W=(w_1\dots w_m)$ are,
respectively, the basis of the vector spaces $V$ and $W$, then
every linear map $f\colon V\to W$ can be represented as an
$m\setproduct n$-matrix. Indeed, for each $v\in V$, $v=k_1v_1 +
\dots +k_nv_n$ and $f(v)=k_1f(v_1) + \dots +k_nf(v_n)$, by linearity
of $f$. For each $v_i$, $f(v_i)$ can be represented as $m\setproduct
1$ column vector by taking as basis $B_W$. Thus the matrix
corresponding to $f$ (w.r.t. $B_V$ and $B_W$) is the one having as
$i$-th column the vector corresponding to $f(v_i)$. In this paper we
will use capital letters $F,G \dots$ to denote the matrices
corresponding to linear maps $f,g \dots$ in lower case. By
multiplying the matrix $F$ with vector $v$ (in symbols,
$F\matrixproduct v$) we can compute $f(v)$. More generally, matrix
multiplication corresponds to composition of linear maps, in
symbols: $$g\comp f = G\matrixproduct F$$
%
The product of two vector spaces $V,W$ is written as $V \vectproduct
W$, and the product of two linear maps $f_1,f_2$ is $f_1\vectproduct
f_2$, defined as for functions. It will be clear from the context
whether $\times$ refer to multiplication of matrix or product of
spaces (or maps). Given a set $X$, and a vector space $V$, the set
$V^X$ (i.e., the set of functions $\varphi\colon X \to V$) carries a
vector space structure where sum and scalar product are defined
point-wise. Hereafter we will use $V^X$ to denote both the vector
space and the underlying carrier set. Given a linear map $f\colon
V_1 \to V_2$, the linear map $f^X\colon V_1^X \to V_2^X$ is defined
as for functions. If $A$ is a finite set we can conveniently think
$V^A$ as the product of $V$ with itself for $|A|$-times ($|A|$ is
the cardinality of $A$). A linear map $f\colon U\to V^A$ can be
decomposed in a family of maps indexed by $A$, $f=\{f_a\colon U \to
V\}_{a\in A}$, such that for all $ u \in U$, $f_a(u)=f(u)(a)$.

\medskip

For a set $X$, the set $\K_{\omega}^X$ (i.e., the set of all finite
support functions $\varphi\colon X\to\K$) carries a vector space structure
where sum and scalar product are defined in the obvious way. This is
called the \emph{free vector space} generated by $X$ and can be
thought of as the space spanned by the elements of $X$: each vector
$k_1x_{i_1} + k_2x_{i_2} + \dots + k_nx_{i_n}$ corresponds to a
function $\varphi\colon X\to\K$ such that $\varphi(x_{i_j})=k_{j}$
and for all $ x \notin \{x_{ij} \mid j = 1,\ldots, n\}$, $\varphi(x)=0$; conversely,
each finite support function $\varphi$ corresponds to a vector
$\varphi(x_{i_1}) x_i + \varphi(x_{i_2})x_{i_2} + \dots +
\varphi(x_{i_n})x_{i_n}$.

A fundamental property holds in the free vector space generated by
$X$: for all functions $f$ from $X$ to the carrier-set of a vector
space $V$, there exists a linear map $f^{\sharp}\colon\K_{\omega}^X
\to V$ that is called the \emph{linearization} of $f$. For all $\varphi\in
\K_{\omega}^X$, $\varphi=k_1x_{i_1} + k_2x_{i_2} + \dots + k_nx_{i_n}$ and
$f^\sharp(\varphi)=k_1f(x_{i_1}) + k_2f(x_{i_2}) + \dots + k_nf(x_{i_n})$.
\[
\xymatrix{\K_{\omega}^X \ar[rrd]^{f^{\sharp}}\\
X \ar[rr]_f \ar[u]^{\eta_X} & &V}
\]
Note that $f^{\sharp}$ is the only linear map such that
$f=f^{\sharp}\comp\eta_X$, where $\eta_X(x)$ is the function
assigning $1$ to $x$ and $0$ to all the other elements of $X$.

\medskip

The \emph{kernel} $\kernel(f)$ of a linear map $f\colon V \to W$ is
the subspace of $V$ containing all the vectors $v\in V$ such that
$f(v)=0$. The \emph{image} $\image(f)$ of $f$ is the subspace of $W$
containing all the $w\in W$ such that $w=f(v)$ for some $v\in V$.
If $V$ has finite dimension, the kernel and the image of $f$ are related by the following
equation:
\begin{eqnarray}\label{Eq:FunLin}
\dim(V)=\dim(\kernel(f))+\dim(\image(f))\,.
\end{eqnarray}
Given two vector spaces $V_1$ and $V_2$, their intersection $V_1
\cap V_2$ is still a vector space, while their union $V_1\cup V_2$
is not. Instead of union we consider the coproduct of vector spaces:
we write $V_1+V_2$ to denote the space $\Span(V_1\cup
V_2)$ (note that in the category of vector spaces, product and
coproduct coincide).



\subsection{From Weighted Automata to Linear Weighted Automata}\label{sec:fromlatolwa}
%
We have now all the ingredients to introduce linear weighted
automata and a coalgebraic characterization of weighted language
equivalence.

\begin{definition}[\textsc{lwa}]\label{def:lwa}
%\mik{Introduced an explicit definition.  When do we need the assumption that the underlying v.s. has finite dimension? FIL: Only for the algorithms}
A \emph{linear weighted automaton} (\textsc{lwa}, for
short) with input alphabet $A$ over the field $\K$ is a coalgebra
for the functor $\fL=\K \vectproduct -^A\colon Vect \to Vect$.
\end{definition}

More
concretely \cite{Bor09}, a \textsc{lwa} is a triple $(V,<o,t>)$, where $V$ is a
vector space (representing the states space), $o\colon V \to \K$ is
a linear map associating to each state its output weight and
$t\colon V \to V^A$ is a linear map that for each input $a\in A$
associates a next state (i.e., a vector) in $V$. We will write $v_1
\tr{a} v_2$ for $t(v_1)(a)=v_2$.

The behaviour of linear weighted automata is expressed in terms of
weighted languages. The \emph{language recognized} by a vector $v\in
V$ of a \lwa\ $(V,<o,t>)$ is defined for all words $a_1 \dots a_n\in
A^*$ as $\beh{v}{\fL}{V}(a_1\dots a_n)=o(v_n)$ where $v_n$ is the
vector reached from $v$ through $a_1\dots a_n$, i.e., $v \tr{a_1}
\dots \tr{a_n}v_n$.
%
We will often use the following (more compact) characterization:
for all $ w \in A^{\star}$, $$\beh{v}{\fL}{V}(w)= \left\{%
\begin{array}{ll}
    o(v), & \hbox{if $w=\epsilon$;} \\
    \beh{t(v)(a)}{\fL}{V}(w'), & \hbox{if $w=aw'$.} \\
\end{array}%
\right.$$
%
Here we use the notation $\beh{-}{\fL}{V}$ because this is the unique
$\fL$-homomorphism into the final $\fL$-coalgebra. In Section
\ref{sec:langeq}, we will provide a proof for this  fact and we will
also discuss the exact correspondence with the function $l_X$
introduced in Section \ref{sec:coalg}.

\medskip

Given a weighted automaton $(X,<o,t>)$, we can build a linear
weighted automaton $(\K_{\omega}^X,<o^{\sharp},t^{\sharp}>)$, where
$\K_{\omega}^X$ is the free vector space generated by $X$ and
$o^{\sharp}$ and $t^{\sharp}$ are the linearizations of $o$ and $t$.
If $X$ is finite, we can represent $t^{\sharp}$ and $o^\sharp$ by
the same matrices that we have introduced in the previous section
for $t$ and $o$. By fixing an ordering $x_1, \dots, x_n$ of the
states in $X$, we have a basis for $\K_{\omega}^X$, i.e., every
vector $v\in\K_{\omega}^X$ is equal to $k_1x_1+\dots + k_nx_n$ and
it can be represented as an $n\times 1$-column vector. The values
$t^{\sharp}(v)(a)$ and $o^{\sharp}(v)$ can be computed via matrix
multiplication as $T_a \matrixproduct v$ and $O \matrixproduct v$.


For a concrete example, look at the weighted automaton
$(X,<o_X,t_X>)$ in Fig. \ref{fig:cospan}. The corresponding linear
weighted automaton
$(\mathbb{R}_{\omega}^X,<o_X^{\sharp},t_X^{\sharp}>)$ has as state
space the space of all the linear combinations of the states in $X$
(i.e., $\{k_1 x_1 + k_2 x_2 +k_3 x_3 \mid
 k_i \in \mathbb{R}\}$).
%
%Given a vector $v=k_1 x_1 + k_2 x_2 +k_3 x_3$, we can can compute
%$o_X^{\sharp}(v)$ and $t_X^{\sharp}(v)(a)$ by matrix multiplication.
%
%\begin{center}
%$o_X^{\sharp}(v)= \left(%
%\begin{array}{ccc}
%  1 & 1 & 1 \\
%\end{array}\right)$
%\hspace{-0.2cm}
%$\left(%
%\begin{array}{c}
%  k_1\\
%  k_2\\
%  k_3\\
%\end{array}\right)$
%$t_X^{\sharp}(v)(a)=\left(
%\begin{array}{ccc}
%  1 & 0 & 0 \\
%  1 & 3 & 0 \\
%  1 & 0 & 3 \\
%\end{array}\right)$
%\hspace{-0.2cm} $\left(
%\begin{array}{c}
%k_1\\
%k_2\\
%k_3\\
%\end{array}
%\right)$
%$t_{X}^{\sharp}(v)(b)=\left(%
%\begin{array}{ccc}
%  3 & 3 & 3 \\
%  0 & 0 & 0 \\
%  0 & 0 & 0 \\
%\end{array}\right)$
%\hspace{-0.2cm} $\left(
%\begin{array}{c}
%k_1\\
%k_2\\
%k_3\\
%\end{array}
%\right)$
%%
%\end{center}
%
The function $o_X^{\sharp}$ maps $v=k_1 x_1 + k_2 x_2 +k_3 x_3$ into
$k_1 o_X(x_1) + k_2 o_X(x_2) + k_3 o_X(x_3)$, i.e., $k_1+k_2+k_3$.
By exploiting the correspondence between functions and vectors in
$\K_{\omega}^X$ (discussed in Section \ref{sec:preliminaries}), we
can write $t_X^{\sharp}(v)(a)= k_1 t_X(x_1)(a)+k_2 t_X(x_2)(a) +k_3
t_X(x_3)(a)$ that is $k_1(x_1 + x_2+ x_3)+k_23x_2+k_33x_3$ and
$t_X^{\sharp}(v)(b)=k_13x_1+k_23x_1+k_33x_1$. This can be
conveniently expressed in terms of matrix multiplication.
\begin{center}
$o_X^{\sharp}(v)= \left(%
\begin{array}{ccc}
  1 & 1 & 1 \\
\end{array}\right)$
\hspace{-0.2cm}
$\left(%
\begin{array}{c}
  k_1\\
  k_2\\
  k_3\\
\end{array}\right)$
$t_X^{\sharp}(v)(a)=\left(
\begin{array}{ccc}
  1 & 0 & 0 \\
  1 & 3 & 0 \\
  1 & 0 & 3 \\
\end{array}\right)$
\hspace{-0.2cm} $\left(
\begin{array}{c}
k_1\\
k_2\\
k_3\\
\end{array}
\right)$
$t_{X}^{\sharp}(v)(b)=\left(%
\begin{array}{ccc}
  3 & 3 & 3 \\
  0 & 0 & 0 \\
  0 & 0 & 0 \\
\end{array}\right)$
\hspace{-0.2cm} $\left(
\begin{array}{c}
k_1\\
k_2\\
k_3\\
\end{array}
\right)$
%
\end{center}


%\cbox{Filippo: Jan put two question marks on this paragraph. I do
%not know if I well understand them. Old Version:
%
%Conversely, given a linear weighted automaton $(V,<o,t>)$, we can
%turn it into a weighted automaton by choosing a basis
%$X=(x_1,x_2\dots)$ of $V$. The resulting weighted automaton is
%$(X,<o_X, t_X>)$ where $o_X \colon X \to \K$ and $t_X \colon X \to
%(\K_{\omega}^{X})^A$ are defined as $o$ and $t$ for all $x\in X$.
%Note that two weighted automata derived from the same \lwa\ but
%w.r.t. two different basis do not have isomorphic transitions,
%although they have the same number of states. As an example, look at
%the weighted automata in Fig. \ref{fig:isomorphic}. They represent
%the same \lwa\ but w.r.t. two different basis: the leftmost w.r.t.
%the basis $X=(x_1,x_2,x_3)$ and the rightmost w.r.t. the basis
%$Y=(x_1+x_2,x_2+x_3,x_3+x_1)$. The transitions and the output
%functions for the two automata are described by the following
%matrices.}

\medskip
A linear map $h\colon  V\to W$ is an $\fL$-homomorphism between
\lwa\ $(V,<o_V,t_V>)$ and $(W,<o_W,t_W>)$ if the following diagram
commutes.
\[
\xymatrix@C=1.5cm{V \ar[d]_{<o_V,t_V>}\ar[r]^{h}& W\ar[d]^{<o_W,t_W>}\\
\K \vectproduct V^A \ar[r]_{id\vectproduct h^A}& \K \vectproduct
W^A}\ \ \ \ \ \ \ \ \
%\ \ g\comp h = G h \comp f
\]
This means that for all $ v \in V, a\in A$, $o_V(v)=o_W(h(v))$ and
$h(t_V(v)(a))=t_W(h(v))(a)$.
%
If $V$ and $W$ have finite dimension, then we can represent all the
morphisms of the above diagram as matrices. In this case, the above
diagram commutes if and only if for all $ a \in A$,
$$O_V=O_W \matrixproduct H \qquad H \matrixproduct T_{V_a} = T_{W_a}\matrixproduct H$$
where $T_{V_a}$ and $T_{W_a}$ are the matrix representations of $t_V$
and $t_W$ for any $a\in A$.

For a function $h\colon X \to Y$, the function $\K^h_{\omega} \colon \K^X_{\omega} \to
\K^Y_{\omega}$ (formally introduced in
Definition \ref{def:monoidfunctor}) is a linear map. % for all $v=k_1x_{i_1} +
%k_2x_{i_2}+\dots k_nx_{i_n}$ as $\K^h(v)=
%k_1h(x_{i_1}) + k_2h(x_{i_2})+\dots k_nh(x_{i_n})$.
%Note that by looking at $v$ as a function $\phi:Y \to \K$,
%this definition corresponds to the one given to
Note that if $h$ is a $\fW$-homomorphism between the \wa\
$(X,<o_X,t_X>)$ and $(Y,<o_Y,t_Y>)$, then $\K^h_{\omega}$ is an
$\fL$-homomorphism between the \lwa\
$(\K^X_{\omega},<o_X^{\sharp},t_X^{\sharp}>)$ and
$(\K^Y_{\omega},<o_Y^{\sharp},t_Y^{\sharp}>)$.
%
For an example, look at the $\fW$-homomorphism $h\colon (X,
<o_X,t_X>) \to (Y, <o_Y,t_Y>)$ represented by the dotted arrows in
Fig. \ref{fig:cospan}. The linear map $\reals^h_{\omega} \colon \reals^X_{\omega} \to \reals^Y_{\omega}$ is
represented by the matrix $H=(1\;1\;1)$ and it is an
$\fL$-homomorphism between $(\reals^X_{\omega},<o_X^{\sharp},t_X^{\sharp}>)$ and
$(\reals^Y_{\omega},<o_Y^{\sharp},t_Y^{\sharp}>)$. This can be easily checked by
showing that $O_X=O_Y \matrixproduct H$, $H \matrixproduct T_{X_a} =
T_{Y_a}\matrixproduct H$ and $H \matrixproduct T_{X_b} =
T_{Y_b}\matrixproduct H$.

\medskip


\begin{figure}[t]{%\small
\MediumPicture
%
%\hspace*{1.2cm}
\begin{center}
\VCDraw{ \HideGrid\HideFrame \ChgEdgeLabelScale{.8}
\begin{VCPicture}{(0,-2)(6,3.5)}
% states
\State[x_1]{(3,2)}{A} \State[x_2]{(1.5,-0.5)}{B}
\State[x_3]{(4.5,-0.5)}{C}
% initial--final
\FinalR{e}{A}{1} \FinalR{e}{B}{1}\FinalR{e}{C}{{1}}  %\Final{C}
% transitions
%\EdgeL[.65]{B}{A}{a,1} \EdgeR[.65]{C}{A}{a,-1} \LoopS{B}{a,1}
\EdgeR[.65]{A}{B}{a,1} \EdgeL[.65]{A}{C}{a,-1} \LoopS[.5]{B}{a,1}
%
\LoopS[.5]{C}{a,1}
%
\end{VCPicture}
%\hspace*{2cm}
\begin{VCPicture}{(0,-2)(8,3.5)}
% states
\StateVar[x_1+x_2]{(4,2)}{A} \StateVar[x_2+x_3]{(1.5,-0.5)}{B}
\StateVar[x_1+x_3]{(6.5,-0.5)}{C}
% initial--final
\FinalR{e}{A}{2} \FinalR{e}{B}{2}\FinalR{e}{C}{{2}}  %\Final{C}
% transitions
\LoopVarN[.5]{A}{a,\frac 3 2}\ArcR[.75]{A}{B}{a,\frac 1 2}
\ArcL[.75]{A}{C}{a,-\frac 3 2} \LoopS[.5]{B}{a,1}
\LArcL[.5]{C}{B}{a,\frac 1 2}\ArcL[.75]{C}{A}{a,\frac 1
2}\LoopS[.5]{C}{a,- \frac 1 2}
%
\end{VCPicture}
} \vspace*{0.2cm}
\end{center}
\caption{The weighted automata $(X,<o_X, t_X>)$ (left) and $(Y,<o_{Y}, t_{Y}>)$ (right). The corresponding linear
weighted automata $(\reals_{\omega}^X,<o_X^{\sharp},
t_X^{\sharp}>)$ and $(\reals_{\omega}^Y,<o_{Y}^{\sharp}, t_{Y}^{\sharp}>)$
are isomorphic.}\label{fig:isomorphic}
%\vspace{2.5cm}
}
\end{figure}



Note that two different weighted automata can {\em represent} the same (up
to isomorphism) linear weighted automaton. As an example, look at
the weighted automata $(X,<o_X, t_X>)$ and $(Y,<o_{Y}, t_{Y}>)$ in
Fig.~\ref{fig:isomorphic}. They represent, respectively, the linear
weighted automata $(\reals_{\omega}^X,<o_X^{\sharp},
t_X^{\sharp}>)$ and $(\reals_{\omega}^Y,<o_{Y}^{\sharp}, t_{Y}^{\sharp}>)$
that are isomorphic. The transitions and the output functions for
the two automata are described by the following matrices.
%
\begin{center}
$T_{X_a}=\left(%
\begin{array}{ccc}
  0 & 0 & 0 \\
  1 & 1 & 0 \\
  -1 & 0 & 1 \\
\end{array}\right)$
$O_X= \left(%
\begin{array}{ccc}
  1 & 1 & 1 \\
\end{array}\right)$\hspace{1cm}
$T_{Y_a}=\left(%
\begin{array}{ccc}
  \frac{3}{2} & 0 & \frac{1}{2} \\
  \frac{1}{2} & 1 & \frac{1}{2} \\
  -\frac{3}{2} & 0 & -\frac{1}{2} \\
\end{array}\right)$
$O_{Y}= \left(%
\begin{array}{ccc}
  2 & 2 &2 \\
\end{array}\right)$
\end{center}
%
%\cbox{ Old Version:
%
%Note that $T_{X_a}$ and $T_{Y_a}$ are \emph{similar}, i.e., they
%represent the same linear map. This can be immediately checked by
%showing that $T_{Y_a} = j^{-1} \comp t_{X_a} \comp j$, where
%$j\colon \K^{Y} \to \K^{X}$ is the isomorphic map representing the
%change of bases and $j^{-1}\colon \K^{X} \to \K^{Y}$ is its inverse.
%Their matrix representation is the following.}
%
Note that $T_{X_a}$ and $T_{Y_a}$ are \emph{similar}, i.e., they
represent the same linear map. This can be immediately checked by
showing that $T_{Y_a} = j^{-1} \comp t_{X_a}\comp j$, where $j\colon
\reals^{Y} \to \reals^{X}$ is the isomorphic map representing the change of
bases from $Y=(x_1+x_2,x_2+x_3,x_3+x_1)$ to $X=(x_1,x_2,x_3)$ and
$j^{-1}\colon \reals^{X} \to \reals^{Y}$ is its inverse. The matrix
representation of $j$ and $j^{-1}$ is the following.

%
\begin{center}
$J=\left(%
\begin{array}{ccc}
  1 & 0 & 1 \\
  1 & 1 & 0 \\
  0 & 1 & 1 \\
\end{array}\right)$
\hspace{1cm}
$J^{-1}=\left(%
\begin{array}{ccc}
  \frac{1}{2} & \frac{1}{2} & -\frac{1}{2} \\
  -\frac{1}{2} & \frac{1}{2} & \frac{1}{2} \\
  \frac{1}{2} & -\frac{1}{2} & \frac{1}{2} \\
\end{array}\right)$
\end{center}
Also $O_X$ and $O_{Y}$ represent the same map in different bases.
Indeed,  $O_{Y} = O_X \matrixproduct J$.

At this point, it is easy to see that the linear isomorphism
$j^{-1}\colon \reals^X \to \reals^{Y}$ is an $\fL$-homomorphism, because
$O_{X}=O_{X}\matrixproduct J \matrixproduct J^{-1}= O_{Y}
\matrixproduct J^{-1}$ and $J^{-1} \matrixproduct T_{X_a} = J^{-1}
\matrixproduct T_{X_a}\matrixproduct J \matrixproduct J^{-1}=
T_{Y_a}\matrixproduct J^{-1}$. Analogously for $j\colon \reals^Y \to
\reals^{X}$.




%\cbox{Old Version:
%
%The converse does not hold: not all $\fL$-homomorphisms $h\colon V_1
%\to V_2$ have a corresponding $W$-homomorphism. As an example
%consider the \lwa\ $(\K^X,<o_X^{\sharp}, t_{X}^{\sharp}>)$ and
%$(\K^{Y},<o_{Y}^{\sharp}, t_{Y}^{\sharp}>)$
%corresponding to the \wa\ in Fig. \ref{fig:isomorphic}. %The matrixes
%%corresponding to their output and transitions function have been
%%shown above and the two linear weighted automata are the same up to
%%isomorphism.
%Note that the linear map $j^{-1}\colon \K^X \to \K^{Y}$ is an
%$\fL$-homomorphism, because $O_{Y} \matrixproduct J^{-1}=
%O_{X}\matrixproduct J \matrixproduct J^{-1}=O_{X}$ and
%$T_{Y_a}\matrixproduct J^{-1} = J^{-1} \matrixproduct T_{X_a}
%\matrixproduct J \matrixproduct J^{-1}=J^{-1} \matrixproduct
%T_{X_a}$. However $J^{-1}$ is not a $\fW$-homomorphism since it
%cannot be seen as a function from $X$ to $Y$.}








%
%OLD VERSION
%
%\subsection{From Weighted Automata to Linear Weighted Automata}
%For a set $X$, the set $\K_{\omega}^X$ (i.e., the set of all
%finite support functions $\varphi\colon X\to\K$) carries a
%vector space where sum and scalar product are defined in the obvious
%way. We will use $\K_{\omega}^X$ to denote both such vector
%space and the underlying set and we will say that
%$\K_{\omega}^X$ is the \emph{free vector space} generated by
%$X$. Given a function $f\colon X\to Y$, the function
%$\K_{\omega}^f\colon \K_{\omega}^X \to
%\K_{\omega}^Y$ (defined as in Def. \ref{def:monoidfunctor})
%is a linear map. In this way, we can define the functor
%$\K_{\omega}^-\colon Set \to Vect$.
%
%The functor $U\colon Vect \to Set$ maps each vector space in the
%underlying carrier set and every linear map in the underlying
%function. We will often write $V$ for $U(V)$ and $f$ for $U(f)$
%since we often identify vector spaces and linear maps with the
%underlying sets and functions. By composing
%$\K_{\omega}^-\colon Set \to Vect$ with $U\colon Vect \to
%Set$, we get the functor $\K_{\omega}^-\colon Set \to Set$
%that we have previously defined in Def. \ref{def:monoidfunctor}.
%
%It is important to note that these functors give rise to the
%following adjunction.
%$$\xymatrix{Set \ar@(ur,ul)[rr]^{\K_{\omega}^-}& \bot &
%Vect\ar@(dl,dr)[ll]^{U}}$$ For each set $X$, there exists a function
%$\eta_X\colon X \to \K_{\omega}^X$ defined $\forall x,x' \in
%X$, as $\eta_X(x)(x')=1$ if $x'=x$, and $\eta_X(x)(x')=0$ otherwise.
%The family of functions $\eta=\{\eta_X \mid X \text{ is a set}\}$ is
%\emph{the unit} of the above adjunction and it enjoys the following
%important property: for all functions $f\colon X\to U(V)$ (for
%$U(V)$ being the carrier set of a vector space $V$), there exists a
%unique linear map $f^{\sharp}\colon \K_{\omega}^X \to V$
%such that the following leftmost diagram commutes.
%$$\xymatrix{X \ar[d]_{f} \ar[r]^{\eta_X} & \K_{\omega}^X \ar[dl]^{f^{\sharp}} & &
% \K_{\omega}^X\ar[d]_{f^{\sharp}} \\
%U(V) & & & V}$$
%%
%Note that the rightmost diagram is in $Vect$, while the leftmost is
%in $Set$, and $\K_{\omega}^X=U(\K_{\omega}^X)$ and
%$f^{\sharp}=U(f^{\sharp})$ (by definition of $U$). We will say that
%$f^{\sharp}$ is the \emph{linearization} of $f$.
%
%\bigskip
%
%%A \emph{linear weighted automaton} (\textsc{lwa}, for short) with
%%input alphabet $A$ is a triple $(V,o,t)$, where $V$ is a vector
%%space (representing the states space), $o:V \to \K$ is a
%%linear map associating to each state its output weight and $t\colonV \to
%%V^A$ is a linear map that for each input $a\in A$ associates a next
%%state in $V$.
%
%
%We have now all the ingredients to develop our theory. A
%\emph{linear weighted automaton} (\textsc{lwa}, for short) with
%input alphabet $A$ over the field $\K$ is a coalgebra for
%the functor $L=\K \vectproduct id^A\colon  Vect \to Vect$. More
%concretely, a \textsc{lwa} is a triple $(V,o,t)$, where $V$ is a
%vector space (representing the states space), $o\colon V \to
%\K$ is a linear map associating to each state its output
%weight and $t\colon V \to V^A$ is a linear map that for each input
%$a\in A$ associates a next state (i.e., a vector) in $V$.
%
%
%Given a weighted automata $(X,o,t)$, we can build a linear weighted
%automata $(\K_{\omega}^X,o^{\sharp},t^{\sharp})$, where
%$\K_{\omega}^X$ is the free vector space generated by $X$,
%while $o^{\sharp}$ and $t^{\sharp}$ are the linearizations of $o$
%and $t$. When $X$ is finite, we can represent $t^{\sharp}$ and
%$o^\sharp$ by the same matrixes that we have introduced in the
%previous section for $t$ and $o$. Indeed, by fixing an ordering
%$x_1, \dots, x_n$ of the states in $X$, we have a basis for
%$\K_{\omega}^X$, i.e., every vector
%$v\in\K_{\omega}^X$ is equal to $k_1x_1+\dots +
%k_nx_n$ and it can be represented as an $n\times 1$-column
%vector. The values $t^{\sharp}(v)(a)$ and $o^{\sharp}(v)$ can be
%computed via matrix multiplication as $t_a \matrixproduct v$ and $o \matrixproduct
%v$.
%
%
%For a concrete example, look at the weighted automata $(Y,o_2,t_2)$
%in Fig. \ref{fig:cospan}. The corresponding linear weighted
%automaton $(\mathbb{R}_{\omega}^Y,o_2^{\sharp},t_2^{\sharp})$ has as
%state space, the space of all the linear combinations of the states
%in $Y$ (i.e., $\{k_1 y_1 + k_2 y_2 +k_3 y_3 \mid
% k_i \in \mathbb{R}\}$). The function $o_2^{\sharp}$
%maps $v=k_1 y_1 + k_2 y_2 +k_3 y_3$ into $k_1
%o_2(y_1) + k_2 o_2(y_2) + k_3 o_2(y_3)$, i.e.,
%$0+k_2+k_3$ while $t_2^{\sharp}(v)(a)= k_1
%t_2(y_1)(a)+k_2 t_2(y_2)(a) +k_3 t_2(y_3)(a)$ that is
%$k_1(2y_2-2y_3)+0+0$. This can be conveniently expressed in
%terms of matrix multiplication.
%\begin{center}
%$t_2^{\sharp}(v)(a)=\left(
%\begin{array}{ccc}
%  0 & 0 & 0 \\
%  2 & 0 & 0 \\
%  -2 & 0 & 0 \\
%\end{array}\right)$
%$\left(
%\begin{array}{c}
%k_1\\
%k_2\\
%k_3\\
%\end{array}
%\right)$
%\hspace{1cm}$o_2^{\sharp}(v)= \left(%
%\begin{array}{ccc}
%  0 & 1 & 1 \\
%\end{array}\right)$
%$\left(%
%\begin{array}{c}
%  k_1\\
%  k_2\\
%  k_3\\
%\end{array}\right)$
%\end{center}
%
%
%\cbox{FILIPPO:Insert example of morphism of Linear Weighted
%Automata}
%
%
%Moreover, given an $W$-homomorphism $h\colon (S,o_1,t_1) \to
%(T,o_2,t_2)$, we can build the $L$-homomorphism
%$\K_{\omega}^h\colon (\K_{\omega}^S,
%o_1^{\sharp},t_1^{\sharp}) \to
%(\K_{\omega}^T,o_2^{\sharp},t_2^{\sharp})$.
%
%\begin{proposition}
%Let $h\colon (X,o_1,t_1) \to (Y,o_2,t_2)$ be a $W$-homomorphism.
%Then $\K_{\omega}^h\colon (\K_{\omega}^X,
%o_1^{\sharp},t_1^{\sharp}) \to
%(\K_{\omega}^Y,o_2^{\sharp},t_2^{\sharp})$ is an
%$\fL$-homomorphism.
%\end{proposition}
%\begin{proof}
%Note that $\K_{\omega}^h$ is an $\fL$-homomorphism if the
%following rightmost diagram commutes in $Vect$.
%
%$$
%\xymatrix@R=10pt@C=10pt{ & \K_{\omega}^X \ar[dddl]_{f^{\sharp}} \ar[rr]^{\K_{\omega}^h}& & \K_{\omega}^Y \ar[dddl]^{g^{\sharp}}\\
%X \ar[rr]^h \ar[dd]_f \ar[ur]^{\eta_X} & & Y \ar[dd]^g \ar[ur]^{\eta_Y} \\
%\\
%\K \vectproduct (\K_{\omega}^X)^A \ar[rr]_{\K
%\vectproduct (\K_{\omega}^h)^A}& & \K \vectproduct
%(\K_{\omega}^Y)^A} \qquad \xymatrix@R=10pt@C=10pt{ &
%\K_{\omega}^X \ar[dddl]_{f^{\sharp}}
%\ar[rr]^{\K_{\omega}^h}& & \K_{\omega}^Y
%\ar[dddl]^{g^{\sharp}}
%\\ \\ \\ \K \vectproduct (\K_{\omega}^X)^A \ar[rr]_{\K \vectproduct
%(\K_{\omega}^h)^A}& & \K \vectproduct
%(\K_{\omega}^Y)^A}
%$$
%
%Note that the rightmost diagram commutes in $Vect$ if and only if
%the backward face of the leftmost diagram commutes in $Set$. Note
%that the top face and the front face commute because (1) $h$ is an
%$W$-homomorphism and (2) $\eta$ is a natural transformation. Thus
%$\K \setproduct (\K_{\omega}^h)^A \comp f^{\sharp}\comp
%\eta_X = g^{\sharp} \comp \K_{\omega}^h\comp \eta_X$. Since
%$\eta$ is the unit of the adjoint, then $\K \setproduct
%(\K_{\omega}^h)^A \comp f^{\sharp} = g^{\sharp} \comp
%\K_{\omega}^h$.
%\end{proof}
%
%By employing the above proposition, we can define a functor
%$LIN\colon Coalg_W \to Coalg_\fL$ that maps each weighted automaton in
%the corresponding linear one.


\subsection{Language equivalence and final
$\fL$-coalgebra}\label{sec:langeq}
%
We introduce the final $\fL$-coalgebra and we show that the
behavioural equivalence $\approx_\fL$, induced by the functor $\fL$, coincides with weighted
language
equivalence. %For each $\fL$-coalgebra $(V,<o,t>)$, the final morphism
%$\beh_V$ maps each state $v\in V$ in the weighted language that it
%recognizes. Thus $\fL$-behavioural equivalence coincides with language
%equivalence.

The set of all weighted languages $\K^{A^*}$ carries a vector space
structure: the sum of two languages $\sigma_1, \sigma_2\in\K^{A^*}$
is the language $\sigma_1 + \sigma_2$ defined for each word $w\in
A^*$ as $(\sigma_1 + \sigma_2)(w)=\sigma_1 (w)+ \sigma_2(w)$; the
product of a language $\sigma$ for a scalar $k\in \K$ is $k\sigma$
defined as $k\sigma(w)=k\cdot \sigma(w)$; the element $0$ of
$\K^{A^*}$ is the language mapping each word into the $0$ of $\K$.


The \emph{empty function} $\emp \colon \K^{A^*} \to \K$ and the
\emph{derivative function} $\der \colon \K^{A^*} \to (\K^{A^{*}})^A$
are defined for all $ \sigma \in \K^{A^*}$, $a\in A$ as
\[\emp(\sigma) =\sigma(\epsilon) \quad \der(\sigma)(a)=\sigma_a\]
where $\sigma_a: A^* \to \K$ denotes the \emph{$a$-derivative} of
$\sigma$ that is defined for all $w\in A^*$ as
$$\sigma_a(w)=\sigma(aw)\text{.}$$


\begin{proposition}\label{prop:linear}
The maps $\emp \colon \K^{A^*} \to \K$ and $\der \colon \K^{A^*} \to
(\K^{A^{*}})^A$ are linear.
\end{proposition}
\begin{proof}
We show the proof for $\der$. The one for $\emp$ is analogous.

Let $\sigma_1, \sigma_2$ be two weighted languages in $\K^{A^*}$.
Now for all $ a\in A, w \in A^*$,
$\der(\sigma_1+\sigma_2)(a)(w)=\sigma_1+\sigma_2(aw)=\sigma_1(aw)+\sigma_2(aw)=\der(\sigma_1)(a)(w)+\der(\sigma_2)(a)(w)$.

Let $k$ be a scalar in $\K$ and $\sigma$ be a weighted language in
$\K^{A^*}$. Now for all $ a\in A, w \in A^*$, $k \cdot
\der(\sigma)(a)(w)=k \cdot \sigma(aw)=\der(k \sigma)(a)(w)$.
\end{proof}

Since $\K^{A^*}$ is a vector space and since $\emp$ and $\der$ are
linear maps, $(\K^{A^*},<\emp,\der>)$ is an $\fL$-coalgebra. The
following theorem shows that it is final.

\begin{theorem}[finality]\label{prop:final}
From every $\fL$-coalgebra $(V,<o,t>)$ there exists a unique
$\fL$-homomorphism into $(\K^{A^*},<\emp,\der>)$.
\[
\xymatrix@C=1.7cm{V \ar[d]_{<o,t>}\ar@{.>}[r]^{\beh{-}{\fL}{V}}& \K^{A^*}\ar[d]^{<\emp,\der>}\\
\fL(V)  \ar@{.>}[r]_{\fL(\beh{-}{\fL}{V})}& \fL(\K^{A})}
\]
\end{theorem}
\begin{proof}
The only function making the above diagram commute is
$\beh{-}{\fL}{V}$, i.e., the function mapping each vector $v\in V$
into the weighted
language that it \emph{recognizes}. %Formally,
%$$\beh_V(v)(w)= \left\{%
%\begin{array}{ll}
%    o(v), & \hbox{if $w=\epsilon$;} \\
%    \beh_V(t(v)(a))(w'), & \hbox{if $w=aw'$;} \\
%\end{array}%
%\right $$
Hereafter we show that $\beh{-}{\fL}{V}$ is a linear map.

By induction on $w$, we prove that for all $ v_1,v_2 \in V$, for all
$ w \in A^*$,
$\beh{v_1+v_2}{\fL}{V}(w)=\beh{v_1}{\fL}{V}(w)+\beh{v_2}{\fL}{V}(w)$.

Suppose that $w=\epsilon$. Then
$\beh{v_1+v_2}{\fL}{V}(\epsilon)=o(v_1+v_2)$. Since $o$ is a
linear map, this is equal to
$o(v_1)+o(v_2)=\beh{v_1}{\fL}{V}(\epsilon)+\beh{v_2}{\fL}{V}(\epsilon)$.

Now suppose that $w=aw'$. Then
$\beh{v_1+v_2}{\fL}{V}(aw')=\beh{t(v_1+v_2)(a)}{\fL}{V}(w')$. Since
$t$ is a linear map, this is equal to
$\beh{t(v_1)(a)+t(v_2)(a)}{\fL}{V}(w')$ that (by induction
hypothesis) is equal to
$\beh{t(v_1)(a)}{\fL}{V}(w')+\beh{t(v_2)(a)}{\fL}{V}(w')=\beh{v_1}{\fL}{V}(aw')+\beh{v_2}{\fL}{V}(aw')$.

We can proceed analogously for the scalar product.
\end{proof}

%
%THIS COROLLARY IS MEANINGLESSS FOLLOWS FROM THE DEFINITION
%
%\begin{corollary} For every $\fL$-coalgebra $(V,<o,t>)$ and $v_1, v_2\in
%V$:
%\[
%v_1 \approx_\fL v_2 \iff \beh{v_1}{\fL}{V} = \beh{v_2}{\fL}{V}
%\]
%\end{corollary}

%Now note that the function $\beh_V$ maps every vector in the
%language that it recognizes. This is defined

%\cbox{The finality of $(\K^{A^*},<\emp,\der>)$ follows from more
%abstract results on \emph{lifting} and \emph{accessibility} of
%functors \cite{}, as shown in \cite{ICALP???}.}


Thus, two vectors $v_1,v_2\in V$ are $\fL$-behaviourally equivalent ($v_1 \approx_{\fL} v_2$) iff
they recognize the same weighted language (as defined in Section \ref{sec:fromlatolwa}).
Proposition~\ref{prop:linearizationlanguage} below shows that
$\beh{-}{\fL}{\K^X_{\omega}} \colon \K^X_{\omega} \to \K^{A^*}$ is
the linearization of the function $l_X \colon X \to \K^{A^*}$
(defined in Section \ref{sec:coalg}) or, in other words, is the only
linear map making the following diagram commute.
\[
\xymatrix{\K_{\omega}^X \ar[rrd]^{\beh{-}{\fL}{\K^X}}\\
X \ar[rr]_{l_X} \ar[u]^{\eta_X} & &\K^{A^*}}
\]
\begin{lemma}\label{lemma:lan}
Let $(X,<o,t>)$ be a \wa\ and $(\K^X_{\omega},
<o^{\sharp},t^{\sharp}>)$ be the corresponding linear weighted
automaton. Then for all $ x \in X$,
$l_X(x)=\beh{x}{\fL}{\K^X_{\omega}}$.
\end{lemma}
\begin{proof}
We prove it by induction on $w\in A^*$.

If $w=\epsilon$, then
$l_X(x)(w)=o_X(x)=o_X^{\sharp}(x)=\beh{x}{\fL}{\K^X_{\omega}}(w)$.

If $w=aw'$, then
$\beh{x}{\fL}{\K^X_{\omega}}(w)=\beh{t^{\sharp}(x)(a)}{\fL}{\K^X_{\omega}}(w')$.
Note that by definition, $t^{\sharp}(x)(a)=\sum_{x'\in
X}t(x)(a)(x')x'$, thus the latter is equal to


%




$$\Beh{\sum_{x'\in X}t(x)(a)(x')\cdot x'}{\fL}{\K^X_{\omega}}(w')$$
%
which, by linearity of $\beh{-}{\fL}{\K^X_{\omega}}$, coincides with
%
$$\sum_{x'\in X}t(x)(a)(x')\cdot \beh{x'}{\fL}{\K^X_{\omega}}(w')\text{.}$$
%
By induction hypothesis
$\beh{x'}{\fL}{\K^X_{\omega}}(w')=l_X(x')(w')$ and thus the above
coincides with
$$\sum_{x'\in X}t(x)(a)(x')\cdot l_X(x')(w')$$
that is $l_X(x)(w)$.
\end{proof}

\begin{proposition}\label{prop:linearizationlanguage}
Let $(X,<o,t>)$ be a \wa\ and $(\K^X_{\omega},
<o^{\sharp},t^{\sharp}>)$ be the corresponding linear weighted
automaton. Then, for all $v=k_1x_{i_1} + \dots +k_nx_{i_n}$,
$\beh{v}{\fL}{\K^X_{\omega}}=k_1l_X(x_{i_1})+ \dots +
k_nl_X(x_{i_n})$.
\end{proposition}
%\begin{proof}
%By induction on $w\in A^*$.
%
%If $w=\epsilon$, then
%$\beh{v}{\fL}{\K^X_{\omega}}(w)=o^{\sharp}(v)$. Since $o^{\sharp}$
%is a linear map and $X$ is a base for $\K^X_{\omega}$,
%$o^{\sharp}(v)=k_1o(x_{i_1})+ \dots + k_no(x_{i_n})$. For all $j$,
%$l_X(x_{i_j})(\epsilon)=o(x_{i_j})$, thus $k_1o(x_{i_1})+ \dots +
%k_no(x_{i_n})=k_1l_X(x_{i_1})(w)+ \dots + k_nl_X(x_{i_n})(w)$.
%
%%If $w=aw'$, then $k_1l_X(x_{i_1})(w)+ \dots +
%%k_nl_X(x_{i_n})(w)=k_1\sum_{x'\in X}t_X(x_{i_i})(a)(x')
%%\times l_X(x_{i_1})(w') + \dots + k_n\sum_{x'\in
%%X}t_X(x_{i_n})(a)(x') \times l_X(x_{i_1})(w')$.
%%
%%
%
%If $w=aw'$, then
%$\beh{v}{\fL}{\K^X_{\omega}}(w)=\beh{t^{\sharp}(v)(a)}{\fL}{\K^X_{\omega}}(w')$.
%Since $t^{\sharp}$ is linear and $X$ is a base for $\K^X_{\omega}$,
%then $t^{\sharp}(v)(a)=k_1t(x_{i_1})(a)+ \dots + k_nt(x_{i_n})(a)$.
%For all $j$, $$t(x_{i_j})(a)= \sum_{x'\in X}\left(
%t(x_{i_j})(a)(x')\cdot x'\right) \text{,}$$ thus
%$\beh{t^{\sharp}(v)(a)}{\fL}{\K^X_{\omega}}(w')$ is equal to
%$$\Beh{ k_1\sum_{x'\in X}\left( t(x_{i_j})(a)(x')\cdot x' \right) + \dots +
%k_n\sum_{x'\in X}\left( t(x_{i_j})(a)(x')\cdot
%x'\right)}{\fL}{\K^X_{\omega}}(w')$$ which, by linearity of
%$\beh{-}{\fL}{\K^X_{\omega}}$, is equal to
%$$k_1\sum_{x'\in X}\left( t(x_{i_j})(a)(x')\cdot \beh{x'}{\fL}{\K^X_{\omega}}(w')\right) + \dots +
%k_n\sum_{x'\in X}\left( t(x_{i_j})(a)(x')\cdot
%\beh{x'}{\fL}{\K^X_{\omega}}(w') \right)\text{.}$$
%
%By induction hypothesis
%$\beh{x'}{\fL}{\K^X_{\omega}}(w')=l_X(x')(w')$ and thus the latter
%coincides with $$k_1\sum_{x'\in X}\left( t(x_{i_j})(a)(x')\cdot
%l_X(x')(w') \right) + \dots + k_n\sum_{x'\in X}\left(
%t_X(x_{i_j})(a)(x')\cdot l_X(x')(w')\right) \text{.}$$
%
%By definition, $l_X(x_{i_j})(w)= \sum_{x'\in X} \left(
%t(x_{i_j})(a)(x')\cdot \l_X(x')(w') \right)$ and thus we can
%concisely express the above formula as
%$$k_1l_X(x_1)(w) + \dots
%+ k_nl_X(x_n)(w)\text{.}$$
%\end{proof}
\begin{proof}
It follows from Lemma \ref{lemma:lan} and linearity of $\beh{-}{\fL}{\K^X_{\omega}}$.
\end{proof}







%
%   OLD VERSION
%
%\subsection{Language equivalence and final $\fL$-coalgebra}
%In order to characterize the final $\fL$-coalgebra is convenient to
%consider the functor $M\colon\K \vectproduct id^A\colon Set \to Set$.
%Coalgebras for this functor are like $\fL$-coalgebras but in $Set$
%instead of $Vect$. More precisely, an $M$-coalgebra consists of a
%triple $(X,o,t)$ where $X$ is a set of states, $o\colonX\to\K$ is
%the output function and $t\colonX \to X^A$ is the transition function.% (that
%%associates to each input symbol $a \in A$ a next state).
%Each state $x\in X$ \emph{recognizes} a weighted language $lan_X(x)$
%in
%$\K^{A^*}$ that is formally defined $\forall x\in X,w \in A^*$ as
%$$lan_X(x)(w)= \left\{%
%\begin{array}{ll}
%    o(x), & \hbox{if $w=\epsilon$;} \\
%    lan_X(t(x)(a))(w'), & \hbox{if $w=aw'$;} \\
%\end{array}%
%\right$$ where $\epsilon$ denoted the empty string and $aw'$ the
%concatenation of input symbol $a$ with the word $w'$.
%%
%In \cite{rutten00}, it is proved that the final $M$-coalgebra is
%$(\K^{A^*}, o_{\Omega}, t_{\Omega})$, where $\forall
%\varphi\in \K^{A^*}, a \in A$, the functions
%$o_{\Omega}\colon\K^{A^*} \to \K$ and
%$t_{\Omega}\colon\K^{A^*} \to \K^{A^{*^A}}$ are defined
%as
%$$o_{\Omega}(\varphi)=\varphi(\epsilon) \qquad t_{\Omega}(\varphi)(a)=\lambda w \in A^*.
%\varphi(aw)\text{.}$$  For every coalgebra $(X,o,t)$, the unique
%$M$-morphism to $(\K^{A^*}, o_{\Omega}, t_{\Omega})$ is
%$lan_X\colonX \to \K^{A^*}$.
%%$lan_S(s)(a_1\dots a_n)= o(s_n) \text{ s.t. } s\tr{a_1}s_1\tr{a_2}
%%\dots\tr{a_n}s_n$.
%\[
%\xymatrix{X \ar[d]_{o,t}\ar[r]^{lan_X}& \K^{A^*}\ar[d]^{o_{\Omega},t_{\Omega}}\\
%\K \vectproduct X^A  \ar[r]_{M(lan_X)}& \K \vectproduct
%\K^{A^{*^A}}}\ \ \ \ \ \ \ \ \
%%\ \ g\comp h = G h \comp f
%\]
%Now, we are going to show that $(\K^{A^*}, o_{\Omega},
%t_{\Omega})$ is also the final $\fL$-coalgebra. First of all, note
%that the set $\K^{A^*}$ carries a vector spaces (sum and
%scalar product are defined as usual). Moreover the function $\langle
%o_{\Omega}, t_{\Omega}\rangle\colon\K^{A^*} \to \K \vectproduct
%\K^{A^{*^A}}$ is a linear map.
%\begin{proposition}\label{prop\colonlinear}
%The functions $o_{\Omega}\colon\K^{A^*} \to \K$ and
%$t_{\Omega}\colon\K^{A^*} \to \K^{A^{*^A}}$ are linear
%maps.
%\end{proposition}
%\begin{proof}
%We prove that $t_{\Omega}$ is linear. The proof for $o_{\Omega}$ is
%analogous.
%
%\fLet $\varphi_1, \varphi_2$ be two weighted languages in
%$\K^{A^*}$. Recall that their sum is the weighted language
%$\varphi_1+\varphi_2$ defined $\forall w\in A^*$ as
%$\varphi_1+\varphi_2(w)=\varphi_1(w)+\varphi_2(w)$. Now $\forall
%a\in A, w \in A^*$,
%$t_{\Omega}(\varphi_1+\varphi_2)(a)(w)=\varphi_1+\varphi_2(aw)=\varphi_1(aw)+\varphi_2(aw)=t_{\Omega}(\varphi_1)(a)(w)+t_{\Omega}(\varphi_2)(a)(w)$.
%
%Let $c$ be a scalar in $\K$ and $\varphi$ be a weighted
%language in $\K^{A^*}$. Recall that the scalar product is
%$c\varphi$ defined $\forall w\in A^*$ as $c\varphi(w)$. Now $\forall
%a\in A, w \in A^*$,
%$ct_{\Omega}(\varphi)(a)(w)=c\varphi(aw)=t_{\Omega}(c\varphi)(a)(w)$.
%\end{proof}
%%
%This proves that $(\K^{A^*}, o_{\Omega}, t_{\Omega})$ is an
%$\fL$-coalgebra. In order to prove that it is also final, we have to
%show that the unique $M$-homomorphism is a linear map.
%%
%\begin{proposition}\label{prop\colonlanguagelinear}
%Let $(V,o,t)$ be an $\fL$-coalgebra. Then the function $lan_V\colonV \to
%\K^{A^*}$ is a linear map.
%\end{proposition}
%\begin{proof}
%Let $v_1,v_2$ be two vectors in $V$. We prove that $\forall w \in
%A^*$, $lan_V(v_1+v_2)(w)=lan_V(v_1)(w)+lan_V(v_2)(w)$ by induction
%on $w$.
%
%Suppose that $w=\varepsilon$. Then
%$lan_V(v_1+v_2)(\varepsilon)=o(v_1+v_2)$. Since $o$ is a linear map,
%this is equal to
%$o(v_1)+o(v_2)=lan_V(v_1)(\varepsilon)+lan_V(v_2)(\varepsilon)$.
%
%Now suppose that $w=aw'$. Then
%$lan_V(v_1+v_2)(aw')=lan_V(t(v_1+v_2)(a))(w')$. Since $t$ is a
%linear map, this is equal to $lan_V(t(v_1)(a)+t(v_2)(a))(w')$ that
%(by inductive hypothesis) is equal to
%$lan_V(t(v_1)(a))(w')+lan_V(t(v_2)(a))(w')=lan_V(v_1)(aw')+lan_V(v_2)(aw')$.
%Analogously for the scalar product.
%\end{proof}
%Now, note that $\fL$ is a \emph{lifting} of $M$, i.e., the following
%diagram commutes.
%\[
%\xymatrix{Vect \ar[d]_{U}\ar[r]^{\fL}& Vect\ar[d]^{U}\\
%Set  \ar[r]_{M}& Set}\ \ \ \ \ \ \ \ \
%%\ \ g\comp h = G h \comp f
%\]
%Thus, for each $\fL$-coalgebra $(V,f\colonV\to \fL(V))$ we can build an
%$M$-coalgebra by taking $(U(V), U(f)\colonU(V)\to U\fL(V))$. The latter is
%an $M$-coalgebra because $U\fL(V)=MU(V)$. Analogously, if $h\colonV\to W$
%is an $\fL$-homomorphism, then $U(h)\colonU(V)\to U(W)$ is an
%$M$-homomorphism. More generally, the following leftmost diagram
%commutes in $Vect$ if and only if the rightmost commutes in $Set$.
%\[
%\xymatrix{V \ar[d]_{f}\ar[r]^{h}& W\ar[d]^{g}\\
%\fL (V) \ar[r]_{\fL h}& \fL W} \qquad \xymatrix{U(V) \ar[d]_{U(f)}\ar[r]^{U(h)}& U(W)\ar[d]^{U(g)}\\
%MU(V) \ar[r]_{MU(h)}& MU(W)}
%\]
%%This defines a functor $\bar{U}\colonCoalg_\fL \to Coalg_M$.
%This in enough for proving that $(\K^{A^*}, o_{\Omega},
%t_{\Omega})$ is a final $\fL$-coalgebra. Indeed for any $\fL$-coalgebra
%$(V,t,o)$, there exists an $M$-coalgebra $(U(V),U(t),U(o))$ and a
%unique $M$-homomorphism into $(\K^{A^*}, o_{\Omega},
%t_{\Omega})$. By the above observation and by Proposition
%\ref{prop\colonlanguagelinear}, this is also the unique $\fL$-homomorphism
%from $(V,t,o)$ to $(\K^{A^*}, o_{\Omega}, t_{\Omega})$.
%%
%\begin{theorem}Behavioural equivalence induced by the final
%coalgebra of $\fL$, coincide with weighted language equivalence, i.e.,
%$\approx_\fL=\sim_l$.
%\end{theorem}
%%
%A more abstract proof can be given without proving directly
%Propositions \ref{prop\colonlinear} and \ref{prop:languagelinear}.
%Indeed, we know that (since $\fL$ is accessible) the final
%$\fL$-coalgebra $(\Omega,\omega)$ is the limit of the terminal
%sequence
%$$1 \leftarrow \fL(1) \leftarrow \fL\fL(1) \leftarrow \dots\text{,}$$
%where $1$ is the final object in $Vect$ (this is the vectors space
%having as carrier set $\{0\}$). Note that $U(1)$ is equal to the set
%$1$ (consisting of a singleton element) and that $UL(1)=MU(1)=M1$.
%Thus, by applying $U$ to the terminal sequence of $\fL$, we obtain the
%terminal sequence of $M$.
%$$1 \leftarrow M(1) \leftarrow MM(1) \leftarrow \dots$$
%The limit of this sequence is the final $M$-coalgebra
%$(\K^{A^*}, o_{\Omega}, t_{\Omega})$ (because $M$ is
%accessible). Since the functor $U$ \emph{creates limits}, the limit
%of the $\fL$-terminal sequence $(\Omega, \omega)$ is such that
%$U(\Omega, \omega)=(\K^{A^*}, o_{\Omega}, t_{\Omega})$.



\subsection{Linear Bisimulations and Subspaces}\label{sec:linearbis}
%
We now introduce a convenient characterization of language
equivalence by means of \emph{linear weighted bisimulations}.
Differently form ordinary (weighted) bisimulations, these can be
seen both as relations and as subspaces. The latter characterization
will be exploited in the next section for defining an algorithm for
checking language equivalence.

%$\sim_l$ is the largest linear weighted bisimulation and thus, in
%order to prove that two vectors $v_1$ and $v_2$ recognize the same
%language, it is enough to provide a linear weighted bisimulation
%relating them.

First, we show how to represent relations over a vector space $V$ as sub-spaces of
$V$, following \cite{Stark,Bor09}.

\begin{definition}[linear relations]\label{Def:LineEq}
Let $U$ be a sub-space of $V$. The binary relation $R_U$ over $V$ is
defined by
\[
v_1\;R_U\;v_2 \mbox{ if and only if }v_1-v_2\in U\,.
\]
A relation $R$ is \emph{linear} if there is a subspace $U$ such that
$R$ equals $R_U$.
\end{definition}

Note that a linear relation is a  total  equivalence relation on
$V$. Let now $R$ be \emph{any} binary  relation  over $V$.  There is
a canonical way of turning $R$  into a linear relation, which we
describe in the following. The \emph{kernel} of $R$ (in symbols
$\kernel(R)$) is the set $\{v_1-v_2 \,|\, v_1\,R\, v_2\}$.
%
%We say $R$  is \emph{linear} if $\kernel(R)$ is a sub-space of $V$.
The \emph{linear extension} of $R$, denoted $R^\ell$, is defined by:
$ v_1 \,R^\ell \,v_2 \;\;\mbox{\rm if and only if } (v_1-v_2)\in
\Span(\kernel(R)) $.
%

\begin{lemma}\label{lemma:LinEq}
%(a)
Let $U$ be a sub-space of $V$, then  $\kernel(R_U)=U$.
%
%(b) Given any binary relation $R$, $R^\ell$ is the smallest linear
%relation containing $R$.
\end{lemma}

According to the above lemma, a linear   relation $R$ is completely
described by its kernel, which is a sub-space, that is
\begin{eqnarray}\label{Eq:LinEq}
v_1 \,R\, v_2 \quad \mbox{ if and only if}\quad (v_1-v_2)\in
\kernel(R)\,.
\end{eqnarray}
Conversely, for any sub-space $U\subseteq V$ there is a
corresponding linear relation $R_U$ whose kernel is $U$.  Hence,
without loss of generality, \emph{we can identify linear   relations
on $V$  with sub-spaces of $V$}. For example,  by slight abuse of
notation, we can write   $v_1\, U\, v_2$ instead of $v_1 \,R_U\,
v_2$; and conversely, we will  sometimes  denote by $R$   the
sub-space $\kernel(R)$.  The context will be sufficient to tell
whether we are actually referring to a linear relation or to the
corresponding sub-space (kernel). Note that the sub-space $\{0\}$
corresponds to the identity relation on $V$, that is
$R_{\{0\}}=Id_V$. In fact: $v_1 \,Id_V\, v_2$ iff $v_1=v_2$ iff
$v_1-v_2=0$. Similarly, the space $V$ itself corresponds to
$R_V=V\vectproduct V$. %Another consequence of the preceding lemma,
%part (b), is that it is not restrictive to confine ourselves, as we
%do below, to relations over $V$ that are linear.  Note that, again
%by virtue of (b), $R^\ell=R$ if $R$ is linear, hence $(-)^\ell$ is
%idempotent: $(R^\ell)^\ell=R^\ell$.


We are now ready to define linear weighted bisimulation. This
definition relies on the familiar step-by-step game played   on
transitions, plus an initial condition requiring that two related
states have the same output weight. We call this form of
bisimulation \emph{linear} to stress the difference with the one
introduced in Definition \ref{def:bis}.

\begin{definition}[linear weighted bisimulation]\label{Def:Bis}
Let $(V, <o,t>)$ be a linear weighted automaton. A linear relation
$R\subseteq V \vectproduct V$ is a \emph{linear weighted
bisimulation} if for all $(v_1,v_2)\in R$, it holds that:
\begin{enumerate}
\item[(1)] $o(v_1)=o(v_2)$,
\item[(2)] $\forall a\in A$, $t(v_1)(a)\; R \;t(v_2)(a)$.
\end{enumerate}
%\emph{Linear Weighted Bisimilarity} is the largest weighted
%bisimulation.
\end{definition}
%The largest linear weighted bisimulation coincides with language
%equivalence. This can be easily proved by employing the same
%arguments of Theorem \ref{theo:bis} and noting that (a) the kernel
%of a $L$-homomorphism is a linear weighted bisimulation and,
%conversely, (b) for each linear weighted bisimulation $R$, the
%linear map $\varepsilon_R$ mapping each vector in its equivalence
%class is an $\fL$-homomorphism.
%

%\cbox{Filippo: Maybe Cambiare Esempio}

For a concrete example, consider the automaton
$(\mathbb{R}_{\omega}^X, <o_X^\sharp,t_X^\sharp>)$ in Fig
\ref{fig:isomorphic}. The relation $R=\{(x_2,x_3)\}$ is not linear,
because $U=\{x_2-x_3\}$ is not a subspace of
$\mathbb{R}_{\omega}^X$. However, we can turn $R$ into a linear
relation by employing its kernel $\kernel(R)=\{x_2-x_3\}$. The
linear extension of $R$ is $R^\ell=\{(k_1x_1 + k_2x_2 + k_3x_3,
k_1'x_1 + k_2'x_2 + k_3'x_3) \;|\; k_1=k_1' \text{ and }
k_2+k_3=k_2'+k_3' \}$. It is easy to see that $R^\ell$ is a linear
weighted bisimulation.

%\cbox{FILIPPO: This is nice but is it really useful?
%
%A useful property of bisimulation on ordinary transition systems is
%that, to prove two states related, exhibiting a ``small" relation
%containing the given pair is sufficient. This property is preserved
%in the present setting, despite the fact that Definition
%\ref{Def:Bis} mentions linear, hence ``big", relations on $V$.
%
%\begin{lemma}\label{lemma:SmallBis}
%Let $\fL$ be a \lwa\ and  $R$ be a binary relation over $V$ satisfying
%clauses (a) and (b) of Definition \ref{Def:Bis}. Then $R^\ell$ is
%the smallest weighted $\fL$-bisimulation containing $R$.
%\end{lemma}
%}

The following lemma provides a characterization of linear weighted
bisimulation as a subspace. %
Let us say that a sub-space $U$ is \emph{$f$-invariant} if
$f(U)\subseteq U$. Bisimulations are transition-invariant relations
that refine the kernel of the output map $o$.

\begin{lemma}\label{lemma:BisAlt}
Let $(V, <o,t>)$ be a \lwa\ and $\R$ be a linear relation over $V$.
$\R$ is a linear weighted bisimulation if and only if
\begin{enumerate}
\item[(1)] $\R \subseteq \kernel(o) $,
\item[(2)] $R$ is $t_a$-invariant for each $a\in A$.
\end{enumerate}
\end{lemma}

This lemma will be fundamental in the next section for defining an
algorithm computing the greatest linear weighted bisimulation. In
the remainder of this section, we show that the greatest linear
weighted bisimulation coincides with the kernel of the final map
$\beh{-}{\fL}{V}$. More generally, the kernel of each
$\fL$-homomorphism is a linear weighted bisimulation $R$ and,
vice versa, for each linear weighted bisimulation $R$ there exists an
$\fL$-homomorphism whose kernel is $R$.

\begin{proposition}\label{prop:MorphismBis}
Let $(V,<o_V,t_V>)$ be a \lwa. If $f\colon V\to W$ is an
$\fL$-homomorphism (for some \lwa\ $(W,<o_W,t_W>)$) then
$\kernel(f)$ is a linear weighted bisimulation. Conversely, if $R$
is a linear weighted bisimulation for $(V,<o,t>)$, then there exists
a \lwa\ $(W,<o_W,t_W>)$ and an $\fL$-homomorphism $f\colon V \to W$
such that $\kernel(f)=R$.
\end{proposition}
\begin{proof}
First, we suppose that $f\colon V\to W$ is an $\fL$-homomorphism and
we prove that $\kernel(f)$ satisfies (1) and (2) of Lemma
\ref{lemma:BisAlt}. Take a vector $v\in \kernel(f)$. Thus, $f(v)=0$
and, since $o_W$ and $t_W$ are linear maps, $o_W(f(v))=0$ and
$t_W(f(v))(a)=0$ for all $a \in A$. Since $f$ is an
$\fL$-homomorphism, we have that (1) $o_V(v)=o_W(f(v))=0$, i.e.,
$\kernel(f)\subseteq \kernel(o_V)$ and (2)
$f(t_V(v)(a))=t_W(f(v))(a)=0$ meaning that $t_V(v)(a)\in
\kernel(f)$, i.e., $\kernel(f)$ is $t_{V_a}$-invariant.

In order to prove the second part, we need to recall \emph{quotient
spaces} and \emph{quotient maps} from \cite{Halmos}. Given a
subspace $U$ of $V$, the equivalence class of $v$ w.r.t. $U$ is
$[v]_U =\{v+u \;|\; u \in U\}$. Note that $v_1\in[v_2]_U$ if and
only if $v_1R_Uv_2$. The quotient space $V/U$ is the space of all
equivalence classes $[v]_U$ where scalar product $k [v]_U$ is
defined as $[k v]_U$ and the sum $[v_1]_U + [v_2]_U$ as
$[v_1+v_2]_U$. It is easy to check that these operations are
well-defined (i.e., independent from the choice of the
representative) and turn $V/U$ into a vector space where the element
$0$ is $U$. Most importantly, the quotient function
$\varepsilon_U\colon V \to V/U$ mapping each vector $v$ into $[v]_U$
is a linear map such that $\kernel(\varepsilon_U)=U$.

Now, let $U$ be the subspace corresponding to the linear weighted
bisimulation $R$. We can take $W=V/U$ and we define $o_W$ as
$o_W([v]_U)=o_V(v)$ and $t_W$ as $t_W([v]_U)(a)=[t(v)(a)]_U$. Note
that both $o_W$ and $t_W$ are well defined: for all $v'\in
[v]_U=\{v+ u \;|\; u\in U \}$, $o_W(v')=o_W(v)$ (since $o_V(u)=0$
for all $u\in U$) and $t_W(v')(a)\in [t_W(v)(a)]_U$ (since
$t_V(u)(a)\in U$ for all $u\in U$). It is also easy to check that
they are linear.

Finally, we take $f\colon V \to W$ as $\varepsilon_U$ and with the
previous definition of $o_W$ and $t_W$ is trivial to check that
$\varepsilon_U$ is an $\fL$-homomorphism. As said above, its kernel
is $U$.
\end{proof}


\begin{theorem}
Let $(V,<o,t>)$ be a \lwa\ and let $\beh{-}{\fL}{V}\colon V \to
\K^{A^*}$ be the unique $\fL$-morphism into the final coalgebra.
Then $\kernel(\beh{-}{\fL}{V})$ is the largest linear weighted
bisimulation on $V$.
\end{theorem}
\begin{proof}
First of all, note that by the first part of Proposition
\ref{prop:MorphismBis}, $\kernel(\beh{-}{\fL}{V})$ is a linear
weighted bisimulation.

Then suppose that $R$ is a linear weighted bisimulation. By the
second part of Proposition \ref{prop:MorphismBis}, there exists a
\lwa\ $(W,<o_W,t_W>)$ and an $\fL$-homomorphism $f\colon V\to W$
such that $R=\kernel(f)$. Now note that, since $(W,<o_W,t_W>)$ is an
$\fL$-coalgebra there exists an $\fL$-homomorphism
$\beh{-}{\fL}{W}\colon W \to \K^{A^*}$ to the final coalgebra. Since
the composition of two $\fL$-homomorphisms is still an
$\fL$-homomorphism, also $\beh{-}{\fL}{W} \comp f\colon V \to
\K^{A^*}$ is an $\fL$-homomorphism. Since $\beh{-}{\fL}{V}$ is the
unique $\fL$-homomorphism from $V$ to $\K^{A^*}$, then
$\beh{-}{\fL}{W} \comp f=\beh{-}{\fL}{V}$. Finally,
$R=\kernel(f)\subseteq \kernel(\beh{-}{\fL}{W} \comp f) =
\kernel(\beh{-}{\fL}{V})$.
\end{proof}
\begin{corollary}\label{corollary}
$\approx_{\fL}$ is the largest linear weighted bisimulation.
\end{corollary}
%\begin{proof}
%$v_1 \approx_{\fL} v_2 $ iff $\beh{v_1}{\fL}{V}=\beh{v_2}{\fL}{V}$ iff $\beh{v_1-v_2}{\fL}{V}=0$ iff $v_1-v_2 \in \kernel(\beh{-}{\fL}{V})$.
%\end{proof}

The characterization of bisimulations as subspaces seems to be
possible in $Vect$ and not in $Set$ because the former category is
\emph{abelian} \cite{Abelian}: every map has a kernel that is a
subspace and every subspace is the kernel of some map. We leave as
future work to study (at a more general level) the categorical
machinery allowing to characterize bisimulations as subspaces.

\smallskip

In Section \ref{sec:coalg}, we have shown that the largest weighted bisimulation ($\sim_w$) is strictly included in language equivalence,
while here we have shown that the largest linear weighted bisimulation coincides with language equivalence.
However, it is not clear yet what is the relationship between weighted bisimulations and linear weighted bisimulations.
The following proposition explains it.

\begin{proposition}
Let $(X, <o,t>)$ be a weighted automaton and $(\K^X_{\omega}, <o^{\sharp},t^{\sharp}>)$ be the corresponding linear weighted automaton.
If $R$ is a weighted bisimulation on $X$, then $R^\ell$ is a linear weighted bisimulation on $\K^X_{\omega}$.
\end{proposition}
\begin{proof}
Recall the weighted automaton $(X/R, <o_{X/R},t_{X/R}>)$ and the function
$\varepsilon_R\colon  X \to X/R$ defined before Theorem \ref{theo:bis} and recall also that $\varepsilon_R$ is a
$\fW$-homomorphism between $(X, <o,t>)$ and $(X/R, <o_{X/R},t_{X/R}>)$.
In Section \ref{sec:fromlatolwa}, we have shown that, for every $\fW$-homomorphism $h$, $\K^{h}_{\omega}$ is a $\fL$-homomorphism.
Therefore $\K^{\varepsilon_R}_{\omega}\colon \K^X_{\omega} \to \K^{X/R}_{\omega} $ is a $\fL$-homomorphism between $(\K^X_{\omega}, <o^{\sharp},t^{\sharp}>)$ and
$(\K^{X/R}_{\omega}, <o_{X/R}^{\sharp},t_{X/R}^{\sharp}>)$.
By Proposition \ref{prop:MorphismBis}, $\kernel(\K^{\varepsilon_R}_{\omega})$ is a linear weighted bisimulation on $\K^X_{\omega}$.

Therefore, in order to complete the proof, we only have to prove that $\kernel(\K^{\varepsilon_R}_{\omega}) = R^{\ell}$.
First of all note that $\K^{\varepsilon_R}_{\omega} \colon \K^X_{\omega} \to \K^{X/R}_{\omega}$ maps each $v=k_1x_1 + \dots + k_nx_n$ in
$$\sum_{[x_i]_R \in X/R} \left( \sum_{x_j \in [x_i]_R} k_j \right) [x_i]_R$$
and thus $v\in \kernel(\K^{\varepsilon_R}_{\omega})$ if and only if for all $x_i$, $\sum_{x_j \in [x_i]_R} k_j=0$.
Then, we show that $v\in R^{\ell}$ if and only if the same condition holds. Indeed, by definition, $v\in R^{\ell}$ iff and only if for all
$j,l \in \{1, \dots, n\}$, exist $k'_{j,l}$ such that (1) $v=\sum_{j=1}^n\sum_{l=1}^n k'_{j,l} (x_j-x_l)$ and (2) if $(x_j,x_l)\notin R$ then
$k'_{j,l}=0$. Thus $$v= \sum_{l=1}^n (k'_{1,l} - k'_{l,1}) x_1 + \dots + \sum_{l=1}^n (k'_{n,l} - k'_{l,n}) x_n \text{,}$$
i.e., for all $j$, $k_j=\sum_{l=1}^n (k'_{j,l} - k'_{l,j})$. Recall that, according to Definition \ref{def:bis}, $R$ is an equivalence relation
and thus $(x_j,x_l)\in R$ iff $x_l \in [x_j]_R$. Then, by (2) above, $k_j=\sum_{x_l\in[x_j]_R} (k'_{j,l} - k'_{l,j})$.
%
For all $x_i$, $\sum_{x_j \in [x_i]_R} k_j =
\sum_{x_j\in [x_i]_R}\sum_{x_l \in [x_j]_R} (k'_{j,l} - k'_{l,j})$. Since $[x_j]_R=[x_i]_R$, each $k'_{j,l}$
occurs excatly once in a positive way and once in a negative way and thus $\sum_{x_j \in [x_i]_R} k_j =0$.
\end{proof}

The other implication does not hold. For instance, there exists no weighted bisimulation
relating the states $y_1$ and $z_1$ in Fig. \ref{fig:examplebislan}, however we can show a
\emph{linear} weighted bisimulation relating them: the linear extension of $R=\{(y_1, z_1), \, (y_2,z_2), \, (y_3+y_5, 2z_3)\}$
is a linear weighted bisimulation.


%
%
%
%DETTO SOPRA NON CI SERVE
%
%
%
%The largest $L$-bisimulation $\approx_L$ coincides with the
%weighted-language semantics.
%\begin{theorem}\label{Th:LangEq}
%%Let $L=(V,(T_a)_{a\in A}, \phi)$ be a \lwa.
%For any $u,v\in V$, we have that $u\approx_L v$ if and only if
%$\sigma_L(u)=\sigma_L(v)$.
%\end{theorem}
%\begin{proof}
%Suppose that  $u\approx_L v$. It is easy to prove by induction on $|x|$
%that $\sigma_L(u)(x)=\sigma_L(v)(x)$, for each $x\in A^*$.
%Conversely, consider the relation $R=\{(u,v)\,|\,
%\sigma_L(u)=\sigma_L(v)\}$. It is easy to see that this is a linear
%relation. Moreover,   it is a $L$-bisimulation. Concerning clause
%(a) of Definition \ref{Def:Bis}, note that
%$\phi(u)=\sigma_L(u)(\epsilon)= \sigma_L(v)(\epsilon)=\phi(v)$.
%Concerning (b), note that, for each  $a$ and $x\in A^*$:
%$\sigma_L(T_a u)(x)= \sigma_L(u)(a x)= \sigma_L(v)(a x)=
%\sigma_L(T_a v)(x)$, so that $\sigma_L(T_a u)=\sigma_L(T_a v)$, that
%is, $T_a u\;R\; T_a v$.
%\end{proof}

\section{Linear Partition Refinement}\label{sec:linearpr}
% MICHELE'S MACRO -- EVENTUALLY MOVE INTO THE PREAMBLE

%\mik{Check the consistency of notation for \lwa\ with previous
%sections.} \cbox{FILIPPO Change this introductory part
%
%Let $L=(V,(T_a)_{a\in A},\phi)$ be a \lwa. That a largest
%$L$-bisimulation exists is quite obvious: in analogy with the
%ordinary case,  one takes  the ({span} of the) union of all
%$L$-bisimulations,  and checks  that it is in turn a
%$L$-bisimulation, the largest one. Note that the mentioned union is
%non-empty, as e.g. the identity relation is a $L$-bisimulation.
%
%In this section, we show how the largest bisimulation can be
%effectively computed via a geometrical partition refinement
%algorithm. We will in fact examine below two versions of the
%algorithm, a forward version and a backward one. The former   is
%straightforward but computationally not very convenient; the latter
%is more convenient, although it requires the introduction of some
%extra machinery. }

In the previous section, we have shown that weighted language
equivalence ($\sim_l$) can be seen as the largest linear weighted
bisimulation.
%
In this section, we exploit this characterization in order to
provide a ``partition refinement'' algorithm that allows to compute
$\sim_l$. We will examine below two versions of the algorithm, a
forward version (Section \ref{sec:forwalg}) and a backward one
(Section \ref{sec:back}). The former is straightforward but
computationally not very convenient; the latter is more convenient,
although it requires the introduction of some extra machinery.
In both cases, we must restrict to \lwa's where the state space is of finite dimension.


\subsection{A forward algorithm}\label{sec:forwalg}

\begin{figure}[t]{%\small
\MediumPicture
%
%\hspace*{1.2cm}
\begin{center}
\VCDraw{ \HideGrid\HideFrame \ChgEdgeLabelScale{.8}
\begin{VCPicture}{(0,-2)(6,3.5)}
% states
\State[x_1]{(3,2)}{A}
%
\State[x_2]{(1.5,-0.5)}{B}
%
\State[x_3]{(4.5,-0.5)}{C}
% initial--final
\FinalR{e}{A}{2} \FinalR{s}{B}{1}\FinalR{s}{C}{{1}}  %\Final{C}
% transitions
%\EdgeL[.65]{B}{A}{a,1} \EdgeR[.65]{C}{A}{a,-1} \LoopS{B}{a,1}
\EdgeL[.5]{B}{A}{a,{\frac 1 3}} \EdgeR[.5]{C}{A}{a,1}
\ArcR[.5]{B}{C}{b,{\frac 1 3}} \ArcR[.5]{C}{B}{b,3}
%
\LoopN{A}{a,1  / b,1}
%
\end{VCPicture}
\begin{VCPicture}{(0,-2)(6,3.5)}
% states
\State[x_1]{(3,2)}{A}
%
\State[x_2]{(1.5,-0.5)}{B}
%
\State[x_3]{(4.5,-0.5)}{C}
% initial--final
\FinalR{e}{A}{2} \FinalR{s}{B}{1}\FinalR{s}{C}{{1}}  %\Final{C}
% transitions
%\EdgeL[.65]{B}{A}{a,1} \EdgeR[.65]{C}{A}{a,-1} \LoopS{B}{a,1}
\EdgeR[.5]{A}{B}{a,{\frac 1 3}} \EdgeL[.5]{A}{C}{a,1}
\ArcR[.5]{C}{B}{b,{\frac 1 3}} \ArcR[.5]{B}{C}{b,3}
%
\LoopN{A}{a,1  / b,1}
%
\end{VCPicture}}
\\
\begin{center}
$O= \left(%
\begin{array}{ccc}
  2 & 1 & 1 \\
\end{array}\right)$
$T_{a}=\left(%
\begin{array}{ccc}
  1 & \frac{1}{3} & 1 \\
  0 & 0 & 0 \\
  0 & 0 & 0 \\
\end{array}\right)$
$T_{b}=\left(%
\begin{array}{ccc}
  1 & 0 & 0 \\
  0 & 0 & 3 \\
  0 & \frac{1}{3} & 0 \\
\end{array}\right)$
$\transp{T_{a}}=\left(%
\begin{array}{ccc}
  1 & 0 & 0 \\
  \frac{1}{3} & 0 & 0 \\
  1 & 0 & 0 \\
\end{array}\right)$
$\transp{T_{b}}=\left(%
\begin{array}{ccc}
  1 & 0 & 0 \\
  0 & 0 & \frac{1}{3} \\
  0 & 3 & 0 \\
\end{array}\right)$
\end{center}
%\hspace*{2cm}
 \vspace*{-.5cm}
\end{center}
\caption{A weighted automata $(V,<o,t>)$ (left) and its reversed
$(V,<o,\transp{t}>)$ (right).}\label{fig:ExAlgorithm}
%\vspace*{-.2cm}
}
\end{figure}

%\cbox{FILIPPO: This has been already introduced in Section 3.4
%
%The following lemma provides a somewhat handier characterization of
%linear weighted bisimulation.
% Let us say that a sub-space $U$ is \emph{$T$-invariant} if $T(U)\subseteq U$.
% Bisimulations are   transition-invariant relations that refine the kernel of $\phi$.
%
%\begin{lemma} Let $L$ be a \lwa\ and $\R$ be linear  relation over $V$.
% $\R$ is a $L$-bisimulation if and only if (a) $\kernel(\phi)\supseteq \R$,
% and (b) $R$ is $T_a$-invariant for each $a\in A$.
%\end{lemma}
%}

Lemma \ref{lemma:BisAlt} suggests that, in order to compute the
largest linear weighed bisimulation for a \lwa\ $(V,<o,t>)$, one
might start from $\kernel(o)$ and refine it until the condition (2)
given in the lemma is satisfied. This is indeed the case.

%Note that the intersection of an arbitrary number of sub-spaces of
%$V$ is still a sub-space of $V$.

\begin{proposition}[partition refinement, forward version]\label{prop:forw}
Let $(V,<o,t>)$ be a \lwa. Consider the sequence $(R_i)_{i\geq 0}$
of sub-spaces of $V$ defined inductively by
\[
R_0\;=\; \kernel(o)\quad\quad R_{i+1}\;=R_i \cap \bigcap_{a\in A}
t(R_i)(a)^{-1}
\]
where $t(R_i)(a)^{-1}$ is the space $\{v\in V \;|\; t(v)(a)\in
R_i\}$. Then there is $j\leq\dim(V)$ such that $R_{j+1}=R_j$. The
largest linear weighted bisimulation is $\approx_\fL=R_j$.
\end{proposition}

\begin{proof}
The $R_i$'s form a descending chain of sub-spaces of $V$. The
corresponding dimensions form a non-increasing sequence, hence the
existence of   $j$ as required is obvious. That $R_j$ is a
bisimulation follows by applying Lemma \ref{lemma:BisAlt}: indeed,
it is obvious that (1) $\kernel(o)\supseteq R_j$, while as to (2) we
have that, since $R_{j+1}=R_j$, then $R_j \cap \bigcap_{a\in A}
t(R_j)(a)^{-1}=R_j$, i.e.,  for all $ a \in A$, $t(R_j)(a)\subseteq
R_j$.

We finally show that any  linear weighted bisimulation $R$ is
included in $\R_j$. We do so by proving that for each $i$,
%$S^\bot\supseteq \R_i$, which implies
$R\subseteq \R_i$, thus, in particular $R\subseteq \R_j$. We proceed
by induction on $i$. Again by Lemma \ref{lemma:BisAlt}, we know that
$\R_0=\kernel(o)\supseteq R$. Assume now $R\subseteq \R_i$. For each
action $a$, by Lemma \ref{lemma:BisAlt} we have that
$t(R)(a)\subseteq R$, which implies $R\subseteq \{v\in R_i \;|\;
\forall a\in A,\; t(v)(a)\in R_i\} = R_{i+1}$.
\end{proof}

%In order to check if $v_1 \approx_L v_2$, we can
Concretely, the algorithm iteratively computes a basis $B_i$ for
each space $R_i$. This can be done by solving systems of linear
equations expressing the constraints in the definition of $R_i$.
Since the backward algorithm presented in the next section is
computationally more efficient, we avoid to give further details
about its implementation and we show, as an example, the algorithm
at work with the linear automata $(V,<o,t>)$ in
Fig.\ref{fig:ExAlgorithm}.


\begin{example}\label{ex:ForwardAlg}
We start by computing  a basis for $R_0=\kernel(o)$. This is
$$B_0=\left\{ \left(%
\begin{array}{c}
  -\frac{1}{2} \\
  1\\
  0 \\
\end{array}\right),
%
\left(%
\begin{array}{c}
  -\frac{1}{2} \\
  0\\
  1 \\
\end{array}\right)
%
\right \}\text{.}
$$
In the first iteration, we compute one basis for the space
$t(R_0)(a)^{-1}$ and one for the space $t(R_0)(b)^{-1}$. These are
respectively
$$B_1^a=\left\{ \left(%
\begin{array}{c}
  -\frac{1}{3} \\
  1\\
  0 \\
\end{array}\right),
%
\left(%
\begin{array}{c}
  -1 \\
  0\\
  1 \\
\end{array}\right)
%
\right \}
%
\text{ and }
%
B_1^b=\left\{ \left(%
\begin{array}{c}
  -\frac{1}{6} \\
  1\\
  0 \\
\end{array}\right),
%
\left(%
\begin{array}{c}
  -\frac{3}{2} \\
  0\\
  1 \\
\end{array}\right)
%
\right \}
%
\text{.}$$
%
Then, $R_1$ is given by the intersection $R_0\cap t(R_0)(a)^{-1}
\cap t(R_0)(b)^{-1}$. A basis for $R_1$ is
%
$$B_1=\left\{ \left(%
\begin{array}{c}
  -2 \\
  3\\
  1\\
\end{array}\right)
%
\right \}\text{.}$$
%
In the second iteration, we compute one basis for the space
$t(R_1)(a)^{-1}$ and one for the space $t(R_1)(b)^{-1}$. These are
respectively
$$B_2^a=\left\{ \left(%
\begin{array}{c}
  -\frac{1}{3} \\
  1\\
  0 \\
\end{array}\right),
%
\left(%
\begin{array}{c}
  -1 \\
  0\\
  1 \\
\end{array}\right)
%
\right \}
%
\text{ and }
%
B_2^b=\left\{ \left(%
\begin{array}{c}
  -2 \\
  3\\
  1 \\
\end{array}\right)
%
\right \}
%
\text{.}$$
%
Then, $R_2$ is the intersection $R_1\cap t(R_1)(a)^{-1} \cap
t(R_0)(b)^{-1}$. A basis for $R_2$ is
%
$$B_2=\left\{ \left(%
\begin{array}{c}
  -2 \\
  3\\
  1\\
\end{array}\right)
%
\right \}$$ that is equal to $B_1$. Since $R_1=R_2$ the algorithm
terminates and returns $R_1$. Now, in order to check if two vectors
$v_1, v_2 \in V$ accept the same weighted language (i.e.,
$v_1\approx_\fL v_2$), we have to look if $v_1-v_2$ belongs to
$R_1$. For instance, $x_1 \approx_\fL \frac{3}{2}x_2 +
\frac{1}{2}x_3$ because $x_1 - \frac{3}{2}x_2 - \frac{1}{2}x_3\in
R_1$.
\end{example}


\bigskip

We note that $\kernel(o)$ is in general a large sub-space: since $o
\colon V\rightarrow \K$ with $\dim(\K)=1$, by virtue of equation
(\ref{Eq:FunLin}) we have that $\dim(\kernel(o))\geq \dimn(V)-1$. This
might be problematic in the actual computation of the basis of
$\approx_\fL$. We present an alternative version in the next
subsection which will avoid this problem.

\subsection{A backward algorithm}\label{sec:back}
Two  well-known concepts from linear algebra will be relied upon to describe
the basic operations of the backward algorithm. More precisely,  annihilators will be used to
describe the complement of a relation, while  transpose maps will be used  to describe
the operation of  ``reversing arrows" in an automaton. These
operations are carried out within the \emph{dual space} of $V$. So
we start by reviewing the concept of {dual space}; an in-depth
treatment can be found in e.g. \cite{Halmos}.

Let $\K$ be any field and $V$ a vector space over $\K$. The
\emph{dual space} of $V$, denoted $\dual V $, is the set of all
linear maps $V\rightarrow \K$, with $\K$ seen as a 1-dimensional
vector space. The elements of $\dual V$ are often called
\emph{functionals} and we use $\psi_1,\psi_2, \dots$ to range over
them. The sum of two functionals $\psi_1+\psi_2$ and the scalar
multiplication $k\cdot \psi$ (for $k\in \K$)  are defined point-wise
as expected, and turn $\dual V$ into a vector space over $\K$. We
will denote functional application $\psi(v)$ as $[v,\psi]$, the
bracket notation intending to emphasize certain analogies with inner
products. Fix an ordered basis $B=(v_1,...,v_n)$ of $V$ and consider
$\dual B= (\dual v_1,...,\dual v_n)$, where the functionals $\dual
v_i$ are specified by $[v_j, \dual v_i]= \delta_{ij}$ for each $i$
and $j$. Here, $\delta_{ij}$ denotes the Kronecker symbol, which
equals $1$ if $i=j$ and 0 otherwise. It is easy to check that $\dual
B$ forms a basis of $\dual V$, referred to as the \emph{dual basis}
of $B$. Hence $\dimn(\dual V)=\dimn(V)$. In particular, the morphism
$\dual{(-)} \colon V\rightarrow \dual V$ that sends each $v_i$ into
$\dual v_i$ is an isomorphism between $V$ and $\dual V$.
%Note that if $  a$ and $  b$ are the column-vectors representing $v$ and $\psi$ in the respective bases $B$ and $\dual B$, then $\psi(v)$ can be computed as the ordinary scalar product   of these vectors, that is
%\begin{eqnarray}\label{Eq:ComputeFunc}
%[v,\psi]= \lpar   a ,  b\rpar \defi \sum_i a_i\cdot b_i\,.
%\end{eqnarray}
A crucial definition is that of   transpose morphism.


\begin{definition}[transpose linear map]
Let $f \colon  V\rightarrow V$ be a linear map.  We let the
\emph{transpose of} $f$  be the endomorphism $\transp f \colon \dual
V\rightarrow \dual V$  defined for all $\psi\in\dual V$ as $\transp
f(\psi)= \psi\comp f$.

 %-- that is,  for each $v$ having as coordinates the column-vector $\tld c$ in the given basis, $M\tld c$ is the column-vector of coordinates of $Tv$ in the basis.
%We let the \emph{transpose} of $T$, written $\transp T$, be the endomorphism   $$ represented by the transpose matrix $\transp M$ in the given basis.
\end{definition}



%\begin{remark}{\em
It is easy  to check that  if $F$ is the matrix representing
$f$ in $V$ w.r.t. to $B$, then the transpose matrix $\transp F$
represents   $\transp f$ in $\dual V$ w.r.t. $\dual B$, whence the
terminology and the notation.
%does \emph{not} depend on the choice of the ortho-normal basis. The transpose operator is of course idempotent, in the sense that $\transp(\transp T)=T$. }%\mik{Valutare se espandere qs. remark.}
%\end{remark}
 Denote by
$\ddual V$ the space $\dual{(\dual V)}$, called double dual of $V$.
There is a natural isomorphism $i$ between $V$ and $\ddual V$, given
by $i \colon v\mapsto [v,\,-\,]$ (note that this isomorphism does
not depend on the choice of a basis). In the sequel, we shall freely
identify  $V$ and $\ddual V$ up to this isomorphism, i.e. identify
$v$ and $[v,-]$ for each $v\in V$. With this identification, one has
that $\transp(\transp f)=f$.

We need another concept from duality theory. Given a subspace $U$ of
$V$, we denote by $U^\ann$ the \emph{annihilator} of $U$, the subset
of functionals that vanish on $U$.

 \begin{definition}[annihilator]
 For any subspace $U\subseteq V$,
 we let $U^\ann= \{\psi\in \dual V\,|\, [u,\psi]=0\mbox{ for each } u\in U\}$.
  \end{definition}

Once again,  the   notation makes the analogy   with   inner
products explicit. We use the following properties of annihilators,
where $U,W$ are subspaces of $V$: (i) $U^\ann$ is a sub-space of
$\dual V$; (ii) $(-)^\ann$ reverses inclusions, i.e. if $U\subseteq
W$ then $W^\ann\subseteq U^\ann$; (iii) $(-)^\ann$ is an involution,
that is $(U^\ann)^\ann=U$  up to the natural isomorphism between $V$
and its double dual. These three properties suggest that $U^\ann$
can be regarded as a \emph{complement}, or negation, of $U$ seen as
a relation. Another useful property is: (iv)
$\dimn(U^\ann)+\dimn(U)=\dimn(V)$. Transpose morphisms and
annihilators  are connected via the following  property, which is
crucial to the development of the algorithm. It basically asserts
that $f$-invariance of $R$ corresponds to $\transp f$-invariance of
the complementary relation represented by $R^\ann$.
 %that characterizes bisimulation  is also enjoyed by the automaton with arrows reversed, but on the orthogonal complement of $U$.

\begin{lemma}\label{lemma:Transp}
Let $U$ be a sub-space of $V$ and $f$ be an endomorphism on $V$. If
$U$ is $f$-invariant then $U^\ann$ is  $\transp f$-invariant.
\end{lemma}

We are now ready to give the backward version of the partition
refinement algorithm. %For our purpose, we can fix $\K=\reals$ in the
%definitions above.
An informal preview of the algorithm is as follows. Rather than
computing directly the sub-space representing $\approx_\fL$, the
algorithm computes the sub-space representing the complementary
relation.  To this end, the algorithm starts from a relation $R_0$
that is the complement of the relation identifying vectors with
equal weights,   then    incrementally  computes the space of all
states that are \emph{backward} reachable from $R_0$. The largest
bisimulation is obtained by taking the complement of this space.
Geometrically,
%$R_0$ is represented by $\kernel(\phi)^\bot$, while
``going backward" means working with the transpose transition
functions  $\transp   t_a$  rather than with $t_a$. Taking the
complement of a relation  actually means taking its annihilator.
 This essentially leads one to work within $\dual V$ rather than $V$.
  Recall that $U+W$   denotes $\Span(U\cup W)$.

\begin{theorem}[partition refinement, backward version]\label{Th:Largest}%largest bisimulation,
Let $(V,<o,t>)$
%=(V,\{T\}_{a\in A},\phi)$
be a \lwa. Consider the sequence $(\R_i)_{i\geq 0}$ of sub-spaces of $\dual V$ inductively defined by:
% \vspace*{-.4cm}
\begin{eqnarray}\label{eq:Back}
\begin{array}{rclrcl}
\R_0 & = & \kernel(o)^\ann &\quad \R_{i+1} & = &\R_i + \sum_{a\in
A}\transp t_a (\R_i)\,.
\end{array}
%\vspace*{-.3cm}
\end{eqnarray}
Then there is $j\leq \dimn(L)$ such that $\R_{j+1}=\R_j$. The
largest $\fL$-bisimulation $\approx_\fL$ is  $\R_j^\ann$, modulo the
natural isomorphism between $V$ and $\ddual V$.
\end{theorem}
\begin{proof}
Since $\R_0\subseteq \R_1\subseteq \R_2\subseteq\cdots\subseteq \dual V$, the sequence of the dimensions of these spaces  is non-decreasing. As a consequence, for some $j\leq \dimn(\dual V)=\dimn(L)$, we get $\dimn(\R_j)=\dimn(\R_{j+1})$. Since $\R_j\subseteq \R_{j+1}$, this implies $\R_j= \R_{j+1}$.

We next show that $\R_j^\ann$ is an $\fL$-bisimulation. Indeed, by
the properties of annihilators and up  to the natural isomorphism:
(1) $\kernel(o)^\ann\subseteq \R_j$ implies
$(\kernel(o)^\ann)^\ann=\kernel(o)\supseteq \R_j^\ann$. Moreover:
(2) for any $a\in A$, $\transp t_a(\R_j)\subseteq \transp
t_a(\R_j)+\R_j\subseteq \R_{j+1}=\R_j$ implies, by Lemma
\ref{lemma:Transp}, that $\transp(\transp
t_a(\R_j^\ann))=t_a(\R_j^\ann)\subseteq\R_j^\ann$; by (1), (2) and
Lemma \ref{lemma:BisAlt}, we conclude that $\R_j^\ann$ is an
$\fL$-bisimulation.

We finally show that any  $\fL$-bisimulation $R$ is included in
$\R_j^\ann$. We do so by proving that for each $i$,
%$S^\bot\supseteq \R_i$, which implies
$R\subseteq \R_i^\ann$, thus, in particular $R\subseteq \R_j^\ann$.
We proceed by induction on $i$. Again by Lemma \ref{lemma:BisAlt},
we know that $\R_0^\ann=\kernel(o)\supseteq R$. Assume now
$R\subseteq \R_i^\ann$, that is,   $R^\ann\supseteq \R_i$. For each
action $a$,  by Lemma \ref{lemma:BisAlt} we have that
$t_a(R)\subseteq R$, which implies $\transp t_a(R^\ann)\subseteq
R^\ann$ by Lemma \ref{lemma:Transp}. Hence
 $R^\ann\supseteq  \transp t_a(R^\ann)\supseteq \transp t_a(R_i)$,
 where the last inclusion stems from $R^\ann\supseteq \R_i$.
 Since this holds for each $a$, we have that
 $R^\ann\supseteq \sum_a  \transp t_a(R_i) + R_i = R_{i+1}$.
  Taking the annihilator of both sides reverses the inclusion and yields  the wanted result.
\end{proof}

%\mik{Discutere algoritmo per il computo delle basi di $\R_i$.}

We note that what is being ``refined" in the algorithm above are
not, of course, the sub-spaces $R_i$, but their   complements:
$R^\ann_0\supseteq R^\ann_1\supseteq \cdots  \supseteq
R^\ann_j=\approx_\fL$. In particular, we start with a ``small" space
$R^\ann_0$ of dimension $\leq 1$: this may represent an advantage in
a practical implementation of the algorithm.
%One could also devise a   version of the above  algorithm that     starts from $\kernel(\phi)$ and refines it working forward, operating with intersections of sub-spaces rather than with sums. %$S_i\cap \bigcap_{a\in A} T_a(S_i)$.
%This ``forward" version appears to be less convenient computationally\iffull for at least two reasons.  First, \else as \fi $\kernel(\phi)$ is a large sub-space:   since  $\phi:V\rightarrow \reals$  with $\dimn(\reals)=1$, by virtue of the fundamental identity relating the dimensions of the kernel and of the image of a morphism\iffull (see equality (\ref{Eq:FundId}) in Appendix \ref{App:Linear})\fi, we have that $\dim(\kernel(\phi))\geq \dimn(V)-1$.  \iffull Second,     the backward algorithm   returns at no additional cost the orthogonal complement of $\sim_\fL$, which is anyway necessary to build the minimal automaton (see next section),   this is not the case for the forward version. \fi %Third, computing intersections of vector spaces may be computationally not as easy as computing sums of them.
%}
%\end{remark}


To conclude the section, we briefly discuss some practical aspects
involved in the implementation of the algorithm. By virtue of
(\ref{Eq:LinEq}), checking $v_1\approx_\fL v_2$, for any pair of vectors
$v_1$ and $v_2$, is equivalent to checking $v_1-v_2\in
\kernel(\approx_\fL)$. This can be done by first computing a basis
of $\approx_\fL$ and then checking for linear (in)dependence of
$v_1-v_2$ with respect to this basis. Alternatively, and more efficiently, one
can check whether $v_1-v_2$ is in  $R_j^\ann$, or, more explicitly,
whether $[v_1-v_2,\psi]=0$ for each $\psi\in R_j$. This reduces to
showing whether $[v_1-v_2,\psi]=0$ for each $\psi\in B_j$, where
$B_j$ is a basis for $R_j$. Thus, our task reduces to computing
such a basis. To do so,   fix any   basis $B$ of $V$ and let $O$ and
$T_a$ ($a\in A$) be the row-vector and matrices, respectively, representing the
weight function $o$ and transition functions $T_a$ of the \lwa\  in the basis $B$. The
concrete computations are carried out representing vectors and
functionals in this basis.
\begin{enumerate}
\item Compute a basis $B_0$ of $R_0$. %Assume that $f=(f_1,...,f_n) $  is not null, say $f_1\neq 0$, otherwise $\sim_\fL$ is trivially the universal relation.
%First, observe that, since it holds $\phi:V\rightarrow \reals$  with $\dimn(\reals)=1$, by virtue of the fundamental identity (\ref{Eq:Fund}), we have that
As already discussed, $\dim(\kernel(o))\geq \dimn(V)-1$, hence
$\dim(\kernel(o)^\ann)\leq 1$.
%In this case, $\kernel(\phi)$ has dimension $n-1$ and $ \kernel(\phi)^\bot$ has dimension
%$1$.
It is   readily checked that $o\in \kernel(o)^\ann$, thus
$\kernel(o)^\ann$  is spanned by   $o$. We thus   set $B_0=\{o\}$.
With respect to the chosen basis $B$, $B_0$ is represented by
$\{O\}$.

\item    For each $i\geq 0$, compute a basis $B_{i+1}$ of
$R_{i+1}  $. This can be obtained by    incrementally joining to
$B_i$ the functionals $\transp t_a (\psi)$, for $a\in A$ and
$\psi\in B_i$, that are linearly independent from previously joined
functionals. With respect to the basis $B$, $\transp t_a (\psi)$ is
represented by $\Psi \times T_a$, where $\Psi$ is the row-vector
representing $\psi$; checking linear independence of  $\transp t_a
(\psi)$ means hence checking linear independence of $\Psi \times
T_a$ from previously joined row-vectors.
\end{enumerate}
After   $j\leq n$ iterations, one finds a set $B_j$ such that
$B_{j+1}=B_j$: this is the basis of $R_j$.
%At each stage of this procedure, the dominant operation is a matrix-vector multiplication, which yields a loose upper bound of $O(n^3)$ floating point operations. Better bounds can probably be obtained under the assumption that the matrices $M_a$ are sparse, which is likely to be the case in many situations.
 %There are standard methods  to construct    first   a basis of $\kernel(\phi)$ and then taking its orthogonal complement -- we will not discuss them here. Then consider the sequence  of bases $(B_i)_{i\geq 0}$,
%Each $B_{i+1}$  is built  by   joining  to  $B_i$  the  vectors  $\transp T_a v$, with   $a\in A$ and $v\in B_i$, that are linearly independent from $B_i$ and from the vectors previously joined.  Note that when implementing this  construction: (a)   it is  convenient to give a representation of the \lwa\ w.r.t.   an  {ortho-normal} basis,  so that the matrices representing $T_a$ and $\transp T_a$ are transpose of one another; (b) checking linear independence between vectors  can be done using standard methods from Linear Algebra (and efficiently so, if the coordinates of elements in $B_i$'s can be arranged to form sparse matrices).
%Eventually, we reach a $j$ such that $B_j=B_{j+1}$: this is the basis of $R_j$, the orthogonal complement of $\sim_\fL$.   Standard methods from Linear Algebra could be used at this point to build  out of $B_j$ a basis of $ R_j^\bot=\sim_\fL$. %$\Span(B_j)^\bot$ , that is $\sim_\fL$.
%However, as hinted above,  this last step is not actually  needed:
%Now, checking   $u\sim_\fL v$, that is,  by (\ref{Eq:LinEq}),  $u-v\in \kernel(\sim_\fL)=R^\bot_j$, is the same as checking that $u-v$  lies in the orthogonal complement of  $ \Span(B_j)$.  To check the latter, it suffices to check that $u-v$ is in orthogonal to every vector in $B_j$.
We illustrate this algorithm in the example below.

%
%
%\begin{example}\label{Ex:Bisim}
%\mik{Fix references.} {Consider the \lwa\
%$(\K^Q,<o_Q^{\sharp},t_Q^{\sharp}>)$ in the left of Figure
%\ref{fig:isomorphic}.
%%$Q$ is an   ortho-normal basis,  so that it is easy to represent the transpose transitions $\transp T_a$.
%The vector $O=(1\;1\;1)$ represents $o$ in the chosen basis $Q$,
%hence, according to the above outline of the algorithm,  we can set
%$B_0=\{O\}$. Next, we apply the algorithm to build the  $B_i$'s.
%Manually, the computation  of the vectors $\Psi T_a=\transp(\transp
%T_a \transp \Psi)$ can be carried out by looking at the transitions
%of the \wa\ with arrows reversed. Doing so, we first get  $O
%T_a=(0\;1\;1)$ and then $(0\;1\;1)T_a=(0\;1\;1)$.
% So we obtain
% \[
%  \begin{array}{rclrcl}
%  B_0  &  = &  \{(1\;1\;1)\}\\
%  B_1  & =  &  \{(1\;1\;1), \; (0\;1\;1)\}\\
% B_2  & =  & B_1  \,.
%  \end{array}
%  \]
%The functionals represented by vectors in $B_1$ are  a basis of
%$(\approx_L)^\ann$. As an example, let us check that $q_1\approx_L
%q_1+q_2-q_3$. To this purpose, note that the difference vector
%$(q_1+q_2-q_3)-q_1=q_2-q_3$   annihilates $B_1$, that is
%$[q_2-q_3,u]=0$ for each $u\in B_1$,
%%(this is computed using (\ref{Eq:ComputeFunc})),
% which is equivalent to  $q_1\approx_L q_1+q_2+q_3$.
%}
%\end{example}

\medskip

\begin{example}\label{Ex:Bisim}
%{
Consider the \lwa\ $(V,<o,t>)$ on the left of Figure~\ref{fig:ExAlgorithm}. At the beginning we can set $B_0=\{O\}$.
Next, we apply the algorithm to build the  $B_i$'s. Manually, the
computation of the vectors $\Psi \times  T_a =\transp(\transp T_a \times  \transp
\Psi)$ can be carried out by looking at the transitions of the \wa\
with arrows reversed (in the right of Figure~\ref{fig:ExAlgorithm}).
%
Doing so, we first get  $O\times  T_a=(2\;\frac{2}{3}\;2)$ and $O
\times  T_b=(2\;\frac{1}{3}\;3)$. Note that $O\times  T_b$ is not linearly independent
 from the other vectors: $O\times  T_b=-(2\;1\;1)+2(2\;\frac{2}{3}\;2)$. Thus $B_1=\{(2\;1\;1),
 \; (2\;\frac{2}{3}\;2)\}$. In the second iteration, we compute $(2\;\frac{2}{3}\;2)
\times  T_a=(2\;\frac{2}{3}\;2)$ and $(2\;\frac{2}{3}\;2) \times
 T_b=(2\;\frac{2}{3}\;2)$ and thus $B_2=\{(2\;1\;1),
 \;(2\;\frac{2}{3}\;2)\}$ that is equal to $B_1$.

The functionals represented by vectors in $B_1$ are  a basis of
$(\approx_\fL)^\ann$. As an example, let us check that $x_1
\approx_\fL \frac{3}{2}x_2 + \frac{1}{2}x_3$. To this purpose, note
that the difference vector $x_1 - \frac{3}{2}x_2 - \frac{1}{2}x_3$
annihilates $B_1$, that is $$[\left(%
\begin{array}{c}
  1 \\
  -\frac{3}{2}\\
  -\frac{1}{2}\\
\end{array}\right),u]=0$$ for each $u\in B_1$,
%(this is computed using (\ref{Eq:ComputeFunc})),
 which is equivalent to $x_1
\approx_\fL \frac{3}{2}x_2+\frac{1}{2}x_3$.
%}
\end{example}

It is quite easy to give an upper bound on the cost of the backward algorithm, in terms of the number of required elementary operations, that is sum and product operations in the underlying field. % and assuming constant times for each of these operations.

A first, crude   analysis is as follows. Let $n$ be the dimension of $V$. Each time we join a new vector $v=\Psi\times T_a$   to the basis $B$, we have a cost of $O(n^2)$ for vector-matrix multiplication, plus a cost of $O(n^3)$ for checking linear independence of $v$ from $B$, for a predominant cost of $O(n^3)$. Since the operation of joining a vector to the basis cannot be done more than $n$ times, we have a global cost of $O(n^4)$.

In fact, this complexity can be improved if we maintain the vectors in the  basis $B$ in \emph{canonical echelon form}, that is, as columns,
they can be arranged  to form   a   matrix that can be augmented so as to become a lower triangular matrix with $1$ on the main diagonal
(in other words,    the first nonzero entry from the top  of any column is $1$ and  lies   below the  first nonzero entry  of the column
on its left). In this case, exploiting the form of $B$, checking that  any vector $v$ is linearly independent from $B$ can be done solving  a system of $k$ equations with $n$ unknowns,  where $k$ is the current cardinality of $B$: using Gaussian elimination, this    costs   $O(kn)$ operations.  Moreover, in case $v$ is linearly independent  from $B$, we can compute a new vector $v'$ such that $\Span(B\cup\{v'\}) = \Span(B\cup\{v\})$  and $B\cup\{v'\}$ is still in canonical echelon form: this $v'$, rather than $v$, is therefore joined to $B$. Using elementary   linear algebra (see e.g. \cite{Saad}), the computation of $v'$ can be done using again $O(kn)$ operations. This modification takes the overall complexity of the algorithm to $O(n^3)$. This matches the complexity of     Sch\"{u}tzenberger's original  minimization algorithm,  as analysed  e.g. by Sakarovitch in \cite[Ch.4]{wahandbook}. This algorithm  can also be used  for deciding  equality between two recognazible formal power series.


%In the case $|A|=1$, one can adapt the Arnoldi's iteration algorithm \cite{Saad} to compute $B$, which takes $O(n^3)$ operations.  It is not clear whether this algorithm can be adapted also to the case $|A| > 1$. In practical cases, the transition matrices tend to be   sparse,  and   the number of iterations after which the algorithms stops may be much less than $n$. By adopting suitable representations for sparse matrices, these circumstances can be used to lower  considerably the practical complexity of the algorithm.

%\cbox{For Michele: Add a remark about complexity}

%\ifnever


%\fi
%
%NEW OBSERVATIONS by MICHELE
%
%We can give a more explicit representation of $(\approx_L)^\ann$ by
%``unrolling" $R_j$, as follows.
%\begin{corollary}\label{cor:unfold}
%Let $(V,<o,t>)$ be a \lwa. Then
%\begin{eqnarray}\label{eq:Unroll}
% (\approx_\fL)^\ann\quad =\quad
% %\Span\{\phi\comp T_x\,|\,x\in A^*,\,|x|<j\}\;=\;
% \Span\{o\comp t_w\,|\,w\in A^*\}\,
%\end{eqnarray}
%where $t_w \colon V \to V$ is defined by $t_{\epsilon}(v)=v$ and
%$t_{aw'}(v)=t_{w}(t(v)(a))$.
%\end{corollary}
%\begin{proof}
%Using the fact that   linear morphisms distribute over sum of
%sub-spaces,     that $ \transp f  \comp\transp g = \transp{(g\comp
%f)}$  for any $f$ and $g$, and that $\ker(o)^\ann=\Span\{o\}$, we
%can unfold the definition of $R_j=(\approx_\fL)^\ann$ using
%(\ref{eq:Back}) and obtain
%\[
%(\approx_\fL)^\ann\quad =\quad R_j\quad =\quad \sum_{w\in A^*,|w|<j}
%\transp{t_w}(\Span\{o\})\,
%\]
%where $|w|$ denote the length of the word $w$. Since $\Span(-)$
%commutes with linear maps and sum of sub-spaces and
%$\transp{t_w}(o)=o\comp\transp t_w$, for any $w\in A^*$, we get
%\[
% (\approx_\fL)^\ann\quad =\quad
% \Span\{o\comp t_w\,|\,w\in A^*,\,|w|<j\}\;=\;
% \Span\{o\comp t_w\,|\,w\in A^*\}\,.
% \]
%where the last equality follows from the fact that any $o\comp
%t_{w'}$ with $|w'|\geq j$ is linearly dependent from the elements
%$o\comp t_w$ with $|w|< j$.
%\end{proof}
%
%
%\begin{remark}
%\mik{Filippo, this remark may be related to your search for terminal
%sequences. Please check consistency of notation and terminology with
%previous sections.
%
%Filippo: It is not easy to relate this argument with the final
%sequence since there we substantially look at the length of the
%words} As a matter of fact, there is a \emph{direct} derivation of
%(\ref{eq:Unroll}) from the properties of the final morphism
%$\beh{\fL}{V}{-} \colon V\rightarrow \K^{A^*}$. Recall that $\approx_\fL
%=\ker(\beh{\fL}{V}{-})$. Now, $v\in \ker(\beh{\fL}{V}{-})$ if and only
%if, for each $w\in A^*$, $\beh{\fL}{V}{v}(w)=(o\comp t_w)(v)=0$. In
%other words,
%\[
% \ker(\beh{\fL}{V}{-} )\;=\;\bigcap_{w\in A^*} \ker(o\comp t_w)\,.
%\]
%This  implies that we can compute $(\approx_\fL)^\ann $ as follows
%(here we use that fact that $(\cap_i U_i)^\ann= \sum_i U_i^\ann$,
%and that $(\ker(\psi))^\ann =\Span\{\psi\}$ for any functional
%$\psi$):
%\[
%\begin{array}{rcl}
%(\approx_\fL)^\ann   & = & \ker(\beh{\fL}{V}{-})^\ann\\
%                & = & \big(\bigcap_{w\in A^*} \ker(o\comp t_w)\big)^\ann\\
%                & = & \sum_{w\in A^*}  \big(\ker(o\comp t_w)\big)^\ann \\
%                & = & \sum_{w\in A^*} \Span\{o\comp t_x\} \\
%                & = & \Span\{o\comp t_w\,|\,w\in A^*\} \,.
%%                & = & R_j
%\end{array}
%\]
%%where the last step follows from (\ref{eq:Unroll}).  In conclusion, we have found that $\sim_\fL^\ann= \ker(\Phi)^\ann= R_j$, that is, taking the annihilator on both sides  and modulo the natural isomorphism
%%\[
%% \approx_\fL= R_j^\ann\,.
%% \]
%\end{remark}


%\newpage
\subsection{The final sequence and the forward algorithm}\label{sec:finalsequence}
%
%\cbox{Insert some motivating arguments}

The theory of coalgebras also provides a way of constructing final
coalgebras by means of \emph{final sequences} (often referred in
literature as terminal sequences)~\cite{Barr93}. Many important
algorithms for computing behavioural equivalences (such as
~\cite{KannelakisSmolka}) can be abstractly described in terms of
final sequences.

%Under certain conditions on the category $C$ and the functor
%$G\colon C \to C$, the final sequence of $G$ converges to the final
%$G$-coalgebra. This allows to define an algorithm for computing
%$G$-behavioural equivalence that

In this section, we describe the relationship between the forward
algorithm (in Proposition \ref{prop:forw}) and the final sequence of
the functor $\fL$. The latter is the cochain
$$1 \stackrel{!}{\longleftarrow} \fL1\stackrel{\fL!}{\longleftarrow} \fL^{2}1 \stackrel{\fL^2!}{\longleftarrow} \dots$$
where $\fL^{n+1}1$ is $\fL\comp (\fL^{n}1)$, $\fL^01=1$ is the final
vector space $\{0\}$, and $!$ is the unique morphism from $\fL1$ to
$1$.

Let $A^*_{n}$ be the set of all words $w\in A^*$ with \emph{length}
smaller than $n$. For each $n$, $\fL^n1$ is isomorphic to $\K^{A^*_{
n}}$, i.e., the space of functions from $A^*_{n}$ to $\K$. Indeed,
for $n=1$, $\fL1$ is by definition $\K\vectproduct 1^A=\K$ that is
isomorphic to the space of functions from $A^*_{1}=\{\epsilon\}$ to
$\K$; and for $n+1$, each $<k, \sigma> \in \K\vectproduct
\fL^n(1)^A=\fL^{n+1}1$ can be seen as a function $A^*_{ n+1} \to \K$
mapping $\epsilon$ into $k$ and $aw$ (for $a\in A$ and $w\in
A^*_{n}$) into $\sigma(a)(w)$.

%
%
%\begin{lemma}
%$L^n(1)$ is isomorphic to $\K^{A^*_{\prec n}}$.
%\end{lemma}
%\begin{proof}
%By induction on $n$. For the base case note that $L(1)$ is by
%definition $\K\vectproduct 1^A=\K$ that is isomorphic to
%the space of functions from $A^*_{\prec
%1}=\{\epsilon\}$ to $\K$. %Now $\fL^2(1)$ is isomorphic to
%%$\K\vectproduct \K^A$. Each $<v, \phi> \in
%%\K\vectproduct \K^A$ corresponds to a function $A^*_{\prec
%%2} \to \K$ that maps $\epsilon$ into $v$ and each $a\in A$
%%into $\phi(a)$.
%
%For the inductive case, note that $\fL^{n+1}(1)$ is by definition
%$\K\vectproduct \fL^n(1)^A$. For each $<v, \phi> \in
%\K\vectproduct \fL^n(1)^A$ there is a function $A^*_{\prec n+1} \to
%\K$ mapping $\epsilon$ into $v$  and $aw$ (for $a\in A$ and
%$w\in A^*_{\prec n}$) into $\phi(a)(w)$.
%\end{proof}
%

For $\sigma \colon A^*_{m}\to \K$ and $n\leq m$, the
$n$-\emph{restriction} of $\sigma$ is $\sigma\upharpoonleft n \colon
A^*_{n}\to \K$ defined as $\sigma$, but in a restricted domain. The
morphism $\fL^n! \colon \fL^{n+1}1 \to \fL^n1$ maps each $\sigma$
into $\sigma\upharpoonleft n$.


The limit of this cochain is $\K^{A^*}$ together with the maps
$\zeta_n \colon \K^{A^*} \to \fL^{n}1$ that assign to each weighted
language $\sigma$ its $n$-restriction $\sigma \upharpoonleft n$.

$$\xymatrix@C=12pt@R=12pt{&&& \K^{A^*}\ar@(r,u)[dl]|(0.8){\zeta_2} \ar@(r,u)[dll]|(0.8){\zeta_1} \ar@(r,u)[dlll]|(0.8){\zeta_0}\\
1  & \fL1 \ar[l]_{!}  & \fL^21\ar[l]_{\fL!} & \dots\ar[l]_{\fL^2!}
}$$

\medskip

\noindent Every $\fL$-coalgebra $(V,<o,t>)$ defines a cone $!^{n}
\colon V \to \fL^n 1$ as follows:
\begin{itemize}
\item $!^0 \colon V \to 1$ is the unique morphism to the final vector space
$1$,
\item $!^{n+1} \colon V \to \fL^{n+1}1=\fL(!^{n})\comp <o,t>$.
\end{itemize}
%
%By definition of $\fL$, the latter is $id_\K \vectproduct
%!^{{n}^A}\comp <o,t>$ that maps each vector $v\in V$ into $<o(v),
%\lambda a. !_n(t(v)(a))>$. More concretely, $\forall v\in V$ and
%$w\in \K^{A^*_{\prec n+1}}$,
%
The latter can be more concretely defined for all $v\in V$ and $w\in
\K^{A^*_{n+1}}$ as
$$!^{n+1}(v)(w)= \left\{%
\begin{array}{ll}
    o(v), & \hbox{if $w=\epsilon$;} \\
    !^n(t(v)(a))(w'), & \hbox{if $w=aw'$.} \\
\end{array}%
\right.$$
%


Note that the final morphism $\beh{-}{\fL}{V} \colon V \to \K^{A^*}$
(mapping each $v\in V$ in the language that it recognizes) is the
unique function such that for all $n$, $\zeta_n \comp
\beh{-}{\fL}{V} = !^n$.
$$\xymatrix@C=12pt@R=12pt{&&& \K^{A^*}\ar@(r,u)[dl]|(0.8){\zeta_2} \ar@(r,u)[dll]|(0.8){\zeta_1} \ar@(r,u)[dlll]|(0.8){\zeta_0}\\
1  & \fL1 \ar[l]_{!}  & \fL^21\ar[l]_{\fL!} & \dots\ar[l]_{\fL^2!} \\
&&& V \ar@{.>}[uu]_(0.6){\beh{-}{\fL}{V}} \ar@(r,d)[ul]|(0.8){!_2}
\ar@(r,d)[ull]|(0.8){!_1} \ar@(r,d)[ulll]|(0.8){!_0}}$$


Recall that the $\fL$-behavioural equivalence on $(V,<o,t>)$ is the
kernel of $\beh{-}{\fL}{V}$. The forward algorithm computes it, by
iteratively computing the kernel of the morphisms $!^n$.
%
\begin{proposition}
Let $(V,<o,t>)$ be a \lwa. Let $R_n$ be the relation computed by the
forward algorithm (Proposition \ref{prop:forw}). Let $!^n \colon
V\to \fL^n1$ be the morphisms described above. Then for all natural
numbers $n$, $R_n=\kernel(!^{n+1})$.
\end{proposition}

\begin{proof}
First of all, note that the kernel of $!^0 \colon V \to 1$ is the
whole $V$. The kernel of $!^{n+1}$ is the space composed of those $v\in V$ such
that $!^{n+1}(v)(w)=0$ for all the words $w\in A^*_{n+1}$, i.e.,
$$\kernel(!^{n+1})=\{v\in V \;|\; o(v)=0 \text{ and }\forall
a\in A, \; t(v)(a) \in \kernel(!^n) \}\text{.}$$

By induction on $n$, we prove that $\kernel(!^{n+1})=R_n$.

For $n=0$, note that $\kernel(!^{1})= \{v\in V \;|\; o(v)=0 \text{
and }\forall a\in A, \; t(v)(a) \in \kernel(!_0) \}$. Since
$\kernel(!^0)=V$, $\kernel(!^{1})= \{v\in V \;|\; o(v)=0 \}=R_0$.

As induction hypothesis suppose that $\kernel(!^{n})=R_{n-1}$. Then
$\kernel(!^{n+1})=\{v\in V \;|\; o(v)=0 \text{ and }\forall a\in A,
\; t(v)(a) \in R_{n-1} \}=R_n$.
\end{proof}

This result can be seen as an alternative proof of the soundness of
the forward algorithm. Indeed, if $R_j$ is the result of the
algorithm, for all $k\geq j$, $R_k=R_j$, i.e.,
$\kernel(!^k)=\kernel(!^j)$. Thus $R_j=\bigcap_n \kernel(!^n)$ and,
by definition of $!^n$, $\bigcap_n
\kernel(!^n)=\kernel(\beh{-}{\fL}{V})$.

%
%
%\begin{proof}
%First of all note that the kernel of $!^0\colonV \to 1$ is the whole $V$.
%By definition $!^{n+1}=\fL(!^n) \comp <o,t>$ and thus
%$\kernel(!^{n+1})= \kernel(\fL(!^n) \comp <o,t>)=\{v\in V\text{ s.t. }
%<o,t>(v) \in \kernel(\fL(!^n))\} = \{v\in V\text{ s.t. } o(v)=0 \text{
%and }\forall a\in A, \; t(v)(a) \in \kernel(!^n) \}$. Now we can
%prove, by induction on $n$, that $\kernel(!^{n+1})=R_n$.
%
%For the base case note that $\kernel(!^{1})= \{v\in V\text{ s.t. }
%o(v)=0 \text{ and }\forall a\in A, \; t(v)(a) \in \kernel(x_0) \}$
%and, since $\kernel(!^0)=R$, $\kernel(!^{1})= \{v\in V\text{ s.t. }
%o(v)=0 \}=R_0$.
%
%For the inductive case suppose that $\kernel(!^{n})=R_{n-1}$. Then
%$\kernel(!^{n+1})=\{v\in V\text{ s.t. } o(v)=0 \text{ and }\forall
%a\in A, \; t(v)(a) \in R_{n-1} \}=R_n$.
%
%\end{proof}

%
% FINAL SEQUENCE MORE ABSTRACTLY
%
%\newpage
%\cbox{The terminal sequence \cite{} of a endofunctor $G$ is the
%ordinal-indexed cochain $(z_{\beta,\alpha} \colon Z_{\beta} \to
%Z_{\alpha})_{\alpha\leq \beta}$ defined as follows:
%\begin{itemize}
%\item If $\lambda$ is a limit ordinal, then $Z_{\lambda}=lim\{z_{\beta,\alpha} \colon Z_{\beta} \to
%Z_{\alpha} \text{ s.t. } \alpha\leq \beta \prec \lambda \}$ and the
%cone $\{z_{\lambda, \alpha} \colon Z_{\lambda}\to Z_{\alpha} \}$ is
%the limiting one; e.g. $Z_0=1$.
%\item $Z_{\alpha+1}=B(Z_{\alpha})$ and $z_{\beta+1,\alpha+1}=G(z_{\beta,\alpha}) \colon Z_{\beta+1}\to
%Z_{\alpha+1}$.
%\end{itemize}
%%
%Every $G$-coalgebra $(X,f)$, determines a cone $x_{\alpha} \colon X
%\to Z_{\alpha}$ as follows:
%\begin{itemize}
%\item If $\lambda$ is a limit ordinal, then the morphisms $x_\alpha \colon X \to
%Z_{\alpha}$ for $\alpha \prec \lambda$ forms a cocone over the
%cochain $(z_{\beta,\alpha} \colon Z_{\beta} \to
%Z_{\alpha})_{\alpha\leq \beta}$ having apex $X$. We take
%$x_{\lambda} \colon X \to Z_{\lambda}$ to be the unique mediating
%morphism: e.g., for $\lambda =0$, $x_{\lambda} \colon X \to
%Z_{\lambda}$ is the final map $X \to 1$.
%
%\item $x_{\alpha+1}$ is the composition of $f \colon X \to B(X)$ and $B(X)\to B(Z_n)=Z_{n+1}$.
%\end{itemize}
%If $G$ is \emph{accessible} then the terminal sequence converges for
%some $\gamma$, meaning that $z_{\gamma,\gamma+1}$ is an iso. In this
%case $z_{\gamma,\gamma+1}^{-1} \colon Z_{\gamma} \to G(Z_{\gamma})$
%is a final $G$-coalgebra \cite{} and $x_{\gamma} \colon X \to
%Z_{\gamma}$ is a final morphism mapping each systems in its
%behaviour.
%
%The terminal sequence often converges for some ordinals $\gamma$
%bigger than $\omega$, but this is not problematic in practical
%application since one is not interested in the whole final coalgebra
%but just in the equivalence that it induces on a coalgebra $(X,f)$,
%i.e., in \emph{kernel} of the final morphism $x_{\gamma}$. Whenever
%$X$ is ``finite'', this can be usually computed by iteratively
%computing the kernel of the morphisms $x_\alpha$.
%
%When taking the functor $G$ equal to $\fL$, this is equivalent to
%executing the forward algorithm described in Section \ref{}. This is
%made formal by the following proposition.
%
%\begin{proposition}
%Let $(V,<o,t>)$ be a \lwa. Let $R_i$ be the relation computed by the
%forward algorithm (Proposition \ref{}). Let $x_i \colon V\to Z_i$ be
%the morphisms described above. Then $\forall i$,
%$R_i=\kernel(x_{i+1})$.
%\end{proposition}
%\begin{proof}
%First of all note that the kernel of $x_0 \colon V \to 1$ is the
%whole $X$. By definition $x_{n+1}=\fL(x_n) \comp <o,t>$ and thus
%$\kernel(x_{n+1})= \kernel(\fL(x_n) \comp <o,t>)=\{v\in V\text{ s.t. }
%<o,t>(v) \in \kernel(\fL(x_n))\} = \{v\in V\text{ s.t. } o(v)=0 \text{
%and }\forall a\in A, \; t(v)(a) \in \kernel(x_n) \}$.
%
%Now we can prove, by induction on $n$, that $\kernel(x_{n+1})=R_n$.
%
%For the base case note that $\kernel(x_{1})= \{v\in V\text{ s.t. }
%o(v)=0 \text{ and }\forall a\in A, \; t(v)(a) \in \kernel(x_0) \}$
%and, since $\kernel(x_0)=R$, $\kernel(x_{1})= \{v\in V\text{ s.t. }
%o(v)=0 \}=R_0$.
%
%For the inductive case suppose that $\kernel(x_{n})=R_{n-1}$. Then
%$\kernel(x_{n+1})=\{v\in V\text{ s.t. } o(v)=0 \text{ and }\forall
%a\in A, \; t(v)(a) \in R_{n-1} \}=R_n$.
%\end{proof}
%
%
%}
%%
%


\section{Weighted languages and rationality}\label{sec:rational}
We recall from Section \ref{sec:linearcoalg} that a linear weighted automaton (\lwa)
is a coalgebra for the functor
$\fL = \K \vectproduct -^A$, i.e., it consists of a vector space $V$ and a linear map $<o,t>:V \to \K \vectproduct V^A$.
We saw in Theorem \ref{prop:final} that the final homomorphism
\[
\beh{-}{\fL}{V} : \, V \to \K^{A^*}
\]
maps every vector $v \in V$ to the weighted language
$\beh{v}{\fL}{V}$ that is accepted by $v$.
%
Moreover, the kernel of this morphism is weighted language equivalence ($\approx_{\fL}$) that, when $V$ is finite dimension,
can be computed via the linear partition refinement algorithm (shown in Section \ref{sec:linearpr}).

The languages in $\K^{A^*}$ that are accepted by \lwa\ with finite dimension states spaces are called \emph{rational} weighted languages
(which are also known as rational formal power series) and they can be syntactically represented by a language of expressions \cite{Schutzenberger61b,Rutten2003-behavioural-differential-equations}.

In this section, we shall directly characterise $\beh{-}{\fL}{V}$ by showing the expression of $\beh{v}{\fL}{V}$ for each $v\in V$ (Theorem \ref{thm:rational}).
Then we shall employ this characterization for computing $\approx_{\fL}$.

We will first treat the special case of \lwa's over a one letter
alphabet $|A|=1$.
Next we will show how to treat the
general case of an arbitrary (finite) alphabet.

We note that for the case of $|A|=1$, the functor
$\fL$ is isomorphic
to
\[
\fL (V) = \,  \K \vectproduct V^A \cong \, \K \vectproduct V
\]
Moreover, the final $\fL$-coalgebra is isomorphic to
the set of streams over the field $\K$:
\[
\K^{A^*} \cong \, \K^\omega
\]
Therefore we shall proceed by recalling
from \cite{Rutten2008:rational-streams-coalgebraically}
the basics of stream
calculus and linear stream differential equations,
in
Subsections \ref{Recalling the basics of stream calculus} and
\ref{Solving linear systems of stream differential equations}.
Next we shall characterise the final homomorphism,
for the case $|A|=1$,
in Subsection \ref{Characterising the final morphism ($|A|=1$}.
Building on \cite{Rutten2003-behavioural-differential-equations},
we shall finally generalise these results
for finite alphabets, in
Subsection \ref{Rational weighted languages}.

\subsection{Recalling the basics of stream calculus}
\label{Recalling the basics of stream calculus}

We define the set of {\em streams\/} over the  field $\K$ by
\[
\K^\omega = \{ \sigma \mid \sigma : \, \mathbb{N }\to \K \}
\]
(where $\mathbb{N }$ is the set of natural numbers).

We often denote elements $\sigma
\in \K^\omega$ by $\sigma = (\sigma(0), \sigma(1), \sigma(2),
\ldots)$. We define the {\em stream derivative\/} of a stream $\sigma$ by
$\sigma' = (\sigma(1), \sigma(2), \sigma(3), \ldots)$, and the {\em
initial value\/} of $\sigma$ by $\sigma(0)$. This definition of initial value and derivative of streams forms the basis
for a calculus of streams, in close analogy
to classical calculus in analysis. Below we present some of its basics; we refer
the reader to \cite{Rut05b} for further details and motivations.

For $k \in \K$, we define the constant stream
\[
[k] = (k,0,0,0, \ldots)
\]
which we often denote again by $k$. Another constant stream is
\[
\X= (0,1,0,0,0, \ldots)
\]
For $\sigma,\tau \in \K\sp{\omega}$ and $n \in \omega$, the
operations of \emph{sum} and (convolution) \emph{product} are given
by
\[
(\sigma + \tau)(n) = \sigma (n) + \tau(n)\enspace,
\;\;\;\;\;\;\;\;\; (\sigma \streamproduct \tau)(n) = \sum_{i=0}^{n}
\sigma (i) \cdot \tau(n-i) \;\;\;\;\;\;
\]
(where, as usual $\; \cdot \;$ denotes product of $\K$).

We call a stream $\pi \in \K\sp{\omega}$ {\em polynomial\/} if there
are $n \geq 0$ and $a_i \in \K$ such that
\[
\pi = a_0 + a_1 \X + a_2 \X^2 + \cdots + a_n  \X^n \;\; = \; (a_0,\,
a_1 ,\, a_2 ,\, \ldots ,\, a_n , \, 0, \, 0, \, 0, \, \ldots )
\]
where we write $a_i\X^i$ for $[a_i] \streamproduct \X^i$ with $\X^i$
the $i$-fold product of $\X$ with itself.

A stream $\sigma$ with $\sigma(0)\neq 0$ has a (unique)
multiplicative inverse $\sigma^{-1}$ in $\K\sp{\omega}$, satisfying
\[
\sigma^{-1} \streamproduct \sigma = [1]
\]
As usual, we shall often write $1/\sigma$ for $\sigma^{-1}$ and
$\sigma / \tau$ for $\sigma \streamproduct \tau^{-1}$.  Note that
the initial value of the sum, product and inverse of streams is
given by the sum, product and inverse of their initial values.

We call a stream $\rho\in \K\sp{\omega}$ {\em rational\/} if it is
the quotient $\rho = \sigma/\tau$ of two polynomial streams $\sigma$
and $\tau$ with $\tau(0) \neq 0$.

One can compute a stream from its initial value and derivative by
the so-called \emph{fundamental theorem} of stream
calculus~\cite{Rut05b}: for all $\sigma \in \K\sp{\omega}$,
\begin{eqnarray}
\label{fundamental_theorem_of_stream_calculus}\sigma = \, \sigma(0)
+ (\X \streamproduct
\sigma')
\end{eqnarray}
(writing $\sigma(0)$ for $[\sigma(0)]$).

%
The fundamental theorem of stream calculus allows us to solve
\emph{stream differential equations} such as
\[
\sigma'= 3\streamproduct \sigma \, , \;\;\; \sigma(0) = 1
\]
by computing $\sigma = \sigma(0) + (\X \streamproduct \sigma') = 1+
(\X \streamproduct 3 \streamproduct \sigma )$, which leads to the
solution
\[
\sigma = \, 1/(1-3\X) = \, (1,\, 3 , \, 3^2 , \, 3^3 , \, \ldots )
\]



\subsection{Solving linear systems of stream differential equations}
\label{Solving linear systems of stream differential equations}
Using some elementary linear algebra notation (matrices and vectors),
we next show how to solve \emph{linear} systems of stream differential equations.
For notational convenience, we shall deal with linear systems of dimension 2,
which can be straightforwardly generalised to systems of higher dimensions.
They are given by the following data:
\begin{eqnarray}\label{linear_system_of_dimension_2}
\begin{pmatrix}%brackets around
\sigma \\ \tau
\end{pmatrix} '
= \; M \matrixproduct
\begin{pmatrix}%brackets around
\sigma \\ \tau
\end{pmatrix}
\;\;\;\;\;\;\;\;\;\;\;
\begin{pmatrix}%brackets around
\sigma \\ \tau
\end{pmatrix} (0)
= \;
N
\end{eqnarray}
where $M$ is a $2 \times 2$-matrix and $N$ is a $2 \times 1$-matrix
over $\K$:
\[
M = \;
\begin{pmatrix}%brackets around
m_{11} & m_{12} \\ m_{21} & m_{22}
\end{pmatrix}
\;\;\;\;\;\;\;\;
N = \;
\begin{pmatrix}%brackets around
n_1 \\ n_2
\end{pmatrix}
\]
for $m_{ij}, \, n_i \in \K$. The above notation is really just a
short hand for the following system of two stream differential
equations:
\[
\sigma ' = (m_{11} \streamproduct \sigma) + \, (m_{12}
\streamproduct \tau ) \;\;\;\;\;\;\;\; \sigma(0) = n_1
\]
\[
\tau  ' = (m_{21} \streamproduct \sigma) + \, (m_{22} \streamproduct
\tau ) \;\;\;\;\;\;\;\; \tau(0) = n_2
\]
%\begin{matrix}%no brackets
%2&0\\ 3&0
%\end{matrix}
%If you replace pmatrix by bmatrix, Bmatrix, vmatrix and Vmatrix you will
%get as delimeters [ ], { },  | | , and || ||.
We can solve such a system of equations by using twice the fundamental theorem
of stream calculus (equation (\ref{fundamental_theorem_of_stream_calculus}) above),
once for $\sigma$  and once for $\tau$:
\[
\sigma = \; \sigma(0) + \, (\X \streamproduct \sigma ')
\]
\[
\tau = \; \tau(0) + \, (\X \streamproduct \tau ')
\]
In matrix notation, the fundamental theorem looks like
\[
\begin{pmatrix}%brackets around
\sigma \\ \tau
\end{pmatrix}
= \;
\begin{pmatrix}%brackets around
\sigma \\ \tau
\end{pmatrix} (0)
\; + \; \X \streamproduct \,
\begin{pmatrix}%brackets around
\sigma \\ \tau
\end{pmatrix} '
\]
Next we can solve our linear system
(\ref{linear_system_of_dimension_2})
above by calculating as follows:
\begin{eqnarray*}
\begin{pmatrix}%brackets around
\sigma \\ \tau
\end{pmatrix}
& = &
\begin{pmatrix}%brackets around
\sigma \\ \tau
\end{pmatrix} (0)
\; + \; \X \streamproduct \,
\begin{pmatrix}%brackets around
\sigma \\ \tau
\end{pmatrix} '
\\
& = & N \; + \; \X \streamproduct M \streamproduct
\begin{pmatrix}%brackets around
\sigma \\ \tau
\end{pmatrix}
\end{eqnarray*}
This leads to
\[
(I - (\X \streamproduct M))
\begin{pmatrix}%brackets around
\sigma \\ \tau
\end{pmatrix}
\, = \; N
\]
where $I$ and $\X \streamproduct M$ are given by
\[
I = \, \begin{pmatrix}
1 & 0 \\ 0 & 1
\end{pmatrix}
\;\;\;\;\;\; \X \streamproduct M = \,
\begin{pmatrix}
m_{11} \streamproduct \X \; & m_{12} \streamproduct \X \\
m_{21} \streamproduct \X \; & m_{22} \streamproduct \X
\end{pmatrix}
\]
Finally, we can express the unique solution of our linear system
of stream differential equations as follows:
\[
\begin{pmatrix}%brackets around
\sigma \\ \tau
\end{pmatrix}
\, = \; (I - (\X \streamproduct M))^{-1} \, \streamproduct \, N
\]
The advantage of the matrix notations above now becomes clear:
we can compute the inverse of
the matrix
\[
(I - (\X \streamproduct M)) \, = \;
\begin{pmatrix}
1 - (m_{11} \streamproduct \X)  \; & - (m_{12} \streamproduct \X) \\
-(m_{21} \streamproduct \X) \; & 1 - (m_{22} \streamproduct \X)
\end{pmatrix}
\]
whose values are simple polynomial streams, by standard
linear algebra.
%\cbox{Filippo> The notion of degree has not been introduced}

Let us look at an example. For
\[
M = \;
\begin{pmatrix}
0 \, & 1 \\ -1 \,  & 2
\end{pmatrix}
\;\;\;\;\;\;
N = \;
\begin{pmatrix}
1  \\  2
\end{pmatrix}
\]
our linear system of stream differential equations
(\ref{linear_system_of_dimension_2}) has the
following solution:
\begin{eqnarray*}
\begin{pmatrix}%brackets around
\sigma \\ \tau
\end{pmatrix}
& = & (I - (\X \streamproduct M))^{-1} \, \streamproduct \, N
\\
& = &
\begin{pmatrix}
1  \, & -  \X \\
\X \, & 1 - 2\X
\end{pmatrix}^{-1}
\, \streamproduct \,
\begin{pmatrix}
1  \\  2
\end{pmatrix}
\\
& = &
\begin{pmatrix}
\frac{1-2\X}{(1-\X)^2} \; & \frac{\X}{(1-\X)^2} \\
\frac{-\X}{(1-\X)^2} \; & \frac{1}{(1-\X)^2}
\end{pmatrix}
\, \streamproduct \,
\begin{pmatrix}
1  \\  2
\end{pmatrix}
\\
& = &
\begin{pmatrix}
\frac{1}{(1-\X)^2} \\
\frac{2-\X}{(1-\X)^2}
\end{pmatrix}
\end{eqnarray*}
We note that the solutions of linear systems of stream differential equations
always consist of rational streams.


%\subsection{The linear final homomorphism}
%Consider again the diagram
%\[
%\xymatrix@+1.0ex@C+2.0ex{ \K^2 \dto_-{ <H,F>}  \ar @{-->}[r]^-{f} &
%\K^\omega \dto
%\\
%\K \vectproduct \K^2 \rto_-{ 1 \vectproduct f } & \K \vectproduct
%\K^\omega }
%\]
%where $H$ and $F$ are (linear mappings represented by) $\K$-valued
%matrices of size $(1 \times 2)$ and $(2 \times 2)$, respectively. We
%will show how our final homomorphism $f$ can be characterised in
%terms of rational streams. To this end, we define
%\[
%\sigma = f((1,0)) \;\;\;\;\;\;\;\;
%\tau = f((0,1))
%\]
%It follows from the commutativity of the diagram above that
%\[
%\begin{pmatrix}
%\sigma \\ \tau
%\end{pmatrix} '
%= \; F^t \streamproduct
%\begin{pmatrix}
%\sigma \\ \tau
%\end{pmatrix}
%\;\;\;\;\;\;\;\;\;\;\;
%\begin{pmatrix}
%\sigma \\ \tau
%\end{pmatrix} (0)
%= \;
%H^t
%\]
%(where the superscript $t$ indicates matrix transpose).
%These identities present $\sigma$ and $\tau$ as the solution
%of a linear system of stream differential equations.
%By the results from Subsection
%\ref{Solving linear systems of stream differential equations},
%it follows that
%\[
%\begin{pmatrix}%brackets around
%\sigma \\ \tau
%\end{pmatrix}
%\, = \; (I - (\X \streamproduct F^t))^{-1} \, \streamproduct \, H^t
%\]
%which leads to the following general formula for $f$:
%\[
%f((r_1,r_2)) = \;
%\begin{pmatrix}
%r_1 \,  & r_2
%\end{pmatrix}
%\, \streamproduct \, (I - (\X \streamproduct F^t))^{-1} \,
%\streamproduct \, H^t
%\]
%For instance, if
%\[
%F = \;
%\begin{pmatrix}
%0 \, & -1 \\ 1 \,  & 2
%\end{pmatrix}
%\;\;\;\;\;\;
%H = \;
%\begin{pmatrix}
%1  &  2
%\end{pmatrix}
%\]
%we find, using the example from Subsection
%\ref{Solving linear systems of stream differential equations},
%that
%\begin{eqnarray*}
%f((r_1,r_2))
%& = &
%\begin{pmatrix}
%r_1 \,  & r_2
%\end{pmatrix}
%\, \streamproduct \, (I - (\X \streamproduct F^t))^{-1} \,
%\streamproduct \, H^t
%\\
%& = &
%\begin{pmatrix}
%r_1 \,  & r_2
%\end{pmatrix}
%\, \streamproduct \, (I - (\X \streamproduct M))^{-1} \,
%\streamproduct \, N
%\\
%& = &
%\begin{pmatrix}
%r_1 \,  & r_2
%\end{pmatrix}
%\, \streamproduct \,
%\begin{pmatrix}
%\frac{1}{(1-\X)^2} \\
%\frac{2-\X}{(1-\X)^2}
%\end{pmatrix}
%\\
%& = &
%\frac{(r_1 + 2 r_2) - \,r_2\X }{(1-\X)^2}
%\end{eqnarray*}


\subsection{Characterising the final morphism: $|A|=1$}
\label{Characterising the final morphism ($|A|=1$}

%\cbox{Filippo: We never say that $\K^\omega$ is the final coalgebra.
%We should maybe mention it and introduce its functor?}
It is easy to see that when $|A|=1$, the final coalgebra for the functor $\fL$ is $(\K^{\omega}, <(-)(0), (-)'>)$ where $(-)(0):\K^{\omega} \to \K$
and $(-)': \K^{\omega} \to \K^{\omega}$ map each stream $\sigma$ to  its initial value $\sigma(0)$ and to its stream derivative $\sigma'$.
Let  $(\K^2 , <o,t>)$ be a \lwa,
with linear maps $o: \, \K^2 \to \K$ and $t: \, \K^2 \to \K^2$ that are represented by a $1 \times 2$-matrix $O$ and by a $2
\times 2$-matrix $T$.
We will now show how the final homomorphism
\[
\xymatrix@+1.0ex@C+3.6ex{ \K^2 \dto_-{ <o,t>}
\ar[r]^-{\beh{-}{\fL}{\K^2}} & \K^\omega \dto^{<(-)(0), (-)'>}
\\
\K \vectproduct \K^2 \rto_-{ id_{\K} \vectproduct \beh{-}{\fL}{\K^2} }
& \K \vectproduct \K^\omega }
\]
can be
characterised in terms of rational streams.
To this end, we define
\[
\sigma = \beh{\begin{pmatrix} 1 \\ 0
\end{pmatrix}}{\fL}{\K^2} \;\;\;\;\;\;\;\;
\tau = \beh{\begin{pmatrix} 0 \\ 1
\end{pmatrix}}{\fL}{\K^2}
\]
%
It follows from the commutativity of the diagram above that
\[
\sigma'=\beh{(T
%
\left(%
\begin{array}{c}
  1 \\
  0
\end{array}\right))}{\fL}{\K^2}
\;\;\;\;\;\;\;\;\;\;\; \sigma(0)=O \left(%
\begin{array}{c}
  1 \\
  0
\end{array}\right)
\]

\[
\tau'=\beh{(T
%
\left(%
\begin{array}{c}
  0 \\
  1
\end{array}\right))}{\fL}{\K^2}
%
\;\;\;\;\;\;\;\;\;\;\; \tau(0)=O \left(%
\begin{array}{c}
  0 \\
  1
\end{array}\right)
\]

and this can be concisely expressed by the following system:
\[
\begin{pmatrix}
\sigma \\ \tau
\end{pmatrix} '
= \; \transp{T} \streamproduct
\begin{pmatrix}
\sigma \\ \tau
\end{pmatrix}
\;\;\;\;\;\;\;\;\;\;\;
\begin{pmatrix}
\sigma \\ \tau
\end{pmatrix} (0)
= \; \transp{O}
\]
(where the superscript $\mathrm{t}$ indicates matrix transpose). These
identities present $\sigma$ and $\tau$ as the solution of a linear
system of stream differential equations. By the results from
Subsection \ref{Solving linear systems of stream differential
equations}, it follows that
\[
\begin{pmatrix}%brackets around
\sigma \\ \tau
\end{pmatrix}
\, = \; (I - (\X \streamproduct \transp{T}))^{-1} \, \streamproduct
\, \transp{O}
\]
which leads to the following general formula for $\beh{-}{\fL}{\K^2}$:
\[
\beh{\begin{pmatrix} k_1 \\ k_2
\end{pmatrix}}{\fL}{\K^2} = \;
\begin{pmatrix}
k_1 \,  & k_2
\end{pmatrix}
\, \streamproduct \, (I - (\X \streamproduct \transp{T}))^{-1} \,
\streamproduct \, \transp{O}
\]
For instance, if
\[
T = \;
\begin{pmatrix}
0 \, & -1 \\ 1 \,  & 2
\end{pmatrix}
\;\;\;\;\;\; O = \;
\begin{pmatrix}
1  &  2
\end{pmatrix}
\]
we find, using the example with $M$ and $N$ from Subsection \ref{Solving linear
systems of stream differential equations}, that
\begin{eqnarray*}
\beh{\begin{pmatrix} k_1 \\ k_2
\end{pmatrix}}{\fL}{\K^2} & = &
\begin{pmatrix}
k_1 \,  & k_2
\end{pmatrix}
\, \streamproduct \, (I - (\X \streamproduct T^t))^{-1} \,
\streamproduct \, O^t
\\
& = &
\begin{pmatrix}
k_1 \,  & k_2
\end{pmatrix}
\, \streamproduct \, (I - (\X \streamproduct M))^{-1} \,
\streamproduct \, N
\\
& = &
\begin{pmatrix}
k_1 \,  & k_2
\end{pmatrix}
\, \streamproduct \,
\begin{pmatrix}
\frac{1}{(1-\X)^2} \\
\frac{2-\X}{(1-\X)^2}
\end{pmatrix}
\\
& = & \frac{(k_1 + 2 k_2) - \,k_2\X }{(1-\X)^2}
\end{eqnarray*}
%\cbox{Filippo: For which value of $k_1$ and $k_2$ this stream is
%equal to $0\; 0\; 0\; 0\; 0\dots$ ???}
Note that the above expression fully characterizes $\beh{-}{\fL}{\K^2}$,
in the sense that it maps each $v\in \K^2$ in the corresponding rational stream.

\paragraph{Computing $\approx_{\fL}$}
We can employ the above characterization in order to compute $\approx_{\fL}$ on $(\K^2 , <o,t>)$. We use the fact
that the final homomorphism identifies precisely all
equivalent states:
\begin{eqnarray*}
\begin{pmatrix}
x_1 \\ x_2
\end{pmatrix}
\approx_\fL
\begin{pmatrix}
y_1 \\ y_2
\end{pmatrix}
& \iff &
\beh{\begin{pmatrix} x_1 \\ x_2
\end{pmatrix}}{\fL}{\K^2}
\, = \;
\beh{\begin{pmatrix} y_1 \\ y_2
\end{pmatrix}}{\fL}{\K^2}
\\
& \iff &
\beh{\begin{pmatrix} x_1 - y_1 \\ x_2 - y_2
\end{pmatrix}}{\fL}{\K^2}
\, = \;0
\end{eqnarray*}
where the $0$ on the right is the stream $[0] = (0,0,0, \ldots)$.
The kernel of the final homomorphism can now be computed using our
characterisation above: for all $k_1, k_2 \in \K$,
\begin{eqnarray*}
\beh{\begin{pmatrix} k_1 \\ k_2
\end{pmatrix}}{\fL}{\K^2}
\, = \;0
& \iff &
\frac{(k_1 + 2 k_2) - \,k_2\X }{(1-\X)^2}
\, = \; 0
\\
& \iff &
(k_1 + 2 k_2) - \,k_2\X \, = \; 0
\\
& \iff &
k_1 = 0 \; \mbox{and} \;\; k_2 = 0
\end{eqnarray*}
As a consequence, we find, for the present example:
\begin{eqnarray*}
\begin{pmatrix}
x_1 \\ x_2
\end{pmatrix}
\approx_\fL
\begin{pmatrix}
y_1 \\ y_2
\end{pmatrix}
& \iff &
\begin{pmatrix}
x_1 \\ x_2
\end{pmatrix}
=
\begin{pmatrix}
y_1 \\ y_2
\end{pmatrix}
\end{eqnarray*}


\subsection{Rational weighted languages}
\label{Rational weighted languages}

\newcommand\Xa{\X_a}
\newcommand\Xb{\X_b}
All the results presented above allow to characterize the final
homomorphism for weighted automata over an alphabet with a single
letter. These results can be generalized in order to deal with
alphabets of size greater than one.

Let $A$ be an arbitrary finite alphabet.
%The letters $a, b, \ldots$ denote typical elements of $A$. Let $A^∗$
%be the set of all finite words over A. Prefixing a word $w$ in $A^∗$ with a
%letter $a \in A$ is denoted by $aw$; concatenation of words $v$ and
%$w$ is denoted by $vw$; and $\epsilon$ denotes the
%empty word.
%
%For an input symbol $a \in A$, the input derivative or a-derivative
%$\sigma_a\colon A^*� \to \K$ of a language $\sigma\in \K^{A^*}$ is
%defined for all $w\in A^*$ as
%\[
%\sigma_a (w) = \sigma(aw)
%\]
%The initial value (or output) of a language $\sigma$ is defined by
%$\sigma(\epsilon)$.
 Recall from Section \ref{sec:langeq} that the final $\fL$-coalgebra
is $(\K^{A^*}, <\emp,\der>)$ where for all $\sigma\in
 \K^{A^*}$ and $a\in A$,
\[
\emp(\sigma) = \sigma(\epsilon)\ \ \ \ \ \ \  \der(\sigma)(a) =
\sigma_a
\]
and $\sigma_a$ denotes the $a$-derivatives of the language $\sigma$.

The calculus presented in the previous section for one-variable power
series (streams) can be generalized for multiple variable
series~\cite{Rutten2003-behavioural-differential-equations}, which we
will recall next.

There are unique operators on series satisfying the
following equations. For all $k\in\K$, $a,b \in A$ and
$\sigma,\tau\in \K^{A^*}$,
\[
\begin{array}{lll}
\hline
\text{Derivative}&\text{Initial Value}&\text{Name}\\
\hline
k_a=0&k(\epsilon)=k &\text{Constant}\\
(\Xa)_a = 1, (\Xa)_b = 0\ (b\neq a) & \Xa(\epsilon) = 0 &\text{Variable}\\
(\sigma+\tau)_a = \sigma_a + \tau_a & (\sigma+\tau)(\epsilon) =
\sigma(\epsilon) + \tau(\epsilon)& \text{Sum}\\
(\sigma\streamproduct\tau)_a = (\sigma_a\streamproduct\tau) +
(\sigma(\epsilon)\streamproduct\tau_a) &
(\sigma\streamproduct\tau)(\epsilon) =
\sigma(\epsilon) \streamproduct \tau(\epsilon)& \text{Convolution product}\\
(\sigma^{-1})_a = -
(\sigma(\epsilon)^{-1}\streamproduct\sigma_a)\streamproduct\sigma^{-1}
&
(\sigma^{-1})(\epsilon) =\sigma(\epsilon)^{-1}, \text{if }
\sigma(\epsilon)\neq 0\ \ \ & \text{Inverse}\\
\\
\hline
\end{array}
\]
A weighted language is \emph{rational} if it can be constructed from
finitely many constants $k\in\K$ and variables $\Xa$, by means of
the operators of sum, product, and inverse. Rational languages constitute the
class of languages that are recognized by finite dimensional weighted automata.



As for streams, one can compute a series from its initial value and
derivatives by the so-called fundamental
theorem~\cite{Rutten2003-behavioural-differential-equations}. That is,
for all weighted
languages $\sigma\in\K^{A^*}$:
\begin{eqnarray}\label{ft-series}
\sigma = \sigma(\epsilon) + \sum_{a\in A} \Xa\streamproduct
\sigma_a
\end{eqnarray}
The fundamental theorem allows us to solve equations, similar to
what happened above for streams. As an example, take $A= \{a,b\}$
(weighted languages over two symbols coincide with infinite binary
trees), and the following equations
\[
\sigma_a = 3\streamproduct \sigma,\ \ \ \sigma_b=3\streamproduct
\sigma, \ \ \ \ \sigma(\epsilon) = 1
\]
Applying the fundamental theorem we reason as follows:
\begin{eqnarray*}
& \sigma &=  \sigma(\epsilon)+(\Xa\streamproduct \sigma_a)+(\Xb\streamproduct \sigma_b)\\
\Leftrightarrow& \sigma&=
1+(3\Xa\streamproduct \sigma)+(3\Xb\streamproduct \sigma)\\
\Leftrightarrow&(1-3\Xa-3\Xb) \sigma&= 1\\
\end{eqnarray*}
which leads to the solution $\sigma = (1-3\Xa-3\Xb)^{-1}$, the
tree depicted in the following picture.
\begin{center}
\includegraphics[scale=.4]{pot3.eps}
\end{center}

Note that the above language is exactly the one recognized by the
automaton in Figure~\ref{fig:cospan}. It is also interesting to
remark the strong similarity with streams: the formula for the
stream $(1,3,9,\ldots)$ is $(1-3\X)^{-1}$.

\bigskip

Now that we know how to compute the solution of a single equation,
moving to systems of equations is precisely as for streams. Again,
for notational convenience, we shall exemplify with linear systems
of dimension $2$. The goal is to solve
\[
\begin{pmatrix}%brackets around
\sigma \\ \tau
\end{pmatrix}_a
=  M_a \streamproduct
\begin{pmatrix}%brackets around
\sigma \\ \tau
\end{pmatrix}
\;\;\;\;\;\;\;\;\;\;\;
\begin{pmatrix}%brackets around
\sigma \\ \tau
\end{pmatrix} (\epsilon)
= \;
N
\]
where, for each $a\in A$, $M_a$ is a $2\times 2$-matrix and $N$ is
a $2\times 1$-matrix over $\K$.

We now solve this system by calculating as follows (similar to the
stream case), now using the fundamental theorem for weighted
languages, given in equation~(\ref{ft-series}):
\begin{eqnarray*}
\begin{pmatrix}%brackets around
\sigma \\ \tau
\end{pmatrix}
& = &
\begin{pmatrix}%brackets around
\sigma \\ \tau
\end{pmatrix} (\epsilon)
\; + \; \sum_{a\in A} \Xa \streamproduct \,
\begin{pmatrix}%brackets around
\sigma \\ \tau
\end{pmatrix}_a
\\
& = & N \; + \; \sum_{a\in A}  \Xa \streamproduct M_a
\streamproduct
\begin{pmatrix}%brackets around
\sigma \\ \tau
\end{pmatrix}
\end{eqnarray*}
This leads to
\[
\left(I - \sum_{a\in A} (\Xa \streamproduct M_a)\right)
\begin{pmatrix}%brackets around
\sigma \\ \tau
\end{pmatrix}
\, = \; N
\]
where $I$  and $\Xa \streamproduct M_a$ are as before.

Finally, we can express the unique solution of our linear system as follows:
\[
\begin{pmatrix}%brackets around
\sigma \\ \tau
\end{pmatrix}
\, = \; \left(I - \sum_{a\in A} (\Xa \streamproduct
M_a)\right)^{-1} \, \streamproduct \, N
\]
Hence, the only difference with the stream case is that instead of
computing the inverse of the matrix $I - (\X\streamproduct M)$ one
needs to compute the inverse of $I - \sum\limits_{a\in A}
(\Xa\streamproduct M)$.

Some remarks on computing the inverse of $I - \sum\limits_{a\in A}
(\Xa\streamproduct M)$ are now in order. Convolution product on
power series in not commutative as soon as $A$ has more than one
element (e.g., $\Xa \streamproduct \Xb \neq \Xb \streamproduct \Xa$).
Thus, the matrix above is a matrix with
 entries stemming from a non-commutative ring. Traditional methods
(Gaussian elimination, Cramer's rule,
\ldots) to
compute the inverse of matrices are not applicable and thus one needs
to resort to other (more complicated) techniques such as
quasi-determinants~\cite{Retakh} or generalized LDU decomposition~\cite{ldu}.


A function to compute the inverse of
a matrix with non-commutative entries is provided in the
\emph{Mathematica}~\cite{mathematica} package
\texttt{NCAlgebra}~\cite{NCAlgebra}. The algorithm implemented in the package is directly base in LDU decomposition~\cite{ldu}. The matrices we show below were all obtained using the aforementioned package.

For instance, for $A=\{a,b,c\}$, if
\[
M_a = M_c = \;
\begin{pmatrix}
2 & 0 \\ 0  & 0
\end{pmatrix}
\;\;\;\;\;\;
M_b = \;
\begin{pmatrix}
0 & 0.5 \\ 0  & 0.5
\end{pmatrix}
\;\;\;\;\;\;
N = \;
\begin{pmatrix}
1  &  1
\end{pmatrix}
\]
then
\[
I-\Xa\streamproduct M_a - \Xb\streamproduct M_b -
\X_c\streamproduct M_c =
\begin{pmatrix}
1-2\Xa-2\X_c  &&  -0.5 \Xb \\
0 && 1-0.5\Xb
\end{pmatrix}
\]
and
\[
(I-\Xa\streamproduct M_a - \Xb\streamproduct M_b -
\X_c\streamproduct M_c)^{-1} =
\begin{pmatrix}
\frac{1}{1-2\Xa-2\X_c}  &&  0.5\frac{1}{1-2\Xa-2\X_c}\Xb\frac{1}{1 -0.5 \Xb} \\
0 && 1-0.5\Xb
\end{pmatrix}
\]
%
%
%

\bigskip
\noindent
The final homomorphism $\beh{-}{\fL}{\K^2}$ is represented in the following diagram
\[
\xymatrix@+1.0ex@C+3.6ex{ \K^2 \dto_-{ <o,t>}  \ar[r]^-{\beh{-}{\fL}{\K^2}} & \K^{A^*}
\dto^{<\emp,\der>}
\\
\K \vectproduct \K^{2^A} \rto_-{ id_{\K} \vectproduct
\beh{-}{\fL}{\K^{2^A} }} & \K \vectproduct \K^{A^*} }
\]
where, as usual, $o$ and $t=\{t_a: \K^2 \to \K^2\}_{a\in A}$ are linear mappings
represented by the $1\times 2$-row vector $O$ and the $2 \times 2$-matrixes $T_a$, respectively.

We will show how the final homomorphism $\beh{-}{\fL}{\K^2}$ can be
characterized in terms of rational weighted languages. To this end, we again
define
\[
\sigma = \beh{\left(%
\begin{array}{c}
  1 \\
  0
\end{array}\right)}{\fL}{\K^2} \;\;\;\;\;\;\;\; \tau = \beh{\left(%
\begin{array}{c}
  0 \\
  1
\end{array}\right)}{\fL}{\K^2}
\]
%
It follows from the commutativity of the diagram above that
\[
\sigma_a=\beh{(T_a
%
\left(%
\begin{array}{c}
  1 \\
  0
\end{array}\right))}{\fL}{\K^2}
\;\;\;\;\;\;\;\;\;\;\; \sigma(\epsilon)=O \left(%
\begin{array}{c}
  1 \\
  0
\end{array}\right)
\]

\[
\tau_a=\beh{(T_a
%
\left(%
\begin{array}{c}
  0 \\
  1
\end{array}\right))}{\fL}{\K^2}
%
\;\;\;\;\;\;\;\;\;\;\; \tau(\epsilon)=O \left(%
\begin{array}{c}
  0 \\
  1
\end{array}\right)
\]
\noindent
and this can be concisely expressed by the following system:
\[
\begin{pmatrix}
\sigma \\ \tau
\end{pmatrix} _a
= \; \transp{T_{a}} \streamproduct
\begin{pmatrix}
\sigma \\ \tau
\end{pmatrix}
\;\;\;\;\;\;\;\;\;\;\;
\begin{pmatrix}
\sigma \\ \tau
\end{pmatrix} (\epsilon)
= \; \transp{O}
\]
It then follows that
\[
\begin{pmatrix}%brackets around
\sigma \\ \tau
\end{pmatrix}
\, = \; (I - \left(\sum_{a\in A} \Xa \streamproduct
\transp{T_{a}}\right))^{-1} \, \streamproduct \, \transp{O}
\]
which leads to the following general formula for
$\beh{-}{\fL}{\K^2}$:
\[
\beh{\begin{pmatrix} k_1 \\ k_2
\end{pmatrix}}{\fL}{\K^2} = \;
\begin{pmatrix}
k_1 \,  & k_2
\end{pmatrix}
\, \streamproduct \, (I - \left(\sum_{a\in A} \Xa \streamproduct
\transp{T_{a}}\right))^{-1} \, \streamproduct \, \transp{O}
\]
For instance, for $A=\{a,b,c\}$ and
\[
T_a = T_c = \;
\begin{pmatrix}
2 & 0 \\ 0  & 0
\end{pmatrix}
\;\;\;\;\;\; T_b = \;
\begin{pmatrix}
0 & 0 \\ 0.5  & 0.5
\end{pmatrix}
\;\;\;\;\;\; O = \;
\begin{pmatrix}
1  &  1
\end{pmatrix}
\]
we find, using the example above, that
\begin{eqnarray*}
\beh{ \begin{pmatrix} k_1  \\  k_2
\end{pmatrix}}{\fL}{\K^2} & = &
\begin{pmatrix}
k_1 \,  & k_2
\end{pmatrix}
\, \streamproduct \, \left(\sum_{a\in A} \Xa \streamproduct
{T_a}^t\right)^{-1} \, \streamproduct \, O^t
\\
& = &
\begin{pmatrix}
k_1 \,  & k_2
\end{pmatrix}
\, \streamproduct \, \left(\sum_{a\in A} \Xa \streamproduct
{T_a}\right)^{-1} \, \streamproduct \, N
\\
& = &
\begin{pmatrix}
k_1 \,  & k_2
\end{pmatrix}
\, \streamproduct \,
\begin{pmatrix}
\frac{1}{1-2\Xa-2\X_c} +0.5 \frac 1 {1-2\Xa-2\X_c} \Xb\frac 1 {1-0.5\Xb} \\
\frac{1}{(1-0.5\Xb)}
\end{pmatrix}
\\
& = & \frac{k_1}{1-2\Xa-2\X_c} +0.5 k_1\frac 1 {1-2\Xa-2\X_c}
\Xb\frac 1 {1-0.5\Xb} + \frac{k_2}{(1-0.5\Xb)} \end{eqnarray*}

By generalizing the above arguments from $\K^2$ to any finite dimesion vector space, we obtain the following theorem.

\begin{theorem}\label{thm:rational}
Let $(V, <o,t>)$ be a linear weighted automaton with $V$ finite dimension. Then, for all $v \in V$
\[
\beh{v}{\fL}{V} = \;
\transp{v}
\, \streamproduct \, (I - \left(\sum_{a\in A} \Xa \streamproduct
\transp{T_{a}}\right))^{-1} \, \streamproduct \, \transp{O}
\]
\end{theorem}


For an example with a three dimensional state
space, we consider the \lwa\ corresponding to the automaton $(V,<o,t>)$
in Fig.~\ref{fig:ExAlgorithm}.

\begin{eqnarray*}
\beh{\begin{pmatrix} k_1 \\ k_2 \\k_3
\end{pmatrix}}{\fL}{V} & = &
\begin{pmatrix}
k_1 \,  & k_2 \,  & k_3
\end{pmatrix}
\, \streamproduct \, (I - \left(\sum_{a\in A} \Xa \streamproduct
\transp{T_{a}}\right))^{-1} \, \streamproduct \, \transp{O}
\\
& = & \begin{pmatrix} k_1 \,  & k_2 \,  & k_3
\end{pmatrix}
\, \streamproduct \, (I -
\begin{pmatrix}
\Xa+\Xb & 0 & 0\\[1.1ex]
\frac{\Xa}{3} & 0 &\frac{\Xb}{3}\\[1.1ex]
\Xa & 3\Xb & 0\\
\end{pmatrix}
)^{-1} \, \streamproduct \, \begin{pmatrix} 2 \\ 1\\1
\end{pmatrix}
\\
& = & \begin{pmatrix} k_1 \,  & k_2 \,  & k_3
\end{pmatrix}
\, \streamproduct \,
\begin{pmatrix}
1-\Xa-\Xb & 0 & 0\\[1.1ex]
-\frac{\Xa}{3} & 1 &-\frac{\Xb}{3}\\[1.1ex]
-\Xa & -3\Xb & 1\\
\end{pmatrix}
^{-1} \, \streamproduct \, \begin{pmatrix} 2 \\ 1\\1
\end{pmatrix}
\\
\end{eqnarray*}
%As discussed above, inverting the matrix in the middle is non trivial since, when $|A|$ is bigger than $1$, the convolution product $\streamproduct$ is not commutative.
%By employing the package \texttt{NCAlgebra} of \emph{Mathematica}, we compute the inverse of the matrix in the middle above (hereafter referred as $M$).
The inverse of the matrix in the middle is
\[
M = \begin{pmatrix}
\frac{1}{1-\Xa-\Xb} & 0 & 0\\[1.2ex]
(\frac 1 3+
\frac{\Xb}{3}\frac{1}{1-\Xb^2} (\Xb+1) ) \Xa\frac{1}{1-\Xa-\Xb} & \quad 1 +
\Xb\frac{1}{1-\Xb^2}\Xb \quad&\frac{\Xb}{3}\frac{1}{1-\Xb^2}\\[1.2ex]
(\frac{1}{1-\Xb^2}) (\Xa + \Xb \Xa)
\frac{1}{1-\Xa-\Xb}  & 3 \frac{1}{1-\Xb^2} \Xb & \frac{1}{1-\Xb^2}\\
\end{pmatrix}
\]
and
\begin{eqnarray*}
M  \streamproduct \, \begin{pmatrix} 2 \\ 1\\1
\end{pmatrix} &=& \begin{pmatrix}
\frac{2}{1-\Xa-\Xb} \\[1.2ex]
(\frac 1 3+
\frac{\Xb}{3}\frac{1}{1-\Xb^2} (\Xb+1) ) \Xa\frac{2}{1-\Xa-\Xb} + 1 +
\Xb\frac{1}{1-\Xb^2}\Xb + \frac{\Xb}{3}\frac{1}{1-\Xb^2}\\[1.2ex]
(\frac{1}{1-\Xb^2}) (\Xa + \Xb \Xa)
\frac{2}{1-\Xa-\Xb} + 3 \frac{1}{1-\Xb^2} \Xb +
\frac{1}{1-\Xb^2}
\end{pmatrix} = \begin{pmatrix} \rho_1 \\ \rho_2 \\ \rho_3 \end{pmatrix}
\end{eqnarray*}
Summarizing
\begin{eqnarray}
\beh{\begin{pmatrix} k_1 \\ k_2 \\k_3
\end{pmatrix}}{\fL}{V} & = &
\begin{pmatrix}
k_1 \,  & k_2 \,  & k_3
\end{pmatrix}
\streamproduct
\begin{pmatrix} \rho_1 \\ \rho_2 \\ \rho_3 \end{pmatrix}
\end{eqnarray}
Note that the above expression fully characterizes $\beh{-}{\fL}{V}$,
in the sense that it maps each $v\in V$ in the rational weighted lanuage that it accepts.

\paragraph{Computing $\approx_{\fL}$} Now, we have a rational expression $\sigma = k_1 \rho_1 + k_2 \rho_2 +
k_3 \rho_3$ characterizing the final homomorphism and we would like to calculate for which values of $k_1$, $k_2$ and $k_3$ this expression equals $0$.
As we have shown before, when $|A|=1$, this can be done by syntactically manipulating the rational expression in a standard way.
In the general case, because of the non-commutativity of the convolution product, this is not trivial at all.


%As shown before, this is standard when $|A|=1$ but, in the general case, this is not standard at all, due to the non commutativity of $\streamproduct$.

Here, we choose to adopt the following approach: first we compute ``some'' derivatives $\sigma_a$, $\sigma_b$, $\sigma_{aa}$, $\sigma_{ab}$ \dots\ and then
we check for wich $k_1$, $k_2$ and $k_3$ the initial values $\sigma(\epsilon)$, $\sigma_a(\epsilon)$, $\sigma_b(\epsilon)$, $\sigma_{aa}(\epsilon)$, $\sigma_{ab}(\epsilon)$ \dots\ are equal to $0$.
The following lemma (proved in \cite{BR-series,Rutten2003-behavioural-differential-equations}) ensures that
we have to compute only finitely many derivatives.

\begin{lemma}
Rational weighted languages have finitely many linearly independent derivatives.
\end{lemma}

In our example, we start by taking the initial value of the expression $\sigma$ itself
obtaining $\sigma(\epsilon) = 2k_1 + k_2 + k_3$. Then we take the $a$ and $b$ derivatives
which give, respectively, the expressions
\begin{eqnarray}\label{eq:der_a}
\sigma_a = k_1(\rho_1)_a + k_2(\rho_2)_a + k_3(\rho_3)_a
\end{eqnarray}
\[
 \begin{pmatrix} \rho_1 \\ \rho_2 \\ \rho_3 \end{pmatrix}_{\!\!\!\!a} =
\begin{pmatrix}
\frac{2}{1-\Xa-\Xb} \\
\frac 1 3
\frac{2}{1-\Xa-\Xb} \\ \frac{2}{1-\Xa-\Xb}
\end{pmatrix}
\]
and
$$
\begin{array}{lcl}
\sigma_b &=& k_1(\rho_1)_b + k_2(\rho_2)_b + k_3(\rho_3)_b\\[2ex]
 \begin{pmatrix} \rho_1 \\[2ex] \rho_2 \\[2ex] \rho_3 \end{pmatrix}_{\!\!\!\!b}
&=&
\begin{pmatrix}
\frac{2}{1-\Xa-\Xb} \\[2ex] (\frac{1}{3}\frac{1}{1-\Xb^2} (\Xb+1) )
\Xa\frac{2}{1-\Xa-\Xb} + \frac{1}{1-\Xb^2}\Xb + \frac 1 3 \frac
{1} {1-\Xb^2} \\[2ex] \Xb (\frac{1}{1-\Xb^2}) (\Xa + \Xb
\Xa)
\frac{2}{1-\Xa-\Xb}   +  \Xa\frac 1 {1-\Xa-\Xb}+ 3\Xb
\frac{1}{1-\Xb^2} \Xb + 3 + \Xb
\frac{1}{1-\Xb^2}
\end{pmatrix}
\end{array}
$$
\noindent
which have initial values $\sigma_a(\epsilon) = 2k_1 + \frac 2 3 k_2 + 2k_3$
and $\sigma_b(\epsilon) =2k_1 +
\frac 1 3 k_2+ 3k_3$.

Now, note that the $a$ derivative, that is the rational expression
(\ref{eq:der_a}), will now always generate the same
derivatives for $a$ and $b$ (since the derivatives of $\frac 2
{1-\Xa-\Xb}$ are the expression itself again; intuitively, this
expression represents an infinite binary
tree with 2's in every node and hence has left and right subtrees
equal to the whole tree).   For the $b$ derivative, we
take another level of derivatives and obtain, respectively,
$$
\begin{array}{lcl}
\sigma_{ba} &=& k_1(\rho_1)_{ba} + k_2(\rho_2)_{ba} + k_3(\rho_3)_{ba}\\[2ex]
 \begin{pmatrix} \rho_1 \\ \rho_2 \\ \rho_3
\end{pmatrix}_{\!\!\!\!ba}
&=& \begin{pmatrix}
\frac{2}{1-\Xa-\Xb} \\
\frac 1 3
\frac{2}{1-\Xa-\Xb} \\ \frac{2}{1-\Xa-\Xb}
\end{pmatrix} = \begin{pmatrix} \rho_1 \\ \rho_2 \\ \rho_3
\end{pmatrix}_{\!\!\!\!a}\\ [6ex]
\multicolumn{3}{l}{\text{and}}\\[2ex]
\sigma_{bb} &=& k_1(\rho_1)_{bb} + k_2(\rho_2)_{bb} + k_3(\rho_3)_{bb}\\[2ex]
 \begin{pmatrix} \rho_1 \\[2ex]
 \rho_2 \\[2ex]
 \rho_3
\end{pmatrix}_{\!\!\!\!bb}
&=& \begin{pmatrix}
\frac{2}{1-\Xa-\Xb} \\[2ex]
 (\frac{1}{3}\Xb\frac{1}{1-\Xb^2} (\Xb+1) +
\frac 1 3)
\Xa\frac{2}{1-\Xa-\Xb} + \Xb\frac{1}{1-\Xb^2}\Xb + 1 + \frac 1
3 \Xb \frac
{1} {1-\Xb^2} \\[2ex] (\frac{1}{1-\Xb^2}) (\Xa + \Xb
\Xa)
\frac{2}{1-\Xa-\Xb}   +  3
\frac{1}{1-\Xb^2} \Xb +
\frac{1}{1-\Xb^2}
\end{pmatrix} =  \begin{pmatrix} \rho_1 \\[2ex] \rho_2 \\[2ex] \rho_3 \end{pmatrix}
\end{array}
$$
The $a$-derivative coincides with (\ref{eq:der_a}) and the $b$
derivative coincides with the original expression $\sigma$. Therefore, we have
found the system of equations we need to solve:
\[
\left\{\begin{array}{l}
\sigma(\epsilon) = 0 \\
\sigma_a(\epsilon) = 0\\
\sigma_b(\epsilon) = 0
\end{array}\right . \Leftrightarrow
\left\{\begin{array}{lcl}
2k_1 + k_2 + k_3 &=& 0\\
2k_1 + \frac 2 3 k_2 + 2k_3 &=&0 \\
2k_1 +
\frac 1 3 k_2+ 3k_3 &=& 0 \\
\end{array}\right .
\]
Solving it yields $k_1 = -2k_3$ and $k_2 = 3k_3$. Hence, the kernel
of the final homomorphism is the space spanned by the vector
$$\left(%
\begin{array}{c}
  -2 \\
  3\\
  1\\
\end{array}\right)$$ which coincides with what was
computed by the forward algorithm in Section~\ref{sec:forwalg}.

%This example also shows that this procedure is in general not more efficient then the forward algorithm.
%Indeed, the three equations of the above system coincide with the spaces computed by the forward algorithm:
%the space (of solutions) of $\sigma(\epsilon) = 0$ is the space spanned by $B_0$ (in Section~\ref{sec:forwalg}),
%the space of $\sigma_a(\epsilon) = 0$ is the one spanned by $B_1^a$, and the space $\sigma_a(\epsilon) = 0$ is
%the one spanned by $B_2^a$.

%We do not know any efficient procedures to compute $\approx_{\fL}$ by syntactically manipulating
%(as in the case of $|A|=1$) the rational expression characterizing $\beh{-}{\fL}{V}$. We leave this
%as an open question.

\section{Discussion}\label{sec:discussion}

In this paper we proposed a novel coalgebraic perspective on weighted automata and their behavioural equivalences.
Weighted automata are $\fW$-coalgebras, for a functor $\fW$ on $Set$, but
they can be regarded also as linear weighted automata, that are $\fL$-coalgebras for a functor $\fL$ on $Vect$.
The behavioural equivalence induced by $\fW$ coincides with weighted bisimilarity, while the equivalence
induced by $\fL$ ($\approx_{\fL}$) with weighted language equivalence.

%In this paper we proposed a novel coalgebraic perspective on weighted automata and their behavioural equivalences.
%Each weighted automaton $(X,<o,t>)$ is a $\fW$-coalgebra on $Set$; it can be transformed via linearization (or, as roughly said in the Introduction, ``determinization'') into
%the linear weighted automata $({\K}^{X} , <o^{\sharp}, t^{\sharp}>)$ that is an $\fL$-coalgebra on $Vect$. The behavioural equivalence induced by
%$\fW$ coincides with weighted bisimilarity, while the equivalence induced by $\fL$ ($\approx_{\fL}$) with weighted language equivalence.

Weighted languages (i.e. formal power series) form the vectors space $\K^{A^*}$ that carries the
final $\fL$-coalgebra: for each linear weighted automata $(V, <o,t>)$, the unique $\fL$-morphism
$\beh{-}{\fL}{V}$ into the final coalgebra maps each vector $v\in V$ into the weighted language
in $\K^{A^*}$ that $v$ accepts. The unique morphism $\beh{-}{\fL}{V}$ is a linear map and its
kernel coincides with $\approx_{\fL}$ that, when $V$ is finite dimension, can be computed in
three different ways. It is important to remark here that the linearity of $\beh{-}{\fL}{V}$ is the
key ingredient (in all the three approaches) to finitely compute the equivalence on an infinite
state space (represented as a vector space of finite dimension).

Theorem \ref{thm:rational} provides an explicit characterization of $\beh{-}{\fL}{V}$ by assigning  a
syntactic expression denoting a rational weighted language to each vector $v\in V$. This characterization
can be employed for computing $\approx_{\fL}$ but, in general terms, it seems to be
inconvenient to be implemented in an automatic prover. The backward algorithm, instead, is very
efficient but its presentation is a bit complex since it requires dual spaces and transpose maps.
The forward algorithm is easier to explain and we have shown it is closely related to the construction
of the final coalgebra.


\bigskip
%\subsection{From Fields to Semirings}
%
\paragraph{From fields to semirings}
Weighted automata are usually defined on \emph{semirings} rather than fields \cite{Moh09}.
We discuss now that part of the results presented in this paper can be extended to semirings.

%
Semirings can be though of as a generalization of fields, where the product is not necessarily commutative and
inverses of sum and product are not required to exist. Semimodules on a semiring generalize the notion of vector spaces on a field. Formally, a semiring $\SR$ consists of a commutative monoid $(\SR,+, 0)$ and a monoid $(\SR,\cdot, 1)$
such that the product distributes over sum (namely, $s_1 \cdot (s_2 +s_3) = (s_1\cdot s_2) + (s_1\cdot s_3)$ and
$( s_1 + s_2) \cdot s_3 = (s_1\cdot s_3) + (s_2\cdot s_3)$) and $0$ annihilates with respect to product ($0\cdot s= 0 = s\cdot 0$). A semimodule on $\SR$ is a commutative monoid $(V,+,0)$ equipped with an external action $(\cdot)\colon \SR \times V \to V$ such that for all
$s,s_1,s_2\in \SR$ and $v,v_1,v_2\in V$ (a) $s\cdot (v_1 + v_2) = (s\cdot v_1) + (s\cdot v_2)$,
(b) $(s_1 +s_2) \cdot v  = (s_1\cdot v) + (s_2\cdot v)$ and (c) $(s_1\cdot s_2) \cdot v = s_1\cdot (s_2 \cdot v)$.
Linear maps between semimodules are defined in the same way as linear maps between vector spaces, namely, as functions preserving $+$ and $(\cdot)$.
Semimodules and linear maps forms the category $SMod$ which has product ($V\times W$) and exponent ($V^A$) defined as in $Vect$.
Given a set $X$, a semimodule $V$ and a function $f\colon X \to V$, the free semimodule generated by $X$ (denoted by $\SR_{\omega}^X$) and
the linearization of $f$ (denoted by $f^\sharp$) are defined as for vector spaces.


With these ingredients, we can extend all the results of Sections \ref{sec:coalg} and \ref{sec:linearcoalg} (apart from Section \ref{sec:linearbis})
to semirings. First of all, we would define the semiring valuation functor $\SR^-_{\omega}\colon Set \to Set$
in the same way as the field valuation functor $\K^-_{\omega}\colon Set \to Set$ (Definition \ref{def:monoidfunctor})
and we would model weighted automata using the functor $\fW =\SR \times (\SR^-_{\omega})^A \colon Set \to Set$. In this way, $\fW$-coalgebras are in one to one
correspondence with weighted automata on $\SR$ and all the proofs and results of Section \ref{sec:coalg} are still valid.
Then, linear weighted automata (Definition \ref{def:lwa}) would be defined as coalgebras on $SMod$ rather than on $Vect$. More precisely, coalgebras for the
functor $\fL = \SR \times -^A \colon SMod \to SMod$. Given an $\SR$-weighted automata $(X, <o,t>)$ we can build the linear weighted automata
$(\SR^X_{\omega}, <o^{\sharp} , t^{\sharp} >)$, where $\SR^X_{\omega}$ is the free semimodule generated by $X$ and
$o^{\sharp}$ and $t^{\sharp}$ are the linearizations of $o$ and $t$. It is easy to see that all the proofs and results of
Section \ref{sec:langeq} are still valid.

The notion of linear relation (Definition \ref{Def:LineEq}) relies on the existence of minus operator ``$-$'' (the inverse of sum)
and thus the results of Section \ref{sec:linearbis} cannot be naively extended to generic semirings.
%
%
%
%As a future work, we would like to extend these results to automata with weights on a semiring $\SR$
%(instead of field $\K$). The coalgebraic characterization of weighted bisimilarity can be easily
%obtained by employing a \emph{semiring evaluation functor} instead of the field evaluation
%functor (Definition \ref{def:monoidfunctor}). For weighted language equivalence on semirings, we should
%define the functor $\fL$ on the category of \emph{semimodules}, instead of $Vect$.
%
The forward algorithm
could be extended (by exploiting it s relationship with the construction of final coalgebras), % in a rather
%straightforward way,
but the convergence in a finite number of iterations might be not guaranteed.
The other two procedures strongly rely on the properties of fields and vector spaces (such
as the existence of the inverse multiplicative or the dual space). Therefore, it seems challenging to
extend them to the case of a generic semiring.  If $\SR$ is a semifield however, then all elements have a
multiplicative inverse and further connections could be explored. An important example of semifield is the tropical
semiring~\cite{HW96} (for which, however, weighted language equivalence is undecidable~\cite{Krob}). Further, when $\SR$ is a commutative ring, annihilators and transpose maps
can be generalized as operations carried out within the dual module (i.e. linear maps from an
$\SR$-module to $\SR$, seen as a module)~\cite{Rot02}. We leave these extensions as future work.

%
%
%As a future work, we would like to extend these results to automata with weights on a semiring $\SR$
%(instead of field $\K$). The coalgebraic characterization of weighted bisimilarity can be easily
%obtained by employing a \emph{semiring evaluation functor} instead of the field evaluation
%functor (Definition \ref{def:monoidfunctor}). For weighted language equivalence on semirings, we should
%define the functor $\fL$ on the category of \emph{semimodules}, instead of $Vect$. The forward algorithm
%could be extended (by exploiting its relationship with the construction of final coalgebras) in a rather
%straightforward way, but the convergence in a finite numbers of iterations might be not guaranteed.
%The other two approaches strongly rely on the properties of fields and vector spaces (such
%as the existence of the inverse multiplicative or the dual space). Therefore, it seems challenging to
%extend them to the case of a generic semiring $\SR$.  If $\SR$ is a semifield however, then all elements have a
%multiplicative inverse. An important example of semifield in this context is the tropical
%semiring~\cite{HW96}. Further, when $\SR$ is a commutative ring, annihilators and transpose maps
%can be generalized as operations carried out within the dual module (i.e. linear maps from an
%$\SR$-module to $\SR$, seen as a module)~\cite{Rot02}. We leave these extensions as future work.


\paragraph{Initial weight}
Besides the output weight $o$ and the transition relation $t$, weighted automata are often equipped
also with a vector $i$, called \emph{initial weight}~\cite{Moh09}. Initial weights cannot
be modeled in our coalgebraic approach and, more generally, coalgebras are usually considered
not suitable to model ``initial states''. However, instead of the language of a weighted automaton $(X,<t,o,i>)$ as in ~\cite{Moh09}
we can equivalently consider the language recognized by vector $i$ of the $\fL$-coalgebra $(\K^X_{\omega}, <o^{\sharp},t^{\sharp}>)$.
Unfortunately, this is possible for $\fL$-coalgebras but not for $\fW$-coalgebras, since the state-spaces of the formers are vector spaces,
while those of the latters are just sets.

%\bigskip
\paragraph{Related work}
Our approach is closely related to the work presented in~\cite{german} whose
weighted automata are equipped with initial weights.
When restricting the automata in~\cite{german} to those having a single initial state
(i.e., the initial weight vector contains one $1$ and the others are $0$s), these closely correspond to $\fW$-coalgebras:
``forward bisimilarity'' of \cite{german} coincides with the coalgebraic $\fW$-behavioural equivalence, ``functional simulations'' are
$\fW$-homomorphisms  and ``aggregated automata'' for a bisimulation $R$ are the $\fW$-coalgebras $(X/R, <o_{X/R},t_{X/R}>)$ of
Section \ref{sec:coalg}.

In \cite{german}, there are also two notions of ``backward bisimilation'', but none of them
is related to the equivalences considered in this work. In particular, they are not related to the backward algorithm,
since the relations computed by such algorithm are \emph{linear} and they approximate the complement of \emph{language equivalence}.
Moreover, ``backward bisimilation'' can be defined in any possible semiring while, as discussed above, the backward algorithm
is possible only in some more specific cases.

%\mar{Two paragraphs added - please check. Jan can you add related work on the stream calculus}
Computing $\approx_{\fL}$ for finite dimensional linear weighted automata
implies the decidability of language equivalence for \emph{finite} state weighted automata
over a field. This was already observed by Schutzenberger~\cite{Schutzenberger61b}
using a cubic reduction algorithm~\cite{BR-series}. By constructing a finite sequence
of simulations, decidability of language equivalence for  finite state weighted automata
over a field is studied in~\cite{BLS06}. More recently, this decidability result has been
extended to automata with weights over a large class of semiring~\cite{EM2010}.

In this paper we have shown that an advantage of linear weighted automata is the
existence of minimization algorithms for them. Minimization algorithms have been
extensively studied in the context of \emph{deterministic} finite state weighted
automata~\cite{Moh97,Moh00,Moh09}. While minimization is not well-defined for
automata with weights on a general semiring, a simple and practical algorithm
that works for all division semirings, and thus also fields, is given in~\cite{Eis03}.
When considering  only automata with weights over fields, our algorithms are more
general, because, they can be used to minimize a non-necessarily deterministic finite
state weighted automaton by first \emph{determinizing} it to a linear weighted one.



%\section*{References}
\bibliographystyle{alpha}
\bibliography{lwac}



\end{document}
